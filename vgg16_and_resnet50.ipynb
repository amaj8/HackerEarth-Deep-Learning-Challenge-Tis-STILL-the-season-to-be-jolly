{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vgg16 and resnet50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMporwvvnc5KdaBw5hCGpUS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amaj8/HackerEarth-Deep-Learning-Challenge-Tis-STILL-the-season-to-be-jolly/blob/main/vgg16_and_resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW2NAVceaJr9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage import transform as tr\n",
        "from skimage.color import gray2rgb\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "\n",
        "\n",
        "plt.ion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42TLbMp0aSb-",
        "outputId": "d9be7bda-13f5-4e0d-b263-4148336422ae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4rW43TqaJsD"
      },
      "source": [
        "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# # For example, here's several helpful packages to load\n",
        "\n",
        "# import numpy as np # linear algebra\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# # Input data files are available in the read-only \"../input/\" directory\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dkYwKMnaJsE",
        "outputId": "c34d23ed-21ba-411e-902d-d90ebe8a905d"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez4Wmdp4aJsE",
        "outputId": "9306d92d-aef5-462a-f400-f550bb25c5af"
      },
      "source": [
        "path = '/content/drive/MyDrive/HackerEarth Holiday Season Challenge/input_dataset/dataset/train.csv'\n",
        "train = pd.read_csv(path)\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6469, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrnafEPxaJsF",
        "outputId": "8cbf9193-a8f7-4e79-8536-1dc44e0cf9aa"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "labels = ['Miscellaneous','Christmas_Tree','Jacket','Candle','Airplane','Snowman']\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "39tsx9mgaJsF",
        "outputId": "40cb8021-a735-428a-8152-276681b3029c"
      },
      "source": [
        "train.Class = le.transform(train.Class)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image3476.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image5198.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image4183.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image1806.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image7831.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Image  Class\n",
              "0  image3476.jpg      4\n",
              "1  image5198.jpg      1\n",
              "2  image4183.jpg      5\n",
              "3  image1806.jpg      4\n",
              "4  image7831.jpg      4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFha_ZBxaJsF"
      },
      "source": [
        "train.to_csv('train2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_DVjjLQaJsG"
      },
      "source": [
        "class HolidayDataset(Dataset):\n",
        "    def __init__(self,csv_file, root_dir,transform = None):\n",
        "        self.csv = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.csv)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "#         if torch.is_tensor(idx):\n",
        "#             idx = idx.tolist()\n",
        "#         print(idx)\n",
        "        img_name = os.path.join(self.root_dir, self.csv.iloc[idx,0])\n",
        "        # converting gray to rgb because some images seem to be grayscale\n",
        "        image = gray2rgb(io.imread(img_name))\n",
        "        label = np.array(self.csv.iloc[idx,1])\n",
        "#         sample = {'image':image, 'label':label}\n",
        "#         sample['image'].permute(1,2,0)\n",
        "        \n",
        "        if self.transform:\n",
        "#             print(sample.type())\n",
        "#             sample['image'] = self.transform(sample['image'])\n",
        "#             sample = self.transform(sample)\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return (image,label)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbl33QdFaJsG"
      },
      "source": [
        "# resizing so that all tensors are of the same size\n",
        "transf = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Resize(256),\n",
        "                                 transforms.CenterCrop(224),\n",
        "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "#                                 transforms.Grayscale(3)\n",
        "#                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "                               ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAxu6384aJsH"
      },
      "source": [
        "csv_file_path = 'train2.csv'\n",
        "root_dir_path = '/content/drive/MyDrive/HackerEarth Holiday Season Challenge/input_dataset/dataset/train'\n",
        "holiday_data_train = HolidayDataset(csv_file = csv_file_path,\n",
        "                             root_dir = root_dir_path,\n",
        "                             transform = transf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KbbULAubu4e"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq86xY7iaJsH"
      },
      "source": [
        "train_set, valid_set = torch.utils.data.random_split(holiday_data_train,[5000,len(holiday_data_train)-5000])\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct-eOFNgaJsH"
      },
      "source": [
        "def accuracy(net,loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    for data in loader:\n",
        "        inputs,labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _,predicted = torch.max(outputs,1)\n",
        "        correct += (predicted == labels).float().sum()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "    return correct/total * 100.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxYLSiXDaJsI"
      },
      "source": [
        "def train_model(model, train_loader, valid_loader, loss_criterion, optimizer, epochs):\n",
        "    start_time = time()\n",
        "    losses = []\n",
        "    train_acc = []\n",
        "    valid_acc = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i,data in enumerate(train_loader):\n",
        "    #         print(i)\n",
        "            inputs,labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_criterion(outputs,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 2000:\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss))\n",
        "    #             running_loss = 0.0\n",
        "\n",
        "        epoch_loss = running_loss/len(train_loader)\n",
        "        losses.append(epoch_loss)\n",
        "        train_acc.append(accuracy(model,train_loader))\n",
        "        valid_acc.append(accuracy(model,valid_loader))\n",
        "\n",
        "    print('Finished Training')\n",
        "    train_time = time() - start_time\n",
        "    return {'epoch_loss':losses, 'train_acc': train_acc, 'valid_acc':valid_acc, 'train_time': train_time}\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAS851Gyb1PV"
      },
      "source": [
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TslTqzB9Lr_0"
      },
      "source": [
        "# Ensemble of VGG16 and Resnet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0okCcRLrg8"
      },
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n",
        "for param in vgg16.parameters():\n",
        "  param.requires_grad_(False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J99VqWtdMAP4"
      },
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "for param in resnet50.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "# resnet50.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvqG1euZOOsb"
      },
      "source": [
        "class MyEnsemble(nn.Module):\n",
        "    def __init__(self,modelA, modelB, nb_classes =6):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.modelA.classifier[6] = nn.Identity()\n",
        "        self.modelB.fc = nn.Identity()\n",
        "        \n",
        "#         self.classifier = nn.Linear(2048 + 4096, nb_classes)\n",
        "        self.classifier = nn.Sequential(\n",
        "                            nn.Linear(2048 + 4096,1024),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.4),\n",
        "                            nn.Linear(1024,512),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.4),\n",
        "                            nn.Linear(512,256),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.4),\n",
        "                            nn.Linear(256,6),\n",
        "                            \n",
        "                            \n",
        "                                )\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x1 = self.modelA(x.clone())\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = self.modelB(x)\n",
        "        x2 = x2.view(x2.size(0),-1)\n",
        "        x = torch.cat((x1,x2), dim = 1)\n",
        "        \n",
        "        x = self.classifier(F.relu(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guo-nzxHO_GQ",
        "outputId": "939f9f39-80dd-42a8-a243-292e5538a6ac"
      },
      "source": [
        "model = MyEnsemble(vgg16,resnet50)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyEnsemble(\n",
              "  (modelA): VGG(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): ReLU(inplace=True)\n",
              "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (18): ReLU(inplace=True)\n",
              "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (20): ReLU(inplace=True)\n",
              "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (22): ReLU(inplace=True)\n",
              "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (25): ReLU(inplace=True)\n",
              "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (27): ReLU(inplace=True)\n",
              "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (29): ReLU(inplace=True)\n",
              "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "    (classifier): Sequential(\n",
              "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              "  (modelB): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Identity()\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=6144, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.4, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.4, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Dropout(p=0.4, inplace=False)\n",
              "    (9): Linear(in_features=256, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvCRE_i0MdJP"
      },
      "source": [
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEEwnYPtMQzL",
        "outputId": "9d0187ab-f245-473e-f05d-c9aed0674146"
      },
      "source": [
        "results = train_model(model, train_loader, valid_loader, criterion, optimizer, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,     2] loss: 3.630\n",
            "[1,     3] loss: 5.458\n",
            "[1,     4] loss: 7.276\n",
            "[1,     5] loss: 9.103\n",
            "[1,     6] loss: 10.909\n",
            "[1,     7] loss: 12.723\n",
            "[1,     8] loss: 14.547\n",
            "[1,     9] loss: 16.359\n",
            "[1,    10] loss: 18.183\n",
            "[1,    11] loss: 20.019\n",
            "[1,    12] loss: 21.829\n",
            "[1,    13] loss: 23.651\n",
            "[1,    14] loss: 25.463\n",
            "[1,    15] loss: 27.276\n",
            "[1,    16] loss: 29.096\n",
            "[1,    17] loss: 30.913\n",
            "[1,    18] loss: 32.731\n",
            "[1,    19] loss: 34.549\n",
            "[1,    20] loss: 36.364\n",
            "[1,    21] loss: 38.176\n",
            "[1,    22] loss: 39.989\n",
            "[1,    23] loss: 41.802\n",
            "[1,    24] loss: 43.612\n",
            "[1,    25] loss: 45.421\n",
            "[1,    26] loss: 47.261\n",
            "[1,    27] loss: 49.085\n",
            "[1,    28] loss: 50.909\n",
            "[1,    29] loss: 52.737\n",
            "[1,    30] loss: 54.552\n",
            "[1,    31] loss: 56.364\n",
            "[1,    32] loss: 58.175\n",
            "[1,    33] loss: 59.997\n",
            "[1,    34] loss: 61.821\n",
            "[1,    35] loss: 63.643\n",
            "[1,    36] loss: 65.451\n",
            "[1,    37] loss: 67.275\n",
            "[1,    38] loss: 69.102\n",
            "[1,    39] loss: 70.931\n",
            "[1,    40] loss: 72.743\n",
            "[1,    41] loss: 74.546\n",
            "[1,    42] loss: 76.382\n",
            "[1,    43] loss: 78.213\n",
            "[1,    44] loss: 80.038\n",
            "[1,    45] loss: 81.867\n",
            "[1,    46] loss: 83.678\n",
            "[1,    47] loss: 85.486\n",
            "[1,    48] loss: 87.311\n",
            "[1,    49] loss: 89.125\n",
            "[1,    50] loss: 90.942\n",
            "[1,    51] loss: 92.759\n",
            "[1,    52] loss: 94.568\n",
            "[1,    53] loss: 96.383\n",
            "[1,    54] loss: 98.187\n",
            "[1,    55] loss: 100.002\n",
            "[1,    56] loss: 101.817\n",
            "[1,    57] loss: 103.624\n",
            "[1,    58] loss: 105.448\n",
            "[1,    59] loss: 107.261\n",
            "[1,    60] loss: 109.068\n",
            "[1,    61] loss: 110.895\n",
            "[1,    62] loss: 112.721\n",
            "[1,    63] loss: 114.529\n",
            "[1,    64] loss: 116.355\n",
            "[1,    65] loss: 118.174\n",
            "[1,    66] loss: 119.976\n",
            "[1,    67] loss: 121.793\n",
            "[1,    68] loss: 123.610\n",
            "[1,    69] loss: 125.438\n",
            "[1,    70] loss: 127.254\n",
            "[1,    71] loss: 129.071\n",
            "[1,    72] loss: 130.888\n",
            "[1,    73] loss: 132.708\n",
            "[1,    74] loss: 134.535\n",
            "[1,    75] loss: 136.358\n",
            "[1,    76] loss: 138.175\n",
            "[1,    77] loss: 140.000\n",
            "[1,    78] loss: 141.819\n",
            "[1,    79] loss: 143.644\n",
            "[1,    80] loss: 145.463\n",
            "[1,    81] loss: 147.286\n",
            "[1,    82] loss: 149.094\n",
            "[1,    83] loss: 150.915\n",
            "[1,    84] loss: 152.726\n",
            "[1,    85] loss: 154.562\n",
            "[1,    86] loss: 156.388\n",
            "[1,    87] loss: 158.211\n",
            "[1,    88] loss: 160.018\n",
            "[1,    89] loss: 161.830\n",
            "[1,    90] loss: 163.647\n",
            "[1,    91] loss: 165.463\n",
            "[1,    92] loss: 167.279\n",
            "[1,    93] loss: 169.092\n",
            "[1,    94] loss: 170.905\n",
            "[1,    95] loss: 172.717\n",
            "[1,    96] loss: 174.546\n",
            "[1,    97] loss: 176.366\n",
            "[1,    98] loss: 178.180\n",
            "[1,    99] loss: 179.989\n",
            "[1,   100] loss: 181.803\n",
            "[1,   101] loss: 183.620\n",
            "[1,   102] loss: 185.435\n",
            "[1,   103] loss: 187.260\n",
            "[1,   104] loss: 189.086\n",
            "[1,   105] loss: 190.903\n",
            "[1,   106] loss: 192.718\n",
            "[1,   107] loss: 194.545\n",
            "[1,   108] loss: 196.378\n",
            "[1,   109] loss: 198.201\n",
            "[1,   110] loss: 200.007\n",
            "[1,   111] loss: 201.825\n",
            "[1,   112] loss: 203.654\n",
            "[1,   113] loss: 205.464\n",
            "[1,   114] loss: 207.289\n",
            "[1,   115] loss: 209.106\n",
            "[1,   116] loss: 210.940\n",
            "[1,   117] loss: 212.744\n",
            "[1,   118] loss: 214.532\n",
            "[1,   119] loss: 216.358\n",
            "[1,   120] loss: 218.173\n",
            "[1,   121] loss: 219.987\n",
            "[1,   122] loss: 221.799\n",
            "[1,   123] loss: 223.617\n",
            "[1,   124] loss: 225.435\n",
            "[1,   125] loss: 227.241\n",
            "[1,   126] loss: 229.057\n",
            "[1,   127] loss: 230.864\n",
            "[1,   128] loss: 232.687\n",
            "[1,   129] loss: 234.497\n",
            "[1,   130] loss: 236.311\n",
            "[1,   131] loss: 238.126\n",
            "[1,   132] loss: 239.951\n",
            "[1,   133] loss: 241.784\n",
            "[1,   134] loss: 243.600\n",
            "[1,   135] loss: 245.408\n",
            "[1,   136] loss: 247.233\n",
            "[1,   137] loss: 249.044\n",
            "[1,   138] loss: 250.857\n",
            "[1,   139] loss: 252.675\n",
            "[1,   140] loss: 254.492\n",
            "[1,   141] loss: 256.307\n",
            "[1,   142] loss: 258.144\n",
            "[1,   143] loss: 259.960\n",
            "[1,   144] loss: 261.778\n",
            "[1,   145] loss: 263.607\n",
            "[1,   146] loss: 265.412\n",
            "[1,   147] loss: 267.231\n",
            "[1,   148] loss: 269.051\n",
            "[1,   149] loss: 270.856\n",
            "[1,   150] loss: 272.660\n",
            "[1,   151] loss: 274.483\n",
            "[1,   152] loss: 276.312\n",
            "[1,   153] loss: 278.127\n",
            "[1,   154] loss: 279.940\n",
            "[1,   155] loss: 281.759\n",
            "[1,   156] loss: 283.573\n",
            "[1,   157] loss: 285.398\n",
            "[2,     2] loss: 3.619\n",
            "[2,     3] loss: 5.428\n",
            "[2,     4] loss: 7.235\n",
            "[2,     5] loss: 9.042\n",
            "[2,     6] loss: 10.849\n",
            "[2,     7] loss: 12.658\n",
            "[2,     8] loss: 14.472\n",
            "[2,     9] loss: 16.275\n",
            "[2,    10] loss: 18.087\n",
            "[2,    11] loss: 19.893\n",
            "[2,    12] loss: 21.705\n",
            "[2,    13] loss: 23.512\n",
            "[2,    14] loss: 25.326\n",
            "[2,    15] loss: 27.133\n",
            "[2,    16] loss: 28.938\n",
            "[2,    17] loss: 30.748\n",
            "[2,    18] loss: 32.553\n",
            "[2,    19] loss: 34.358\n",
            "[2,    20] loss: 36.163\n",
            "[2,    21] loss: 37.972\n",
            "[2,    22] loss: 39.778\n",
            "[2,    23] loss: 41.590\n",
            "[2,    24] loss: 43.397\n",
            "[2,    25] loss: 45.210\n",
            "[2,    26] loss: 47.025\n",
            "[2,    27] loss: 48.828\n",
            "[2,    28] loss: 50.632\n",
            "[2,    29] loss: 52.442\n",
            "[2,    30] loss: 54.249\n",
            "[2,    31] loss: 56.057\n",
            "[2,    32] loss: 57.868\n",
            "[2,    33] loss: 59.676\n",
            "[2,    34] loss: 61.485\n",
            "[2,    35] loss: 63.286\n",
            "[2,    36] loss: 65.089\n",
            "[2,    37] loss: 66.901\n",
            "[2,    38] loss: 68.708\n",
            "[2,    39] loss: 70.521\n",
            "[2,    40] loss: 72.318\n",
            "[2,    41] loss: 74.127\n",
            "[2,    42] loss: 75.941\n",
            "[2,    43] loss: 77.748\n",
            "[2,    44] loss: 79.562\n",
            "[2,    45] loss: 81.368\n",
            "[2,    46] loss: 83.176\n",
            "[2,    47] loss: 84.984\n",
            "[2,    48] loss: 86.794\n",
            "[2,    49] loss: 88.596\n",
            "[2,    50] loss: 90.406\n",
            "[2,    51] loss: 92.217\n",
            "[2,    52] loss: 94.026\n",
            "[2,    53] loss: 95.830\n",
            "[2,    54] loss: 97.636\n",
            "[2,    55] loss: 99.442\n",
            "[2,    56] loss: 101.252\n",
            "[2,    57] loss: 103.061\n",
            "[2,    58] loss: 104.868\n",
            "[2,    59] loss: 106.673\n",
            "[2,    60] loss: 108.476\n",
            "[2,    61] loss: 110.283\n",
            "[2,    62] loss: 112.095\n",
            "[2,    63] loss: 113.904\n",
            "[2,    64] loss: 115.715\n",
            "[2,    65] loss: 117.515\n",
            "[2,    66] loss: 119.319\n",
            "[2,    67] loss: 121.126\n",
            "[2,    68] loss: 122.932\n",
            "[2,    69] loss: 124.734\n",
            "[2,    70] loss: 126.543\n",
            "[2,    71] loss: 128.348\n",
            "[2,    72] loss: 130.155\n",
            "[2,    73] loss: 131.966\n",
            "[2,    74] loss: 133.771\n",
            "[2,    75] loss: 135.576\n",
            "[2,    76] loss: 137.382\n",
            "[2,    77] loss: 139.189\n",
            "[2,    78] loss: 140.993\n",
            "[2,    79] loss: 142.804\n",
            "[2,    80] loss: 144.611\n",
            "[2,    81] loss: 146.418\n",
            "[2,    82] loss: 148.229\n",
            "[2,    83] loss: 150.035\n",
            "[2,    84] loss: 151.840\n",
            "[2,    85] loss: 153.648\n",
            "[2,    86] loss: 155.459\n",
            "[2,    87] loss: 157.273\n",
            "[2,    88] loss: 159.086\n",
            "[2,    89] loss: 160.886\n",
            "[2,    90] loss: 162.693\n",
            "[2,    91] loss: 164.508\n",
            "[2,    92] loss: 166.318\n",
            "[2,    93] loss: 168.122\n",
            "[2,    94] loss: 169.926\n",
            "[2,    95] loss: 171.728\n",
            "[2,    96] loss: 173.534\n",
            "[2,    97] loss: 175.343\n",
            "[2,    98] loss: 177.146\n",
            "[2,    99] loss: 178.961\n",
            "[2,   100] loss: 180.771\n",
            "[2,   101] loss: 182.581\n",
            "[2,   102] loss: 184.386\n",
            "[2,   103] loss: 186.193\n",
            "[2,   104] loss: 188.002\n",
            "[2,   105] loss: 189.806\n",
            "[2,   106] loss: 191.618\n",
            "[2,   107] loss: 193.424\n",
            "[2,   108] loss: 195.235\n",
            "[2,   109] loss: 197.042\n",
            "[2,   110] loss: 198.851\n",
            "[2,   111] loss: 200.655\n",
            "[2,   112] loss: 202.463\n",
            "[2,   113] loss: 204.267\n",
            "[2,   114] loss: 206.078\n",
            "[2,   115] loss: 207.888\n",
            "[2,   116] loss: 209.701\n",
            "[2,   117] loss: 211.510\n",
            "[2,   118] loss: 213.316\n",
            "[2,   119] loss: 215.130\n",
            "[2,   120] loss: 216.935\n",
            "[2,   121] loss: 218.744\n",
            "[2,   122] loss: 220.555\n",
            "[2,   123] loss: 222.359\n",
            "[2,   124] loss: 224.173\n",
            "[2,   125] loss: 225.982\n",
            "[2,   126] loss: 227.793\n",
            "[2,   127] loss: 229.603\n",
            "[2,   128] loss: 231.414\n",
            "[2,   129] loss: 233.226\n",
            "[2,   130] loss: 235.038\n",
            "[2,   131] loss: 236.847\n",
            "[2,   132] loss: 238.650\n",
            "[2,   133] loss: 240.457\n",
            "[2,   134] loss: 242.261\n",
            "[2,   135] loss: 244.071\n",
            "[2,   136] loss: 245.876\n",
            "[2,   137] loss: 247.687\n",
            "[2,   138] loss: 249.496\n",
            "[2,   139] loss: 251.308\n",
            "[2,   140] loss: 253.122\n",
            "[2,   141] loss: 254.928\n",
            "[2,   142] loss: 256.736\n",
            "[2,   143] loss: 258.546\n",
            "[2,   144] loss: 260.359\n",
            "[2,   145] loss: 262.164\n",
            "[2,   146] loss: 263.974\n",
            "[2,   147] loss: 265.782\n",
            "[2,   148] loss: 267.596\n",
            "[2,   149] loss: 269.402\n",
            "[2,   150] loss: 271.210\n",
            "[2,   151] loss: 273.013\n",
            "[2,   152] loss: 274.826\n",
            "[2,   153] loss: 276.634\n",
            "[2,   154] loss: 278.446\n",
            "[2,   155] loss: 280.248\n",
            "[2,   156] loss: 282.054\n",
            "[2,   157] loss: 283.842\n",
            "[3,     2] loss: 3.614\n",
            "[3,     3] loss: 5.416\n",
            "[3,     4] loss: 7.229\n",
            "[3,     5] loss: 9.034\n",
            "[3,     6] loss: 10.841\n",
            "[3,     7] loss: 12.656\n",
            "[3,     8] loss: 14.469\n",
            "[3,     9] loss: 16.271\n",
            "[3,    10] loss: 18.078\n",
            "[3,    11] loss: 19.880\n",
            "[3,    12] loss: 21.687\n",
            "[3,    13] loss: 23.497\n",
            "[3,    14] loss: 25.306\n",
            "[3,    15] loss: 27.114\n",
            "[3,    16] loss: 28.924\n",
            "[3,    17] loss: 30.733\n",
            "[3,    18] loss: 32.542\n",
            "[3,    19] loss: 34.347\n",
            "[3,    20] loss: 36.152\n",
            "[3,    21] loss: 37.965\n",
            "[3,    22] loss: 39.774\n",
            "[3,    23] loss: 41.585\n",
            "[3,    24] loss: 43.396\n",
            "[3,    25] loss: 45.199\n",
            "[3,    26] loss: 47.008\n",
            "[3,    27] loss: 48.817\n",
            "[3,    28] loss: 50.622\n",
            "[3,    29] loss: 52.434\n",
            "[3,    30] loss: 54.242\n",
            "[3,    31] loss: 56.044\n",
            "[3,    32] loss: 57.851\n",
            "[3,    33] loss: 59.656\n",
            "[3,    34] loss: 61.464\n",
            "[3,    35] loss: 63.269\n",
            "[3,    36] loss: 65.073\n",
            "[3,    37] loss: 66.885\n",
            "[3,    38] loss: 68.695\n",
            "[3,    39] loss: 70.504\n",
            "[3,    40] loss: 72.309\n",
            "[3,    41] loss: 74.123\n",
            "[3,    42] loss: 75.927\n",
            "[3,    43] loss: 77.735\n",
            "[3,    44] loss: 79.540\n",
            "[3,    45] loss: 81.348\n",
            "[3,    46] loss: 83.153\n",
            "[3,    47] loss: 84.965\n",
            "[3,    48] loss: 86.770\n",
            "[3,    49] loss: 88.586\n",
            "[3,    50] loss: 90.394\n",
            "[3,    51] loss: 92.210\n",
            "[3,    52] loss: 94.016\n",
            "[3,    53] loss: 95.820\n",
            "[3,    54] loss: 97.625\n",
            "[3,    55] loss: 99.439\n",
            "[3,    56] loss: 101.241\n",
            "[3,    57] loss: 103.044\n",
            "[3,    58] loss: 104.852\n",
            "[3,    59] loss: 106.659\n",
            "[3,    60] loss: 108.468\n",
            "[3,    61] loss: 110.281\n",
            "[3,    62] loss: 112.087\n",
            "[3,    63] loss: 113.896\n",
            "[3,    64] loss: 115.703\n",
            "[3,    65] loss: 117.514\n",
            "[3,    66] loss: 119.327\n",
            "[3,    67] loss: 121.138\n",
            "[3,    68] loss: 122.949\n",
            "[3,    69] loss: 124.749\n",
            "[3,    70] loss: 126.560\n",
            "[3,    71] loss: 128.373\n",
            "[3,    72] loss: 130.179\n",
            "[3,    73] loss: 131.981\n",
            "[3,    74] loss: 133.791\n",
            "[3,    75] loss: 135.596\n",
            "[3,    76] loss: 137.406\n",
            "[3,    77] loss: 139.219\n",
            "[3,    78] loss: 141.024\n",
            "[3,    79] loss: 142.831\n",
            "[3,    80] loss: 144.642\n",
            "[3,    81] loss: 146.448\n",
            "[3,    82] loss: 148.250\n",
            "[3,    83] loss: 150.061\n",
            "[3,    84] loss: 151.870\n",
            "[3,    85] loss: 153.685\n",
            "[3,    86] loss: 155.493\n",
            "[3,    87] loss: 157.296\n",
            "[3,    88] loss: 159.101\n",
            "[3,    89] loss: 160.919\n",
            "[3,    90] loss: 162.720\n",
            "[3,    91] loss: 164.530\n",
            "[3,    92] loss: 166.342\n",
            "[3,    93] loss: 168.159\n",
            "[3,    94] loss: 169.968\n",
            "[3,    95] loss: 171.772\n",
            "[3,    96] loss: 173.580\n",
            "[3,    97] loss: 175.395\n",
            "[3,    98] loss: 177.210\n",
            "[3,    99] loss: 179.013\n",
            "[3,   100] loss: 180.826\n",
            "[3,   101] loss: 182.630\n",
            "[3,   102] loss: 184.437\n",
            "[3,   103] loss: 186.240\n",
            "[3,   104] loss: 188.047\n",
            "[3,   105] loss: 189.849\n",
            "[3,   106] loss: 191.656\n",
            "[3,   107] loss: 193.461\n",
            "[3,   108] loss: 195.262\n",
            "[3,   109] loss: 197.074\n",
            "[3,   110] loss: 198.884\n",
            "[3,   111] loss: 200.691\n",
            "[3,   112] loss: 202.499\n",
            "[3,   113] loss: 204.313\n",
            "[3,   114] loss: 206.126\n",
            "[3,   115] loss: 207.937\n",
            "[3,   116] loss: 209.749\n",
            "[3,   117] loss: 211.551\n",
            "[3,   118] loss: 213.362\n",
            "[3,   119] loss: 215.168\n",
            "[3,   120] loss: 216.976\n",
            "[3,   121] loss: 218.785\n",
            "[3,   122] loss: 220.588\n",
            "[3,   123] loss: 222.396\n",
            "[3,   124] loss: 224.205\n",
            "[3,   125] loss: 226.021\n",
            "[3,   126] loss: 227.828\n",
            "[3,   127] loss: 229.635\n",
            "[3,   128] loss: 231.448\n",
            "[3,   129] loss: 233.249\n",
            "[3,   130] loss: 235.056\n",
            "[3,   131] loss: 236.867\n",
            "[3,   132] loss: 238.673\n",
            "[3,   133] loss: 240.482\n",
            "[3,   134] loss: 242.291\n",
            "[3,   135] loss: 244.102\n",
            "[3,   136] loss: 245.909\n",
            "[3,   137] loss: 247.722\n",
            "[3,   138] loss: 249.536\n",
            "[3,   139] loss: 251.345\n",
            "[3,   140] loss: 253.156\n",
            "[3,   141] loss: 254.962\n",
            "[3,   142] loss: 256.770\n",
            "[3,   143] loss: 258.576\n",
            "[3,   144] loss: 260.386\n",
            "[3,   145] loss: 262.190\n",
            "[3,   146] loss: 263.990\n",
            "[3,   147] loss: 265.800\n",
            "[3,   148] loss: 267.609\n",
            "[3,   149] loss: 269.408\n",
            "[3,   150] loss: 271.210\n",
            "[3,   151] loss: 273.017\n",
            "[3,   152] loss: 274.826\n",
            "[3,   153] loss: 276.632\n",
            "[3,   154] loss: 278.441\n",
            "[3,   155] loss: 280.238\n",
            "[3,   156] loss: 282.048\n",
            "[3,   157] loss: 283.861\n",
            "[4,     2] loss: 3.609\n",
            "[4,     3] loss: 5.411\n",
            "[4,     4] loss: 7.218\n",
            "[4,     5] loss: 9.023\n",
            "[4,     6] loss: 10.824\n",
            "[4,     7] loss: 12.634\n",
            "[4,     8] loss: 14.444\n",
            "[4,     9] loss: 16.256\n",
            "[4,    10] loss: 18.065\n",
            "[4,    11] loss: 19.874\n",
            "[4,    12] loss: 21.682\n",
            "[4,    13] loss: 23.492\n",
            "[4,    14] loss: 25.294\n",
            "[4,    15] loss: 27.099\n",
            "[4,    16] loss: 28.908\n",
            "[4,    17] loss: 30.717\n",
            "[4,    18] loss: 32.520\n",
            "[4,    19] loss: 34.335\n",
            "[4,    20] loss: 36.147\n",
            "[4,    21] loss: 37.960\n",
            "[4,    22] loss: 39.764\n",
            "[4,    23] loss: 41.570\n",
            "[4,    24] loss: 43.370\n",
            "[4,    25] loss: 45.180\n",
            "[4,    26] loss: 46.990\n",
            "[4,    27] loss: 48.802\n",
            "[4,    28] loss: 50.605\n",
            "[4,    29] loss: 52.408\n",
            "[4,    30] loss: 54.213\n",
            "[4,    31] loss: 56.020\n",
            "[4,    32] loss: 57.829\n",
            "[4,    33] loss: 59.632\n",
            "[4,    34] loss: 61.442\n",
            "[4,    35] loss: 63.255\n",
            "[4,    36] loss: 65.058\n",
            "[4,    37] loss: 66.870\n",
            "[4,    38] loss: 68.678\n",
            "[4,    39] loss: 70.492\n",
            "[4,    40] loss: 72.307\n",
            "[4,    41] loss: 74.116\n",
            "[4,    42] loss: 75.929\n",
            "[4,    43] loss: 77.731\n",
            "[4,    44] loss: 79.535\n",
            "[4,    45] loss: 81.343\n",
            "[4,    46] loss: 83.152\n",
            "[4,    47] loss: 84.959\n",
            "[4,    48] loss: 86.766\n",
            "[4,    49] loss: 88.571\n",
            "[4,    50] loss: 90.384\n",
            "[4,    51] loss: 92.187\n",
            "[4,    52] loss: 93.992\n",
            "[4,    53] loss: 95.805\n",
            "[4,    54] loss: 97.626\n",
            "[4,    55] loss: 99.425\n",
            "[4,    56] loss: 101.230\n",
            "[4,    57] loss: 103.036\n",
            "[4,    58] loss: 104.843\n",
            "[4,    59] loss: 106.648\n",
            "[4,    60] loss: 108.460\n",
            "[4,    61] loss: 110.271\n",
            "[4,    62] loss: 112.080\n",
            "[4,    63] loss: 113.891\n",
            "[4,    64] loss: 115.701\n",
            "[4,    65] loss: 117.515\n",
            "[4,    66] loss: 119.328\n",
            "[4,    67] loss: 121.132\n",
            "[4,    68] loss: 122.940\n",
            "[4,    69] loss: 124.753\n",
            "[4,    70] loss: 126.566\n",
            "[4,    71] loss: 128.368\n",
            "[4,    72] loss: 130.172\n",
            "[4,    73] loss: 131.982\n",
            "[4,    74] loss: 133.783\n",
            "[4,    75] loss: 135.587\n",
            "[4,    76] loss: 137.393\n",
            "[4,    77] loss: 139.196\n",
            "[4,    78] loss: 141.003\n",
            "[4,    79] loss: 142.811\n",
            "[4,    80] loss: 144.618\n",
            "[4,    81] loss: 146.425\n",
            "[4,    82] loss: 148.233\n",
            "[4,    83] loss: 150.045\n",
            "[4,    84] loss: 151.852\n",
            "[4,    85] loss: 153.654\n",
            "[4,    86] loss: 155.463\n",
            "[4,    87] loss: 157.279\n",
            "[4,    88] loss: 159.087\n",
            "[4,    89] loss: 160.892\n",
            "[4,    90] loss: 162.700\n",
            "[4,    91] loss: 164.501\n",
            "[4,    92] loss: 166.309\n",
            "[4,    93] loss: 168.121\n",
            "[4,    94] loss: 169.924\n",
            "[4,    95] loss: 171.737\n",
            "[4,    96] loss: 173.547\n",
            "[4,    97] loss: 175.353\n",
            "[4,    98] loss: 177.158\n",
            "[4,    99] loss: 178.966\n",
            "[4,   100] loss: 180.773\n",
            "[4,   101] loss: 182.581\n",
            "[4,   102] loss: 184.386\n",
            "[4,   103] loss: 186.195\n",
            "[4,   104] loss: 188.008\n",
            "[4,   105] loss: 189.821\n",
            "[4,   106] loss: 191.633\n",
            "[4,   107] loss: 193.445\n",
            "[4,   108] loss: 195.255\n",
            "[4,   109] loss: 197.066\n",
            "[4,   110] loss: 198.879\n",
            "[4,   111] loss: 200.684\n",
            "[4,   112] loss: 202.487\n",
            "[4,   113] loss: 204.289\n",
            "[4,   114] loss: 206.099\n",
            "[4,   115] loss: 207.911\n",
            "[4,   116] loss: 209.723\n",
            "[4,   117] loss: 211.530\n",
            "[4,   118] loss: 213.344\n",
            "[4,   119] loss: 215.147\n",
            "[4,   120] loss: 216.954\n",
            "[4,   121] loss: 218.763\n",
            "[4,   122] loss: 220.574\n",
            "[4,   123] loss: 222.384\n",
            "[4,   124] loss: 224.191\n",
            "[4,   125] loss: 225.992\n",
            "[4,   126] loss: 227.803\n",
            "[4,   127] loss: 229.602\n",
            "[4,   128] loss: 231.408\n",
            "[4,   129] loss: 233.215\n",
            "[4,   130] loss: 235.026\n",
            "[4,   131] loss: 236.828\n",
            "[4,   132] loss: 238.638\n",
            "[4,   133] loss: 240.445\n",
            "[4,   134] loss: 242.244\n",
            "[4,   135] loss: 244.051\n",
            "[4,   136] loss: 245.855\n",
            "[4,   137] loss: 247.657\n",
            "[4,   138] loss: 249.466\n",
            "[4,   139] loss: 251.281\n",
            "[4,   140] loss: 253.094\n",
            "[4,   141] loss: 254.902\n",
            "[4,   142] loss: 256.715\n",
            "[4,   143] loss: 258.528\n",
            "[4,   144] loss: 260.345\n",
            "[4,   145] loss: 262.153\n",
            "[4,   146] loss: 263.959\n",
            "[4,   147] loss: 265.772\n",
            "[4,   148] loss: 267.579\n",
            "[4,   149] loss: 269.389\n",
            "[4,   150] loss: 271.195\n",
            "[4,   151] loss: 273.006\n",
            "[4,   152] loss: 274.821\n",
            "[4,   153] loss: 276.621\n",
            "[4,   154] loss: 278.427\n",
            "[4,   155] loss: 280.241\n",
            "[4,   156] loss: 282.050\n",
            "[4,   157] loss: 283.856\n",
            "[5,     2] loss: 3.617\n",
            "[5,     3] loss: 5.422\n",
            "[5,     4] loss: 7.227\n",
            "[5,     5] loss: 9.036\n",
            "[5,     6] loss: 10.837\n",
            "[5,     7] loss: 12.637\n",
            "[5,     8] loss: 14.448\n",
            "[5,     9] loss: 16.256\n",
            "[5,    10] loss: 18.066\n",
            "[5,    11] loss: 19.876\n",
            "[5,    12] loss: 21.686\n",
            "[5,    13] loss: 23.492\n",
            "[5,    14] loss: 25.301\n",
            "[5,    15] loss: 27.106\n",
            "[5,    16] loss: 28.914\n",
            "[5,    17] loss: 30.722\n",
            "[5,    18] loss: 32.525\n",
            "[5,    19] loss: 34.328\n",
            "[5,    20] loss: 36.138\n",
            "[5,    21] loss: 37.952\n",
            "[5,    22] loss: 39.761\n",
            "[5,    23] loss: 41.569\n",
            "[5,    24] loss: 43.377\n",
            "[5,    25] loss: 45.182\n",
            "[5,    26] loss: 46.983\n",
            "[5,    27] loss: 48.787\n",
            "[5,    28] loss: 50.600\n",
            "[5,    29] loss: 52.403\n",
            "[5,    30] loss: 54.215\n",
            "[5,    31] loss: 56.028\n",
            "[5,    32] loss: 57.838\n",
            "[5,    33] loss: 59.648\n",
            "[5,    34] loss: 61.449\n",
            "[5,    35] loss: 63.258\n",
            "[5,    36] loss: 65.057\n",
            "[5,    37] loss: 66.868\n",
            "[5,    38] loss: 68.673\n",
            "[5,    39] loss: 70.481\n",
            "[5,    40] loss: 72.287\n",
            "[5,    41] loss: 74.096\n",
            "[5,    42] loss: 75.905\n",
            "[5,    43] loss: 77.717\n",
            "[5,    44] loss: 79.523\n",
            "[5,    45] loss: 81.321\n",
            "[5,    46] loss: 83.130\n",
            "[5,    47] loss: 84.940\n",
            "[5,    48] loss: 86.749\n",
            "[5,    49] loss: 88.557\n",
            "[5,    50] loss: 90.361\n",
            "[5,    51] loss: 92.167\n",
            "[5,    52] loss: 93.976\n",
            "[5,    53] loss: 95.787\n",
            "[5,    54] loss: 97.593\n",
            "[5,    55] loss: 99.389\n",
            "[5,    56] loss: 101.194\n",
            "[5,    57] loss: 103.004\n",
            "[5,    58] loss: 104.813\n",
            "[5,    59] loss: 106.621\n",
            "[5,    60] loss: 108.437\n",
            "[5,    61] loss: 110.246\n",
            "[5,    62] loss: 112.050\n",
            "[5,    63] loss: 113.854\n",
            "[5,    64] loss: 115.654\n",
            "[5,    65] loss: 117.465\n",
            "[5,    66] loss: 119.279\n",
            "[5,    67] loss: 121.086\n",
            "[5,    68] loss: 122.895\n",
            "[5,    69] loss: 124.702\n",
            "[5,    70] loss: 126.512\n",
            "[5,    71] loss: 128.321\n",
            "[5,    72] loss: 130.125\n",
            "[5,    73] loss: 131.933\n",
            "[5,    74] loss: 133.744\n",
            "[5,    75] loss: 135.537\n",
            "[5,    76] loss: 137.349\n",
            "[5,    77] loss: 139.165\n",
            "[5,    78] loss: 140.982\n",
            "[5,    79] loss: 142.793\n",
            "[5,    80] loss: 144.600\n",
            "[5,    81] loss: 146.406\n",
            "[5,    82] loss: 148.214\n",
            "[5,    83] loss: 150.016\n",
            "[5,    84] loss: 151.823\n",
            "[5,    85] loss: 153.639\n",
            "[5,    86] loss: 155.449\n",
            "[5,    87] loss: 157.252\n",
            "[5,    88] loss: 159.062\n",
            "[5,    89] loss: 160.873\n",
            "[5,    90] loss: 162.679\n",
            "[5,    91] loss: 164.490\n",
            "[5,    92] loss: 166.299\n",
            "[5,    93] loss: 168.104\n",
            "[5,    94] loss: 169.918\n",
            "[5,    95] loss: 171.723\n",
            "[5,    96] loss: 173.529\n",
            "[5,    97] loss: 175.334\n",
            "[5,    98] loss: 177.145\n",
            "[5,    99] loss: 178.958\n",
            "[5,   100] loss: 180.761\n",
            "[5,   101] loss: 182.564\n",
            "[5,   102] loss: 184.372\n",
            "[5,   103] loss: 186.177\n",
            "[5,   104] loss: 187.983\n",
            "[5,   105] loss: 189.791\n",
            "[5,   106] loss: 191.602\n",
            "[5,   107] loss: 193.407\n",
            "[5,   108] loss: 195.218\n",
            "[5,   109] loss: 197.017\n",
            "[5,   110] loss: 198.825\n",
            "[5,   111] loss: 200.634\n",
            "[5,   112] loss: 202.444\n",
            "[5,   113] loss: 204.251\n",
            "[5,   114] loss: 206.062\n",
            "[5,   115] loss: 207.868\n",
            "[5,   116] loss: 209.670\n",
            "[5,   117] loss: 211.476\n",
            "[5,   118] loss: 213.287\n",
            "[5,   119] loss: 215.092\n",
            "[5,   120] loss: 216.907\n",
            "[5,   121] loss: 218.714\n",
            "[5,   122] loss: 220.517\n",
            "[5,   123] loss: 222.329\n",
            "[5,   124] loss: 224.140\n",
            "[5,   125] loss: 225.954\n",
            "[5,   126] loss: 227.765\n",
            "[5,   127] loss: 229.571\n",
            "[5,   128] loss: 231.376\n",
            "[5,   129] loss: 233.190\n",
            "[5,   130] loss: 234.997\n",
            "[5,   131] loss: 236.807\n",
            "[5,   132] loss: 238.614\n",
            "[5,   133] loss: 240.421\n",
            "[5,   134] loss: 242.231\n",
            "[5,   135] loss: 244.044\n",
            "[5,   136] loss: 245.858\n",
            "[5,   137] loss: 247.673\n",
            "[5,   138] loss: 249.482\n",
            "[5,   139] loss: 251.295\n",
            "[5,   140] loss: 253.106\n",
            "[5,   141] loss: 254.910\n",
            "[5,   142] loss: 256.723\n",
            "[5,   143] loss: 258.526\n",
            "[5,   144] loss: 260.330\n",
            "[5,   145] loss: 262.132\n",
            "[5,   146] loss: 263.940\n",
            "[5,   147] loss: 265.753\n",
            "[5,   148] loss: 267.560\n",
            "[5,   149] loss: 269.372\n",
            "[5,   150] loss: 271.184\n",
            "[5,   151] loss: 272.995\n",
            "[5,   152] loss: 274.808\n",
            "[5,   153] loss: 276.618\n",
            "[5,   154] loss: 278.428\n",
            "[5,   155] loss: 280.238\n",
            "[5,   156] loss: 282.051\n",
            "[5,   157] loss: 283.852\n",
            "[6,     2] loss: 3.617\n",
            "[6,     3] loss: 5.415\n",
            "[6,     4] loss: 7.213\n",
            "[6,     5] loss: 9.015\n",
            "[6,     6] loss: 10.819\n",
            "[6,     7] loss: 12.628\n",
            "[6,     8] loss: 14.434\n",
            "[6,     9] loss: 16.237\n",
            "[6,    10] loss: 18.043\n",
            "[6,    11] loss: 19.848\n",
            "[6,    12] loss: 21.657\n",
            "[6,    13] loss: 23.468\n",
            "[6,    14] loss: 25.272\n",
            "[6,    15] loss: 27.074\n",
            "[6,    16] loss: 28.878\n",
            "[6,    17] loss: 30.684\n",
            "[6,    18] loss: 32.502\n",
            "[6,    19] loss: 34.306\n",
            "[6,    20] loss: 36.118\n",
            "[6,    21] loss: 37.923\n",
            "[6,    22] loss: 39.735\n",
            "[6,    23] loss: 41.544\n",
            "[6,    24] loss: 43.351\n",
            "[6,    25] loss: 45.160\n",
            "[6,    26] loss: 46.965\n",
            "[6,    27] loss: 48.777\n",
            "[6,    28] loss: 50.588\n",
            "[6,    29] loss: 52.393\n",
            "[6,    30] loss: 54.205\n",
            "[6,    31] loss: 56.014\n",
            "[6,    32] loss: 57.817\n",
            "[6,    33] loss: 59.626\n",
            "[6,    34] loss: 61.434\n",
            "[6,    35] loss: 63.246\n",
            "[6,    36] loss: 65.056\n",
            "[6,    37] loss: 66.858\n",
            "[6,    38] loss: 68.665\n",
            "[6,    39] loss: 70.477\n",
            "[6,    40] loss: 72.282\n",
            "[6,    41] loss: 74.093\n",
            "[6,    42] loss: 75.899\n",
            "[6,    43] loss: 77.703\n",
            "[6,    44] loss: 79.513\n",
            "[6,    45] loss: 81.320\n",
            "[6,    46] loss: 83.126\n",
            "[6,    47] loss: 84.936\n",
            "[6,    48] loss: 86.748\n",
            "[6,    49] loss: 88.555\n",
            "[6,    50] loss: 90.369\n",
            "[6,    51] loss: 92.186\n",
            "[6,    52] loss: 93.998\n",
            "[6,    53] loss: 95.803\n",
            "[6,    54] loss: 97.612\n",
            "[6,    55] loss: 99.423\n",
            "[6,    56] loss: 101.225\n",
            "[6,    57] loss: 103.029\n",
            "[6,    58] loss: 104.840\n",
            "[6,    59] loss: 106.650\n",
            "[6,    60] loss: 108.458\n",
            "[6,    61] loss: 110.271\n",
            "[6,    62] loss: 112.080\n",
            "[6,    63] loss: 113.885\n",
            "[6,    64] loss: 115.694\n",
            "[6,    65] loss: 117.505\n",
            "[6,    66] loss: 119.308\n",
            "[6,    67] loss: 121.118\n",
            "[6,    68] loss: 122.920\n",
            "[6,    69] loss: 124.727\n",
            "[6,    70] loss: 126.532\n",
            "[6,    71] loss: 128.345\n",
            "[6,    72] loss: 130.151\n",
            "[6,    73] loss: 131.966\n",
            "[6,    74] loss: 133.778\n",
            "[6,    75] loss: 135.584\n",
            "[6,    76] loss: 137.395\n",
            "[6,    77] loss: 139.207\n",
            "[6,    78] loss: 141.015\n",
            "[6,    79] loss: 142.822\n",
            "[6,    80] loss: 144.633\n",
            "[6,    81] loss: 146.442\n",
            "[6,    82] loss: 148.252\n",
            "[6,    83] loss: 150.063\n",
            "[6,    84] loss: 151.866\n",
            "[6,    85] loss: 153.667\n",
            "[6,    86] loss: 155.477\n",
            "[6,    87] loss: 157.286\n",
            "[6,    88] loss: 159.097\n",
            "[6,    89] loss: 160.901\n",
            "[6,    90] loss: 162.708\n",
            "[6,    91] loss: 164.521\n",
            "[6,    92] loss: 166.334\n",
            "[6,    93] loss: 168.142\n",
            "[6,    94] loss: 169.950\n",
            "[6,    95] loss: 171.762\n",
            "[6,    96] loss: 173.570\n",
            "[6,    97] loss: 175.373\n",
            "[6,    98] loss: 177.183\n",
            "[6,    99] loss: 178.993\n",
            "[6,   100] loss: 180.801\n",
            "[6,   101] loss: 182.613\n",
            "[6,   102] loss: 184.415\n",
            "[6,   103] loss: 186.215\n",
            "[6,   104] loss: 188.018\n",
            "[6,   105] loss: 189.832\n",
            "[6,   106] loss: 191.636\n",
            "[6,   107] loss: 193.449\n",
            "[6,   108] loss: 195.257\n",
            "[6,   109] loss: 197.065\n",
            "[6,   110] loss: 198.879\n",
            "[6,   111] loss: 200.686\n",
            "[6,   112] loss: 202.491\n",
            "[6,   113] loss: 204.298\n",
            "[6,   114] loss: 206.112\n",
            "[6,   115] loss: 207.920\n",
            "[6,   116] loss: 209.723\n",
            "[6,   117] loss: 211.534\n",
            "[6,   118] loss: 213.344\n",
            "[6,   119] loss: 215.148\n",
            "[6,   120] loss: 216.957\n",
            "[6,   121] loss: 218.766\n",
            "[6,   122] loss: 220.574\n",
            "[6,   123] loss: 222.383\n",
            "[6,   124] loss: 224.193\n",
            "[6,   125] loss: 226.005\n",
            "[6,   126] loss: 227.811\n",
            "[6,   127] loss: 229.622\n",
            "[6,   128] loss: 231.430\n",
            "[6,   129] loss: 233.238\n",
            "[6,   130] loss: 235.042\n",
            "[6,   131] loss: 236.845\n",
            "[6,   132] loss: 238.657\n",
            "[6,   133] loss: 240.466\n",
            "[6,   134] loss: 242.270\n",
            "[6,   135] loss: 244.078\n",
            "[6,   136] loss: 245.895\n",
            "[6,   137] loss: 247.691\n",
            "[6,   138] loss: 249.501\n",
            "[6,   139] loss: 251.309\n",
            "[6,   140] loss: 253.122\n",
            "[6,   141] loss: 254.930\n",
            "[6,   142] loss: 256.743\n",
            "[6,   143] loss: 258.557\n",
            "[6,   144] loss: 260.360\n",
            "[6,   145] loss: 262.168\n",
            "[6,   146] loss: 263.969\n",
            "[6,   147] loss: 265.766\n",
            "[6,   148] loss: 267.576\n",
            "[6,   149] loss: 269.384\n",
            "[6,   150] loss: 271.189\n",
            "[6,   151] loss: 272.998\n",
            "[6,   152] loss: 274.813\n",
            "[6,   153] loss: 276.625\n",
            "[6,   154] loss: 278.430\n",
            "[6,   155] loss: 280.243\n",
            "[6,   156] loss: 282.050\n",
            "[6,   157] loss: 283.856\n",
            "[7,     2] loss: 3.616\n",
            "[7,     3] loss: 5.424\n",
            "[7,     4] loss: 7.237\n",
            "[7,     5] loss: 9.045\n",
            "[7,     6] loss: 10.857\n",
            "[7,     7] loss: 12.660\n",
            "[7,     8] loss: 14.468\n",
            "[7,     9] loss: 16.279\n",
            "[7,    10] loss: 18.085\n",
            "[7,    11] loss: 19.888\n",
            "[7,    12] loss: 21.689\n",
            "[7,    13] loss: 23.496\n",
            "[7,    14] loss: 25.298\n",
            "[7,    15] loss: 27.105\n",
            "[7,    16] loss: 28.911\n",
            "[7,    17] loss: 30.721\n",
            "[7,    18] loss: 32.533\n",
            "[7,    19] loss: 34.340\n",
            "[7,    20] loss: 36.145\n",
            "[7,    21] loss: 37.954\n",
            "[7,    22] loss: 39.762\n",
            "[7,    23] loss: 41.570\n",
            "[7,    24] loss: 43.376\n",
            "[7,    25] loss: 45.181\n",
            "[7,    26] loss: 46.987\n",
            "[7,    27] loss: 48.797\n",
            "[7,    28] loss: 50.604\n",
            "[7,    29] loss: 52.414\n",
            "[7,    30] loss: 54.224\n",
            "[7,    31] loss: 56.037\n",
            "[7,    32] loss: 57.847\n",
            "[7,    33] loss: 59.648\n",
            "[7,    34] loss: 61.456\n",
            "[7,    35] loss: 63.268\n",
            "[7,    36] loss: 65.076\n",
            "[7,    37] loss: 66.886\n",
            "[7,    38] loss: 68.696\n",
            "[7,    39] loss: 70.509\n",
            "[7,    40] loss: 72.312\n",
            "[7,    41] loss: 74.119\n",
            "[7,    42] loss: 75.928\n",
            "[7,    43] loss: 77.734\n",
            "[7,    44] loss: 79.540\n",
            "[7,    45] loss: 81.350\n",
            "[7,    46] loss: 83.167\n",
            "[7,    47] loss: 84.978\n",
            "[7,    48] loss: 86.787\n",
            "[7,    49] loss: 88.595\n",
            "[7,    50] loss: 90.409\n",
            "[7,    51] loss: 92.213\n",
            "[7,    52] loss: 94.021\n",
            "[7,    53] loss: 95.828\n",
            "[7,    54] loss: 97.638\n",
            "[7,    55] loss: 99.455\n",
            "[7,    56] loss: 101.261\n",
            "[7,    57] loss: 103.071\n",
            "[7,    58] loss: 104.881\n",
            "[7,    59] loss: 106.691\n",
            "[7,    60] loss: 108.498\n",
            "[7,    61] loss: 110.304\n",
            "[7,    62] loss: 112.113\n",
            "[7,    63] loss: 113.920\n",
            "[7,    64] loss: 115.730\n",
            "[7,    65] loss: 117.536\n",
            "[7,    66] loss: 119.353\n",
            "[7,    67] loss: 121.163\n",
            "[7,    68] loss: 122.974\n",
            "[7,    69] loss: 124.785\n",
            "[7,    70] loss: 126.597\n",
            "[7,    71] loss: 128.400\n",
            "[7,    72] loss: 130.209\n",
            "[7,    73] loss: 132.020\n",
            "[7,    74] loss: 133.831\n",
            "[7,    75] loss: 135.640\n",
            "[7,    76] loss: 137.446\n",
            "[7,    77] loss: 139.249\n",
            "[7,    78] loss: 141.061\n",
            "[7,    79] loss: 142.867\n",
            "[7,    80] loss: 144.678\n",
            "[7,    81] loss: 146.485\n",
            "[7,    82] loss: 148.293\n",
            "[7,    83] loss: 150.106\n",
            "[7,    84] loss: 151.920\n",
            "[7,    85] loss: 153.730\n",
            "[7,    86] loss: 155.544\n",
            "[7,    87] loss: 157.347\n",
            "[7,    88] loss: 159.154\n",
            "[7,    89] loss: 160.970\n",
            "[7,    90] loss: 162.781\n",
            "[7,    91] loss: 164.581\n",
            "[7,    92] loss: 166.384\n",
            "[7,    93] loss: 168.192\n",
            "[7,    94] loss: 169.999\n",
            "[7,    95] loss: 171.806\n",
            "[7,    96] loss: 173.616\n",
            "[7,    97] loss: 175.424\n",
            "[7,    98] loss: 177.235\n",
            "[7,    99] loss: 179.047\n",
            "[7,   100] loss: 180.850\n",
            "[7,   101] loss: 182.658\n",
            "[7,   102] loss: 184.471\n",
            "[7,   103] loss: 186.279\n",
            "[7,   104] loss: 188.090\n",
            "[7,   105] loss: 189.890\n",
            "[7,   106] loss: 191.701\n",
            "[7,   107] loss: 193.505\n",
            "[7,   108] loss: 195.311\n",
            "[7,   109] loss: 197.114\n",
            "[7,   110] loss: 198.915\n",
            "[7,   111] loss: 200.727\n",
            "[7,   112] loss: 202.532\n",
            "[7,   113] loss: 204.334\n",
            "[7,   114] loss: 206.143\n",
            "[7,   115] loss: 207.949\n",
            "[7,   116] loss: 209.755\n",
            "[7,   117] loss: 211.565\n",
            "[7,   118] loss: 213.370\n",
            "[7,   119] loss: 215.173\n",
            "[7,   120] loss: 216.977\n",
            "[7,   121] loss: 218.775\n",
            "[7,   122] loss: 220.587\n",
            "[7,   123] loss: 222.397\n",
            "[7,   124] loss: 224.209\n",
            "[7,   125] loss: 226.017\n",
            "[7,   126] loss: 227.826\n",
            "[7,   127] loss: 229.629\n",
            "[7,   128] loss: 231.426\n",
            "[7,   129] loss: 233.236\n",
            "[7,   130] loss: 235.042\n",
            "[7,   131] loss: 236.850\n",
            "[7,   132] loss: 238.659\n",
            "[7,   133] loss: 240.461\n",
            "[7,   134] loss: 242.260\n",
            "[7,   135] loss: 244.070\n",
            "[7,   136] loss: 245.885\n",
            "[7,   137] loss: 247.691\n",
            "[7,   138] loss: 249.502\n",
            "[7,   139] loss: 251.310\n",
            "[7,   140] loss: 253.120\n",
            "[7,   141] loss: 254.922\n",
            "[7,   142] loss: 256.727\n",
            "[7,   143] loss: 258.536\n",
            "[7,   144] loss: 260.344\n",
            "[7,   145] loss: 262.156\n",
            "[7,   146] loss: 263.963\n",
            "[7,   147] loss: 265.773\n",
            "[7,   148] loss: 267.575\n",
            "[7,   149] loss: 269.388\n",
            "[7,   150] loss: 271.196\n",
            "[7,   151] loss: 273.001\n",
            "[7,   152] loss: 274.812\n",
            "[7,   153] loss: 276.622\n",
            "[7,   154] loss: 278.428\n",
            "[7,   155] loss: 280.237\n",
            "[7,   156] loss: 282.048\n",
            "[7,   157] loss: 283.860\n",
            "[8,     2] loss: 3.616\n",
            "[8,     3] loss: 5.429\n",
            "[8,     4] loss: 7.237\n",
            "[8,     5] loss: 9.043\n",
            "[8,     6] loss: 10.848\n",
            "[8,     7] loss: 12.659\n",
            "[8,     8] loss: 14.466\n",
            "[8,     9] loss: 16.281\n",
            "[8,    10] loss: 18.095\n",
            "[8,    11] loss: 19.895\n",
            "[8,    12] loss: 21.703\n",
            "[8,    13] loss: 23.509\n",
            "[8,    14] loss: 25.315\n",
            "[8,    15] loss: 27.118\n",
            "[8,    16] loss: 28.931\n",
            "[8,    17] loss: 30.741\n",
            "[8,    18] loss: 32.550\n",
            "[8,    19] loss: 34.355\n",
            "[8,    20] loss: 36.159\n",
            "[8,    21] loss: 37.964\n",
            "[8,    22] loss: 39.772\n",
            "[8,    23] loss: 41.581\n",
            "[8,    24] loss: 43.376\n",
            "[8,    25] loss: 45.187\n",
            "[8,    26] loss: 46.992\n",
            "[8,    27] loss: 48.798\n",
            "[8,    28] loss: 50.611\n",
            "[8,    29] loss: 52.419\n",
            "[8,    30] loss: 54.232\n",
            "[8,    31] loss: 56.040\n",
            "[8,    32] loss: 57.847\n",
            "[8,    33] loss: 59.658\n",
            "[8,    34] loss: 61.471\n",
            "[8,    35] loss: 63.275\n",
            "[8,    36] loss: 65.087\n",
            "[8,    37] loss: 66.895\n",
            "[8,    38] loss: 68.702\n",
            "[8,    39] loss: 70.512\n",
            "[8,    40] loss: 72.317\n",
            "[8,    41] loss: 74.124\n",
            "[8,    42] loss: 75.928\n",
            "[8,    43] loss: 77.731\n",
            "[8,    44] loss: 79.534\n",
            "[8,    45] loss: 81.352\n",
            "[8,    46] loss: 83.152\n",
            "[8,    47] loss: 84.961\n",
            "[8,    48] loss: 86.761\n",
            "[8,    49] loss: 88.565\n",
            "[8,    50] loss: 90.375\n",
            "[8,    51] loss: 92.179\n",
            "[8,    52] loss: 93.984\n",
            "[8,    53] loss: 95.801\n",
            "[8,    54] loss: 97.604\n",
            "[8,    55] loss: 99.413\n",
            "[8,    56] loss: 101.223\n",
            "[8,    57] loss: 103.030\n",
            "[8,    58] loss: 104.840\n",
            "[8,    59] loss: 106.654\n",
            "[8,    60] loss: 108.463\n",
            "[8,    61] loss: 110.265\n",
            "[8,    62] loss: 112.077\n",
            "[8,    63] loss: 113.892\n",
            "[8,    64] loss: 115.694\n",
            "[8,    65] loss: 117.500\n",
            "[8,    66] loss: 119.311\n",
            "[8,    67] loss: 121.120\n",
            "[8,    68] loss: 122.933\n",
            "[8,    69] loss: 124.746\n",
            "[8,    70] loss: 126.555\n",
            "[8,    71] loss: 128.365\n",
            "[8,    72] loss: 130.178\n",
            "[8,    73] loss: 131.981\n",
            "[8,    74] loss: 133.786\n",
            "[8,    75] loss: 135.592\n",
            "[8,    76] loss: 137.396\n",
            "[8,    77] loss: 139.199\n",
            "[8,    78] loss: 141.013\n",
            "[8,    79] loss: 142.818\n",
            "[8,    80] loss: 144.628\n",
            "[8,    81] loss: 146.438\n",
            "[8,    82] loss: 148.248\n",
            "[8,    83] loss: 150.061\n",
            "[8,    84] loss: 151.873\n",
            "[8,    85] loss: 153.681\n",
            "[8,    86] loss: 155.483\n",
            "[8,    87] loss: 157.286\n",
            "[8,    88] loss: 159.093\n",
            "[8,    89] loss: 160.909\n",
            "[8,    90] loss: 162.715\n",
            "[8,    91] loss: 164.529\n",
            "[8,    92] loss: 166.336\n",
            "[8,    93] loss: 168.145\n",
            "[8,    94] loss: 169.961\n",
            "[8,    95] loss: 171.773\n",
            "[8,    96] loss: 173.585\n",
            "[8,    97] loss: 175.398\n",
            "[8,    98] loss: 177.204\n",
            "[8,    99] loss: 179.007\n",
            "[8,   100] loss: 180.818\n",
            "[8,   101] loss: 182.625\n",
            "[8,   102] loss: 184.432\n",
            "[8,   103] loss: 186.238\n",
            "[8,   104] loss: 188.046\n",
            "[8,   105] loss: 189.856\n",
            "[8,   106] loss: 191.664\n",
            "[8,   107] loss: 193.468\n",
            "[8,   108] loss: 195.278\n",
            "[8,   109] loss: 197.086\n",
            "[8,   110] loss: 198.899\n",
            "[8,   111] loss: 200.698\n",
            "[8,   112] loss: 202.505\n",
            "[8,   113] loss: 204.310\n",
            "[8,   114] loss: 206.123\n",
            "[8,   115] loss: 207.934\n",
            "[8,   116] loss: 209.740\n",
            "[8,   117] loss: 211.549\n",
            "[8,   118] loss: 213.359\n",
            "[8,   119] loss: 215.164\n",
            "[8,   120] loss: 216.974\n",
            "[8,   121] loss: 218.784\n",
            "[8,   122] loss: 220.595\n",
            "[8,   123] loss: 222.404\n",
            "[8,   124] loss: 224.215\n",
            "[8,   125] loss: 226.013\n",
            "[8,   126] loss: 227.815\n",
            "[8,   127] loss: 229.623\n",
            "[8,   128] loss: 231.431\n",
            "[8,   129] loss: 233.238\n",
            "[8,   130] loss: 235.046\n",
            "[8,   131] loss: 236.848\n",
            "[8,   132] loss: 238.661\n",
            "[8,   133] loss: 240.467\n",
            "[8,   134] loss: 242.278\n",
            "[8,   135] loss: 244.090\n",
            "[8,   136] loss: 245.901\n",
            "[8,   137] loss: 247.702\n",
            "[8,   138] loss: 249.506\n",
            "[8,   139] loss: 251.317\n",
            "[8,   140] loss: 253.124\n",
            "[8,   141] loss: 254.935\n",
            "[8,   142] loss: 256.742\n",
            "[8,   143] loss: 258.548\n",
            "[8,   144] loss: 260.354\n",
            "[8,   145] loss: 262.172\n",
            "[8,   146] loss: 263.984\n",
            "[8,   147] loss: 265.790\n",
            "[8,   148] loss: 267.594\n",
            "[8,   149] loss: 269.406\n",
            "[8,   150] loss: 271.216\n",
            "[8,   151] loss: 273.023\n",
            "[8,   152] loss: 274.825\n",
            "[8,   153] loss: 276.627\n",
            "[8,   154] loss: 278.433\n",
            "[8,   155] loss: 280.239\n",
            "[8,   156] loss: 282.050\n",
            "[8,   157] loss: 283.855\n",
            "[9,     2] loss: 3.622\n",
            "[9,     3] loss: 5.432\n",
            "[9,     4] loss: 7.234\n",
            "[9,     5] loss: 9.042\n",
            "[9,     6] loss: 10.854\n",
            "[9,     7] loss: 12.663\n",
            "[9,     8] loss: 14.471\n",
            "[9,     9] loss: 16.284\n",
            "[9,    10] loss: 18.086\n",
            "[9,    11] loss: 19.895\n",
            "[9,    12] loss: 21.705\n",
            "[9,    13] loss: 23.514\n",
            "[9,    14] loss: 25.320\n",
            "[9,    15] loss: 27.124\n",
            "[9,    16] loss: 28.933\n",
            "[9,    17] loss: 30.742\n",
            "[9,    18] loss: 32.556\n",
            "[9,    19] loss: 34.368\n",
            "[9,    20] loss: 36.180\n",
            "[9,    21] loss: 37.989\n",
            "[9,    22] loss: 39.796\n",
            "[9,    23] loss: 41.600\n",
            "[9,    24] loss: 43.407\n",
            "[9,    25] loss: 45.220\n",
            "[9,    26] loss: 47.028\n",
            "[9,    27] loss: 48.833\n",
            "[9,    28] loss: 50.640\n",
            "[9,    29] loss: 52.441\n",
            "[9,    30] loss: 54.247\n",
            "[9,    31] loss: 56.059\n",
            "[9,    32] loss: 57.871\n",
            "[9,    33] loss: 59.685\n",
            "[9,    34] loss: 61.504\n",
            "[9,    35] loss: 63.316\n",
            "[9,    36] loss: 65.125\n",
            "[9,    37] loss: 66.928\n",
            "[9,    38] loss: 68.738\n",
            "[9,    39] loss: 70.543\n",
            "[9,    40] loss: 72.346\n",
            "[9,    41] loss: 74.144\n",
            "[9,    42] loss: 75.960\n",
            "[9,    43] loss: 77.764\n",
            "[9,    44] loss: 79.567\n",
            "[9,    45] loss: 81.370\n",
            "[9,    46] loss: 83.177\n",
            "[9,    47] loss: 84.994\n",
            "[9,    48] loss: 86.802\n",
            "[9,    49] loss: 88.603\n",
            "[9,    50] loss: 90.410\n",
            "[9,    51] loss: 92.223\n",
            "[9,    52] loss: 94.029\n",
            "[9,    53] loss: 95.830\n",
            "[9,    54] loss: 97.640\n",
            "[9,    55] loss: 99.447\n",
            "[9,    56] loss: 101.263\n",
            "[9,    57] loss: 103.077\n",
            "[9,    58] loss: 104.892\n",
            "[9,    59] loss: 106.702\n",
            "[9,    60] loss: 108.514\n",
            "[9,    61] loss: 110.323\n",
            "[9,    62] loss: 112.135\n",
            "[9,    63] loss: 113.943\n",
            "[9,    64] loss: 115.749\n",
            "[9,    65] loss: 117.557\n",
            "[9,    66] loss: 119.365\n",
            "[9,    67] loss: 121.165\n",
            "[9,    68] loss: 122.970\n",
            "[9,    69] loss: 124.781\n",
            "[9,    70] loss: 126.586\n",
            "[9,    71] loss: 128.393\n",
            "[9,    72] loss: 130.203\n",
            "[9,    73] loss: 132.007\n",
            "[9,    74] loss: 133.822\n",
            "[9,    75] loss: 135.638\n",
            "[9,    76] loss: 137.449\n",
            "[9,    77] loss: 139.257\n",
            "[9,    78] loss: 141.061\n",
            "[9,    79] loss: 142.870\n",
            "[9,    80] loss: 144.673\n",
            "[9,    81] loss: 146.476\n",
            "[9,    82] loss: 148.285\n",
            "[9,    83] loss: 150.096\n",
            "[9,    84] loss: 151.903\n",
            "[9,    85] loss: 153.713\n",
            "[9,    86] loss: 155.523\n",
            "[9,    87] loss: 157.331\n",
            "[9,    88] loss: 159.142\n",
            "[9,    89] loss: 160.950\n",
            "[9,    90] loss: 162.759\n",
            "[9,    91] loss: 164.564\n",
            "[9,    92] loss: 166.375\n",
            "[9,    93] loss: 168.169\n",
            "[9,    94] loss: 169.976\n",
            "[9,    95] loss: 171.786\n",
            "[9,    96] loss: 173.591\n",
            "[9,    97] loss: 175.401\n",
            "[9,    98] loss: 177.211\n",
            "[9,    99] loss: 179.021\n",
            "[9,   100] loss: 180.831\n",
            "[9,   101] loss: 182.642\n",
            "[9,   102] loss: 184.453\n",
            "[9,   103] loss: 186.262\n",
            "[9,   104] loss: 188.072\n",
            "[9,   105] loss: 189.878\n",
            "[9,   106] loss: 191.689\n",
            "[9,   107] loss: 193.496\n",
            "[9,   108] loss: 195.299\n",
            "[9,   109] loss: 197.112\n",
            "[9,   110] loss: 198.917\n",
            "[9,   111] loss: 200.725\n",
            "[9,   112] loss: 202.523\n",
            "[9,   113] loss: 204.327\n",
            "[9,   114] loss: 206.134\n",
            "[9,   115] loss: 207.946\n",
            "[9,   116] loss: 209.757\n",
            "[9,   117] loss: 211.564\n",
            "[9,   118] loss: 213.371\n",
            "[9,   119] loss: 215.177\n",
            "[9,   120] loss: 216.987\n",
            "[9,   121] loss: 218.790\n",
            "[9,   122] loss: 220.601\n",
            "[9,   123] loss: 222.406\n",
            "[9,   124] loss: 224.223\n",
            "[9,   125] loss: 226.028\n",
            "[9,   126] loss: 227.828\n",
            "[9,   127] loss: 229.637\n",
            "[9,   128] loss: 231.445\n",
            "[9,   129] loss: 233.253\n",
            "[9,   130] loss: 235.057\n",
            "[9,   131] loss: 236.860\n",
            "[9,   132] loss: 238.667\n",
            "[9,   133] loss: 240.478\n",
            "[9,   134] loss: 242.293\n",
            "[9,   135] loss: 244.098\n",
            "[9,   136] loss: 245.899\n",
            "[9,   137] loss: 247.704\n",
            "[9,   138] loss: 249.507\n",
            "[9,   139] loss: 251.311\n",
            "[9,   140] loss: 253.119\n",
            "[9,   141] loss: 254.929\n",
            "[9,   142] loss: 256.737\n",
            "[9,   143] loss: 258.545\n",
            "[9,   144] loss: 260.351\n",
            "[9,   145] loss: 262.158\n",
            "[9,   146] loss: 263.963\n",
            "[9,   147] loss: 265.774\n",
            "[9,   148] loss: 267.587\n",
            "[9,   149] loss: 269.391\n",
            "[9,   150] loss: 271.199\n",
            "[9,   151] loss: 273.012\n",
            "[9,   152] loss: 274.813\n",
            "[9,   153] loss: 276.630\n",
            "[9,   154] loss: 278.443\n",
            "[9,   155] loss: 280.248\n",
            "[9,   156] loss: 282.048\n",
            "[9,   157] loss: 283.861\n",
            "[10,     2] loss: 3.620\n",
            "[10,     3] loss: 5.432\n",
            "[10,     4] loss: 7.247\n",
            "[10,     5] loss: 9.054\n",
            "[10,     6] loss: 10.863\n",
            "[10,     7] loss: 12.670\n",
            "[10,     8] loss: 14.483\n",
            "[10,     9] loss: 16.293\n",
            "[10,    10] loss: 18.101\n",
            "[10,    11] loss: 19.910\n",
            "[10,    12] loss: 21.722\n",
            "[10,    13] loss: 23.535\n",
            "[10,    14] loss: 25.340\n",
            "[10,    15] loss: 27.143\n",
            "[10,    16] loss: 28.949\n",
            "[10,    17] loss: 30.753\n",
            "[10,    18] loss: 32.557\n",
            "[10,    19] loss: 34.365\n",
            "[10,    20] loss: 36.177\n",
            "[10,    21] loss: 37.979\n",
            "[10,    22] loss: 39.782\n",
            "[10,    23] loss: 41.589\n",
            "[10,    24] loss: 43.404\n",
            "[10,    25] loss: 45.217\n",
            "[10,    26] loss: 47.019\n",
            "[10,    27] loss: 48.833\n",
            "[10,    28] loss: 50.642\n",
            "[10,    29] loss: 52.447\n",
            "[10,    30] loss: 54.253\n",
            "[10,    31] loss: 56.059\n",
            "[10,    32] loss: 57.866\n",
            "[10,    33] loss: 59.668\n",
            "[10,    34] loss: 61.474\n",
            "[10,    35] loss: 63.283\n",
            "[10,    36] loss: 65.094\n",
            "[10,    37] loss: 66.894\n",
            "[10,    38] loss: 68.702\n",
            "[10,    39] loss: 70.511\n",
            "[10,    40] loss: 72.322\n",
            "[10,    41] loss: 74.130\n",
            "[10,    42] loss: 75.945\n",
            "[10,    43] loss: 77.752\n",
            "[10,    44] loss: 79.556\n",
            "[10,    45] loss: 81.362\n",
            "[10,    46] loss: 83.169\n",
            "[10,    47] loss: 84.973\n",
            "[10,    48] loss: 86.787\n",
            "[10,    49] loss: 88.600\n",
            "[10,    50] loss: 90.411\n",
            "[10,    51] loss: 92.221\n",
            "[10,    52] loss: 94.026\n",
            "[10,    53] loss: 95.832\n",
            "[10,    54] loss: 97.633\n",
            "[10,    55] loss: 99.440\n",
            "[10,    56] loss: 101.251\n",
            "[10,    57] loss: 103.051\n",
            "[10,    58] loss: 104.864\n",
            "[10,    59] loss: 106.671\n",
            "[10,    60] loss: 108.478\n",
            "[10,    61] loss: 110.290\n",
            "[10,    62] loss: 112.101\n",
            "[10,    63] loss: 113.912\n",
            "[10,    64] loss: 115.713\n",
            "[10,    65] loss: 117.514\n",
            "[10,    66] loss: 119.320\n",
            "[10,    67] loss: 121.129\n",
            "[10,    68] loss: 122.933\n",
            "[10,    69] loss: 124.744\n",
            "[10,    70] loss: 126.557\n",
            "[10,    71] loss: 128.368\n",
            "[10,    72] loss: 130.180\n",
            "[10,    73] loss: 131.991\n",
            "[10,    74] loss: 133.795\n",
            "[10,    75] loss: 135.602\n",
            "[10,    76] loss: 137.407\n",
            "[10,    77] loss: 139.216\n",
            "[10,    78] loss: 141.023\n",
            "[10,    79] loss: 142.839\n",
            "[10,    80] loss: 144.649\n",
            "[10,    81] loss: 146.457\n",
            "[10,    82] loss: 148.267\n",
            "[10,    83] loss: 150.076\n",
            "[10,    84] loss: 151.885\n",
            "[10,    85] loss: 153.694\n",
            "[10,    86] loss: 155.500\n",
            "[10,    87] loss: 157.313\n",
            "[10,    88] loss: 159.121\n",
            "[10,    89] loss: 160.930\n",
            "[10,    90] loss: 162.731\n",
            "[10,    91] loss: 164.537\n",
            "[10,    92] loss: 166.338\n",
            "[10,    93] loss: 168.139\n",
            "[10,    94] loss: 169.949\n",
            "[10,    95] loss: 171.762\n",
            "[10,    96] loss: 173.575\n",
            "[10,    97] loss: 175.384\n",
            "[10,    98] loss: 177.195\n",
            "[10,    99] loss: 179.004\n",
            "[10,   100] loss: 180.802\n",
            "[10,   101] loss: 182.610\n",
            "[10,   102] loss: 184.418\n",
            "[10,   103] loss: 186.225\n",
            "[10,   104] loss: 188.039\n",
            "[10,   105] loss: 189.848\n",
            "[10,   106] loss: 191.660\n",
            "[10,   107] loss: 193.464\n",
            "[10,   108] loss: 195.272\n",
            "[10,   109] loss: 197.077\n",
            "[10,   110] loss: 198.886\n",
            "[10,   111] loss: 200.693\n",
            "[10,   112] loss: 202.492\n",
            "[10,   113] loss: 204.306\n",
            "[10,   114] loss: 206.110\n",
            "[10,   115] loss: 207.919\n",
            "[10,   116] loss: 209.727\n",
            "[10,   117] loss: 211.525\n",
            "[10,   118] loss: 213.337\n",
            "[10,   119] loss: 215.143\n",
            "[10,   120] loss: 216.952\n",
            "[10,   121] loss: 218.761\n",
            "[10,   122] loss: 220.571\n",
            "[10,   123] loss: 222.378\n",
            "[10,   124] loss: 224.186\n",
            "[10,   125] loss: 225.986\n",
            "[10,   126] loss: 227.799\n",
            "[10,   127] loss: 229.616\n",
            "[10,   128] loss: 231.422\n",
            "[10,   129] loss: 233.236\n",
            "[10,   130] loss: 235.040\n",
            "[10,   131] loss: 236.848\n",
            "[10,   132] loss: 238.661\n",
            "[10,   133] loss: 240.469\n",
            "[10,   134] loss: 242.281\n",
            "[10,   135] loss: 244.090\n",
            "[10,   136] loss: 245.900\n",
            "[10,   137] loss: 247.717\n",
            "[10,   138] loss: 249.520\n",
            "[10,   139] loss: 251.329\n",
            "[10,   140] loss: 253.136\n",
            "[10,   141] loss: 254.950\n",
            "[10,   142] loss: 256.756\n",
            "[10,   143] loss: 258.560\n",
            "[10,   144] loss: 260.362\n",
            "[10,   145] loss: 262.173\n",
            "[10,   146] loss: 263.979\n",
            "[10,   147] loss: 265.788\n",
            "[10,   148] loss: 267.592\n",
            "[10,   149] loss: 269.398\n",
            "[10,   150] loss: 271.203\n",
            "[10,   151] loss: 273.004\n",
            "[10,   152] loss: 274.811\n",
            "[10,   153] loss: 276.619\n",
            "[10,   154] loss: 278.430\n",
            "[10,   155] loss: 280.236\n",
            "[10,   156] loss: 282.051\n",
            "[10,   157] loss: 283.853\n",
            "[11,     2] loss: 3.626\n",
            "[11,     3] loss: 5.435\n",
            "[11,     4] loss: 7.240\n",
            "[11,     5] loss: 9.040\n",
            "[11,     6] loss: 10.854\n",
            "[11,     7] loss: 12.658\n",
            "[11,     8] loss: 14.470\n",
            "[11,     9] loss: 16.281\n",
            "[11,    10] loss: 18.081\n",
            "[11,    11] loss: 19.885\n",
            "[11,    12] loss: 21.690\n",
            "[11,    13] loss: 23.506\n",
            "[11,    14] loss: 25.317\n",
            "[11,    15] loss: 27.118\n",
            "[11,    16] loss: 28.928\n",
            "[11,    17] loss: 30.736\n",
            "[11,    18] loss: 32.551\n",
            "[11,    19] loss: 34.354\n",
            "[11,    20] loss: 36.158\n",
            "[11,    21] loss: 37.965\n",
            "[11,    22] loss: 39.767\n",
            "[11,    23] loss: 41.575\n",
            "[11,    24] loss: 43.385\n",
            "[11,    25] loss: 45.187\n",
            "[11,    26] loss: 47.000\n",
            "[11,    27] loss: 48.813\n",
            "[11,    28] loss: 50.624\n",
            "[11,    29] loss: 52.431\n",
            "[11,    30] loss: 54.244\n",
            "[11,    31] loss: 56.062\n",
            "[11,    32] loss: 57.872\n",
            "[11,    33] loss: 59.674\n",
            "[11,    34] loss: 61.477\n",
            "[11,    35] loss: 63.284\n",
            "[11,    36] loss: 65.098\n",
            "[11,    37] loss: 66.909\n",
            "[11,    38] loss: 68.720\n",
            "[11,    39] loss: 70.529\n",
            "[11,    40] loss: 72.343\n",
            "[11,    41] loss: 74.143\n",
            "[11,    42] loss: 75.950\n",
            "[11,    43] loss: 77.761\n",
            "[11,    44] loss: 79.578\n",
            "[11,    45] loss: 81.386\n",
            "[11,    46] loss: 83.189\n",
            "[11,    47] loss: 84.990\n",
            "[11,    48] loss: 86.801\n",
            "[11,    49] loss: 88.606\n",
            "[11,    50] loss: 90.413\n",
            "[11,    51] loss: 92.224\n",
            "[11,    52] loss: 94.033\n",
            "[11,    53] loss: 95.848\n",
            "[11,    54] loss: 97.653\n",
            "[11,    55] loss: 99.468\n",
            "[11,    56] loss: 101.275\n",
            "[11,    57] loss: 103.089\n",
            "[11,    58] loss: 104.897\n",
            "[11,    59] loss: 106.704\n",
            "[11,    60] loss: 108.518\n",
            "[11,    61] loss: 110.326\n",
            "[11,    62] loss: 112.127\n",
            "[11,    63] loss: 113.935\n",
            "[11,    64] loss: 115.745\n",
            "[11,    65] loss: 117.556\n",
            "[11,    66] loss: 119.369\n",
            "[11,    67] loss: 121.176\n",
            "[11,    68] loss: 122.981\n",
            "[11,    69] loss: 124.785\n",
            "[11,    70] loss: 126.584\n",
            "[11,    71] loss: 128.397\n",
            "[11,    72] loss: 130.210\n",
            "[11,    73] loss: 132.019\n",
            "[11,    74] loss: 133.830\n",
            "[11,    75] loss: 135.633\n",
            "[11,    76] loss: 137.433\n",
            "[11,    77] loss: 139.244\n",
            "[11,    78] loss: 141.052\n",
            "[11,    79] loss: 142.859\n",
            "[11,    80] loss: 144.666\n",
            "[11,    81] loss: 146.474\n",
            "[11,    82] loss: 148.281\n",
            "[11,    83] loss: 150.090\n",
            "[11,    84] loss: 151.894\n",
            "[11,    85] loss: 153.700\n",
            "[11,    86] loss: 155.501\n",
            "[11,    87] loss: 157.306\n",
            "[11,    88] loss: 159.114\n",
            "[11,    89] loss: 160.922\n",
            "[11,    90] loss: 162.734\n",
            "[11,    91] loss: 164.546\n",
            "[11,    92] loss: 166.353\n",
            "[11,    93] loss: 168.161\n",
            "[11,    94] loss: 169.968\n",
            "[11,    95] loss: 171.777\n",
            "[11,    96] loss: 173.587\n",
            "[11,    97] loss: 175.399\n",
            "[11,    98] loss: 177.211\n",
            "[11,    99] loss: 179.023\n",
            "[11,   100] loss: 180.829\n",
            "[11,   101] loss: 182.638\n",
            "[11,   102] loss: 184.453\n",
            "[11,   103] loss: 186.263\n",
            "[11,   104] loss: 188.066\n",
            "[11,   105] loss: 189.873\n",
            "[11,   106] loss: 191.682\n",
            "[11,   107] loss: 193.488\n",
            "[11,   108] loss: 195.289\n",
            "[11,   109] loss: 197.090\n",
            "[11,   110] loss: 198.883\n",
            "[11,   111] loss: 200.695\n",
            "[11,   112] loss: 202.497\n",
            "[11,   113] loss: 204.301\n",
            "[11,   114] loss: 206.104\n",
            "[11,   115] loss: 207.909\n",
            "[11,   116] loss: 209.720\n",
            "[11,   117] loss: 211.529\n",
            "[11,   118] loss: 213.335\n",
            "[11,   119] loss: 215.143\n",
            "[11,   120] loss: 216.949\n",
            "[11,   121] loss: 218.757\n",
            "[11,   122] loss: 220.564\n",
            "[11,   123] loss: 222.367\n",
            "[11,   124] loss: 224.175\n",
            "[11,   125] loss: 225.980\n",
            "[11,   126] loss: 227.792\n",
            "[11,   127] loss: 229.601\n",
            "[11,   128] loss: 231.414\n",
            "[11,   129] loss: 233.228\n",
            "[11,   130] loss: 235.029\n",
            "[11,   131] loss: 236.840\n",
            "[11,   132] loss: 238.648\n",
            "[11,   133] loss: 240.464\n",
            "[11,   134] loss: 242.271\n",
            "[11,   135] loss: 244.079\n",
            "[11,   136] loss: 245.884\n",
            "[11,   137] loss: 247.693\n",
            "[11,   138] loss: 249.505\n",
            "[11,   139] loss: 251.314\n",
            "[11,   140] loss: 253.123\n",
            "[11,   141] loss: 254.929\n",
            "[11,   142] loss: 256.738\n",
            "[11,   143] loss: 258.548\n",
            "[11,   144] loss: 260.358\n",
            "[11,   145] loss: 262.168\n",
            "[11,   146] loss: 263.978\n",
            "[11,   147] loss: 265.782\n",
            "[11,   148] loss: 267.592\n",
            "[11,   149] loss: 269.393\n",
            "[11,   150] loss: 271.194\n",
            "[11,   151] loss: 273.003\n",
            "[11,   152] loss: 274.815\n",
            "[11,   153] loss: 276.626\n",
            "[11,   154] loss: 278.431\n",
            "[11,   155] loss: 280.239\n",
            "[11,   156] loss: 282.048\n",
            "[11,   157] loss: 283.862\n",
            "[12,     2] loss: 3.609\n",
            "[12,     3] loss: 5.415\n",
            "[12,     4] loss: 7.221\n",
            "[12,     5] loss: 9.025\n",
            "[12,     6] loss: 10.824\n",
            "[12,     7] loss: 12.632\n",
            "[12,     8] loss: 14.445\n",
            "[12,     9] loss: 16.258\n",
            "[12,    10] loss: 18.063\n",
            "[12,    11] loss: 19.877\n",
            "[12,    12] loss: 21.684\n",
            "[12,    13] loss: 23.484\n",
            "[12,    14] loss: 25.291\n",
            "[12,    15] loss: 27.092\n",
            "[12,    16] loss: 28.898\n",
            "[12,    17] loss: 30.707\n",
            "[12,    18] loss: 32.517\n",
            "[12,    19] loss: 34.321\n",
            "[12,    20] loss: 36.129\n",
            "[12,    21] loss: 37.937\n",
            "[12,    22] loss: 39.746\n",
            "[12,    23] loss: 41.552\n",
            "[12,    24] loss: 43.361\n",
            "[12,    25] loss: 45.175\n",
            "[12,    26] loss: 46.974\n",
            "[12,    27] loss: 48.778\n",
            "[12,    28] loss: 50.591\n",
            "[12,    29] loss: 52.404\n",
            "[12,    30] loss: 54.221\n",
            "[12,    31] loss: 56.033\n",
            "[12,    32] loss: 57.840\n",
            "[12,    33] loss: 59.643\n",
            "[12,    34] loss: 61.446\n",
            "[12,    35] loss: 63.258\n",
            "[12,    36] loss: 65.069\n",
            "[12,    37] loss: 66.880\n",
            "[12,    38] loss: 68.688\n",
            "[12,    39] loss: 70.493\n",
            "[12,    40] loss: 72.299\n",
            "[12,    41] loss: 74.101\n",
            "[12,    42] loss: 75.913\n",
            "[12,    43] loss: 77.725\n",
            "[12,    44] loss: 79.528\n",
            "[12,    45] loss: 81.333\n",
            "[12,    46] loss: 83.143\n",
            "[12,    47] loss: 84.952\n",
            "[12,    48] loss: 86.759\n",
            "[12,    49] loss: 88.568\n",
            "[12,    50] loss: 90.381\n",
            "[12,    51] loss: 92.185\n",
            "[12,    52] loss: 93.988\n",
            "[12,    53] loss: 95.794\n",
            "[12,    54] loss: 97.600\n",
            "[12,    55] loss: 99.416\n",
            "[12,    56] loss: 101.231\n",
            "[12,    57] loss: 103.032\n",
            "[12,    58] loss: 104.837\n",
            "[12,    59] loss: 106.649\n",
            "[12,    60] loss: 108.459\n",
            "[12,    61] loss: 110.264\n",
            "[12,    62] loss: 112.072\n",
            "[12,    63] loss: 113.881\n",
            "[12,    64] loss: 115.690\n",
            "[12,    65] loss: 117.497\n",
            "[12,    66] loss: 119.305\n",
            "[12,    67] loss: 121.114\n",
            "[12,    68] loss: 122.929\n",
            "[12,    69] loss: 124.738\n",
            "[12,    70] loss: 126.547\n",
            "[12,    71] loss: 128.358\n",
            "[12,    72] loss: 130.166\n",
            "[12,    73] loss: 131.968\n",
            "[12,    74] loss: 133.780\n",
            "[12,    75] loss: 135.589\n",
            "[12,    76] loss: 137.399\n",
            "[12,    77] loss: 139.209\n",
            "[12,    78] loss: 141.017\n",
            "[12,    79] loss: 142.830\n",
            "[12,    80] loss: 144.647\n",
            "[12,    81] loss: 146.449\n",
            "[12,    82] loss: 148.261\n",
            "[12,    83] loss: 150.073\n",
            "[12,    84] loss: 151.875\n",
            "[12,    85] loss: 153.683\n",
            "[12,    86] loss: 155.497\n",
            "[12,    87] loss: 157.303\n",
            "[12,    88] loss: 159.111\n",
            "[12,    89] loss: 160.922\n",
            "[12,    90] loss: 162.734\n",
            "[12,    91] loss: 164.542\n",
            "[12,    92] loss: 166.344\n",
            "[12,    93] loss: 168.150\n",
            "[12,    94] loss: 169.956\n",
            "[12,    95] loss: 171.771\n",
            "[12,    96] loss: 173.581\n",
            "[12,    97] loss: 175.390\n",
            "[12,    98] loss: 177.202\n",
            "[12,    99] loss: 179.010\n",
            "[12,   100] loss: 180.821\n",
            "[12,   101] loss: 182.634\n",
            "[12,   102] loss: 184.438\n",
            "[12,   103] loss: 186.242\n",
            "[12,   104] loss: 188.049\n",
            "[12,   105] loss: 189.850\n",
            "[12,   106] loss: 191.660\n",
            "[12,   107] loss: 193.465\n",
            "[12,   108] loss: 195.271\n",
            "[12,   109] loss: 197.071\n",
            "[12,   110] loss: 198.885\n",
            "[12,   111] loss: 200.691\n",
            "[12,   112] loss: 202.495\n",
            "[12,   113] loss: 204.305\n",
            "[12,   114] loss: 206.111\n",
            "[12,   115] loss: 207.918\n",
            "[12,   116] loss: 209.727\n",
            "[12,   117] loss: 211.537\n",
            "[12,   118] loss: 213.340\n",
            "[12,   119] loss: 215.149\n",
            "[12,   120] loss: 216.963\n",
            "[12,   121] loss: 218.769\n",
            "[12,   122] loss: 220.572\n",
            "[12,   123] loss: 222.384\n",
            "[12,   124] loss: 224.197\n",
            "[12,   125] loss: 226.008\n",
            "[12,   126] loss: 227.816\n",
            "[12,   127] loss: 229.626\n",
            "[12,   128] loss: 231.435\n",
            "[12,   129] loss: 233.248\n",
            "[12,   130] loss: 235.057\n",
            "[12,   131] loss: 236.869\n",
            "[12,   132] loss: 238.670\n",
            "[12,   133] loss: 240.484\n",
            "[12,   134] loss: 242.291\n",
            "[12,   135] loss: 244.099\n",
            "[12,   136] loss: 245.908\n",
            "[12,   137] loss: 247.725\n",
            "[12,   138] loss: 249.531\n",
            "[12,   139] loss: 251.337\n",
            "[12,   140] loss: 253.142\n",
            "[12,   141] loss: 254.949\n",
            "[12,   142] loss: 256.759\n",
            "[12,   143] loss: 258.568\n",
            "[12,   144] loss: 260.367\n",
            "[12,   145] loss: 262.177\n",
            "[12,   146] loss: 263.985\n",
            "[12,   147] loss: 265.785\n",
            "[12,   148] loss: 267.597\n",
            "[12,   149] loss: 269.407\n",
            "[12,   150] loss: 271.208\n",
            "[12,   151] loss: 273.015\n",
            "[12,   152] loss: 274.823\n",
            "[12,   153] loss: 276.626\n",
            "[12,   154] loss: 278.434\n",
            "[12,   155] loss: 280.244\n",
            "[12,   156] loss: 282.053\n",
            "[12,   157] loss: 283.847\n",
            "[13,     2] loss: 3.623\n",
            "[13,     3] loss: 5.435\n",
            "[13,     4] loss: 7.246\n",
            "[13,     5] loss: 9.049\n",
            "[13,     6] loss: 10.854\n",
            "[13,     7] loss: 12.665\n",
            "[13,     8] loss: 14.470\n",
            "[13,     9] loss: 16.275\n",
            "[13,    10] loss: 18.082\n",
            "[13,    11] loss: 19.888\n",
            "[13,    12] loss: 21.693\n",
            "[13,    13] loss: 23.506\n",
            "[13,    14] loss: 25.315\n",
            "[13,    15] loss: 27.121\n",
            "[13,    16] loss: 28.929\n",
            "[13,    17] loss: 30.735\n",
            "[13,    18] loss: 32.544\n",
            "[13,    19] loss: 34.355\n",
            "[13,    20] loss: 36.167\n",
            "[13,    21] loss: 37.969\n",
            "[13,    22] loss: 39.780\n",
            "[13,    23] loss: 41.585\n",
            "[13,    24] loss: 43.382\n",
            "[13,    25] loss: 45.190\n",
            "[13,    26] loss: 47.001\n",
            "[13,    27] loss: 48.806\n",
            "[13,    28] loss: 50.617\n",
            "[13,    29] loss: 52.426\n",
            "[13,    30] loss: 54.227\n",
            "[13,    31] loss: 56.037\n",
            "[13,    32] loss: 57.851\n",
            "[13,    33] loss: 59.659\n",
            "[13,    34] loss: 61.469\n",
            "[13,    35] loss: 63.264\n",
            "[13,    36] loss: 65.070\n",
            "[13,    37] loss: 66.882\n",
            "[13,    38] loss: 68.684\n",
            "[13,    39] loss: 70.490\n",
            "[13,    40] loss: 72.300\n",
            "[13,    41] loss: 74.106\n",
            "[13,    42] loss: 75.903\n",
            "[13,    43] loss: 77.717\n",
            "[13,    44] loss: 79.521\n",
            "[13,    45] loss: 81.328\n",
            "[13,    46] loss: 83.138\n",
            "[13,    47] loss: 84.939\n",
            "[13,    48] loss: 86.746\n",
            "[13,    49] loss: 88.559\n",
            "[13,    50] loss: 90.358\n",
            "[13,    51] loss: 92.170\n",
            "[13,    52] loss: 93.983\n",
            "[13,    53] loss: 95.786\n",
            "[13,    54] loss: 97.592\n",
            "[13,    55] loss: 99.402\n",
            "[13,    56] loss: 101.211\n",
            "[13,    57] loss: 103.011\n",
            "[13,    58] loss: 104.824\n",
            "[13,    59] loss: 106.626\n",
            "[13,    60] loss: 108.435\n",
            "[13,    61] loss: 110.239\n",
            "[13,    62] loss: 112.039\n",
            "[13,    63] loss: 113.851\n",
            "[13,    64] loss: 115.658\n",
            "[13,    65] loss: 117.470\n",
            "[13,    66] loss: 119.285\n",
            "[13,    67] loss: 121.093\n",
            "[13,    68] loss: 122.897\n",
            "[13,    69] loss: 124.713\n",
            "[13,    70] loss: 126.527\n",
            "[13,    71] loss: 128.332\n",
            "[13,    72] loss: 130.139\n",
            "[13,    73] loss: 131.947\n",
            "[13,    74] loss: 133.754\n",
            "[13,    75] loss: 135.566\n",
            "[13,    76] loss: 137.370\n",
            "[13,    77] loss: 139.179\n",
            "[13,    78] loss: 140.998\n",
            "[13,    79] loss: 142.800\n",
            "[13,    80] loss: 144.610\n",
            "[13,    81] loss: 146.414\n",
            "[13,    82] loss: 148.224\n",
            "[13,    83] loss: 150.033\n",
            "[13,    84] loss: 151.846\n",
            "[13,    85] loss: 153.660\n",
            "[13,    86] loss: 155.464\n",
            "[13,    87] loss: 157.274\n",
            "[13,    88] loss: 159.085\n",
            "[13,    89] loss: 160.894\n",
            "[13,    90] loss: 162.698\n",
            "[13,    91] loss: 164.510\n",
            "[13,    92] loss: 166.319\n",
            "[13,    93] loss: 168.126\n",
            "[13,    94] loss: 169.933\n",
            "[13,    95] loss: 171.742\n",
            "[13,    96] loss: 173.545\n",
            "[13,    97] loss: 175.348\n",
            "[13,    98] loss: 177.155\n",
            "[13,    99] loss: 178.967\n",
            "[13,   100] loss: 180.778\n",
            "[13,   101] loss: 182.586\n",
            "[13,   102] loss: 184.398\n",
            "[13,   103] loss: 186.210\n",
            "[13,   104] loss: 188.021\n",
            "[13,   105] loss: 189.828\n",
            "[13,   106] loss: 191.640\n",
            "[13,   107] loss: 193.451\n",
            "[13,   108] loss: 195.261\n",
            "[13,   109] loss: 197.069\n",
            "[13,   110] loss: 198.874\n",
            "[13,   111] loss: 200.684\n",
            "[13,   112] loss: 202.487\n",
            "[13,   113] loss: 204.295\n",
            "[13,   114] loss: 206.102\n",
            "[13,   115] loss: 207.910\n",
            "[13,   116] loss: 209.717\n",
            "[13,   117] loss: 211.530\n",
            "[13,   118] loss: 213.334\n",
            "[13,   119] loss: 215.144\n",
            "[13,   120] loss: 216.955\n",
            "[13,   121] loss: 218.758\n",
            "[13,   122] loss: 220.566\n",
            "[13,   123] loss: 222.371\n",
            "[13,   124] loss: 224.178\n",
            "[13,   125] loss: 225.983\n",
            "[13,   126] loss: 227.788\n",
            "[13,   127] loss: 229.599\n",
            "[13,   128] loss: 231.409\n",
            "[13,   129] loss: 233.215\n",
            "[13,   130] loss: 235.023\n",
            "[13,   131] loss: 236.834\n",
            "[13,   132] loss: 238.641\n",
            "[13,   133] loss: 240.453\n",
            "[13,   134] loss: 242.261\n",
            "[13,   135] loss: 244.061\n",
            "[13,   136] loss: 245.868\n",
            "[13,   137] loss: 247.681\n",
            "[13,   138] loss: 249.489\n",
            "[13,   139] loss: 251.294\n",
            "[13,   140] loss: 253.100\n",
            "[13,   141] loss: 254.899\n",
            "[13,   142] loss: 256.715\n",
            "[13,   143] loss: 258.522\n",
            "[13,   144] loss: 260.329\n",
            "[13,   145] loss: 262.131\n",
            "[13,   146] loss: 263.948\n",
            "[13,   147] loss: 265.763\n",
            "[13,   148] loss: 267.565\n",
            "[13,   149] loss: 269.369\n",
            "[13,   150] loss: 271.181\n",
            "[13,   151] loss: 272.992\n",
            "[13,   152] loss: 274.802\n",
            "[13,   153] loss: 276.612\n",
            "[13,   154] loss: 278.422\n",
            "[13,   155] loss: 280.239\n",
            "[13,   156] loss: 282.047\n",
            "[13,   157] loss: 283.864\n",
            "[14,     2] loss: 3.625\n",
            "[14,     3] loss: 5.433\n",
            "[14,     4] loss: 7.236\n",
            "[14,     5] loss: 9.045\n",
            "[14,     6] loss: 10.859\n",
            "[14,     7] loss: 12.666\n",
            "[14,     8] loss: 14.475\n",
            "[14,     9] loss: 16.284\n",
            "[14,    10] loss: 18.090\n",
            "[14,    11] loss: 19.897\n",
            "[14,    12] loss: 21.707\n",
            "[14,    13] loss: 23.518\n",
            "[14,    14] loss: 25.328\n",
            "[14,    15] loss: 27.143\n",
            "[14,    16] loss: 28.949\n",
            "[14,    17] loss: 30.758\n",
            "[14,    18] loss: 32.566\n",
            "[14,    19] loss: 34.371\n",
            "[14,    20] loss: 36.177\n",
            "[14,    21] loss: 37.989\n",
            "[14,    22] loss: 39.789\n",
            "[14,    23] loss: 41.597\n",
            "[14,    24] loss: 43.405\n",
            "[14,    25] loss: 45.216\n",
            "[14,    26] loss: 47.026\n",
            "[14,    27] loss: 48.827\n",
            "[14,    28] loss: 50.637\n",
            "[14,    29] loss: 52.451\n",
            "[14,    30] loss: 54.258\n",
            "[14,    31] loss: 56.061\n",
            "[14,    32] loss: 57.870\n",
            "[14,    33] loss: 59.685\n",
            "[14,    34] loss: 61.499\n",
            "[14,    35] loss: 63.311\n",
            "[14,    36] loss: 65.114\n",
            "[14,    37] loss: 66.924\n",
            "[14,    38] loss: 68.733\n",
            "[14,    39] loss: 70.534\n",
            "[14,    40] loss: 72.350\n",
            "[14,    41] loss: 74.156\n",
            "[14,    42] loss: 75.967\n",
            "[14,    43] loss: 77.775\n",
            "[14,    44] loss: 79.580\n",
            "[14,    45] loss: 81.392\n",
            "[14,    46] loss: 83.206\n",
            "[14,    47] loss: 85.021\n",
            "[14,    48] loss: 86.833\n",
            "[14,    49] loss: 88.645\n",
            "[14,    50] loss: 90.449\n",
            "[14,    51] loss: 92.260\n",
            "[14,    52] loss: 94.064\n",
            "[14,    53] loss: 95.882\n",
            "[14,    54] loss: 97.696\n",
            "[14,    55] loss: 99.502\n",
            "[14,    56] loss: 101.312\n",
            "[14,    57] loss: 103.118\n",
            "[14,    58] loss: 104.919\n",
            "[14,    59] loss: 106.728\n",
            "[14,    60] loss: 108.535\n",
            "[14,    61] loss: 110.344\n",
            "[14,    62] loss: 112.154\n",
            "[14,    63] loss: 113.960\n",
            "[14,    64] loss: 115.764\n",
            "[14,    65] loss: 117.566\n",
            "[14,    66] loss: 119.369\n",
            "[14,    67] loss: 121.181\n",
            "[14,    68] loss: 122.985\n",
            "[14,    69] loss: 124.790\n",
            "[14,    70] loss: 126.603\n",
            "[14,    71] loss: 128.414\n",
            "[14,    72] loss: 130.225\n",
            "[14,    73] loss: 132.038\n",
            "[14,    74] loss: 133.847\n",
            "[14,    75] loss: 135.647\n",
            "[14,    76] loss: 137.455\n",
            "[14,    77] loss: 139.265\n",
            "[14,    78] loss: 141.072\n",
            "[14,    79] loss: 142.887\n",
            "[14,    80] loss: 144.693\n",
            "[14,    81] loss: 146.498\n",
            "[14,    82] loss: 148.299\n",
            "[14,    83] loss: 150.105\n",
            "[14,    84] loss: 151.911\n",
            "[14,    85] loss: 153.723\n",
            "[14,    86] loss: 155.534\n",
            "[14,    87] loss: 157.346\n",
            "[14,    88] loss: 159.152\n",
            "[14,    89] loss: 160.960\n",
            "[14,    90] loss: 162.763\n",
            "[14,    91] loss: 164.569\n",
            "[14,    92] loss: 166.375\n",
            "[14,    93] loss: 168.186\n",
            "[14,    94] loss: 169.991\n",
            "[14,    95] loss: 171.794\n",
            "[14,    96] loss: 173.602\n",
            "[14,    97] loss: 175.408\n",
            "[14,    98] loss: 177.218\n",
            "[14,    99] loss: 179.029\n",
            "[14,   100] loss: 180.838\n",
            "[14,   101] loss: 182.639\n",
            "[14,   102] loss: 184.446\n",
            "[14,   103] loss: 186.252\n",
            "[14,   104] loss: 188.053\n",
            "[14,   105] loss: 189.864\n",
            "[14,   106] loss: 191.670\n",
            "[14,   107] loss: 193.475\n",
            "[14,   108] loss: 195.280\n",
            "[14,   109] loss: 197.094\n",
            "[14,   110] loss: 198.905\n",
            "[14,   111] loss: 200.714\n",
            "[14,   112] loss: 202.522\n",
            "[14,   113] loss: 204.328\n",
            "[14,   114] loss: 206.137\n",
            "[14,   115] loss: 207.946\n",
            "[14,   116] loss: 209.755\n",
            "[14,   117] loss: 211.555\n",
            "[14,   118] loss: 213.359\n",
            "[14,   119] loss: 215.167\n",
            "[14,   120] loss: 216.971\n",
            "[14,   121] loss: 218.775\n",
            "[14,   122] loss: 220.575\n",
            "[14,   123] loss: 222.381\n",
            "[14,   124] loss: 224.191\n",
            "[14,   125] loss: 226.000\n",
            "[14,   126] loss: 227.809\n",
            "[14,   127] loss: 229.618\n",
            "[14,   128] loss: 231.430\n",
            "[14,   129] loss: 233.236\n",
            "[14,   130] loss: 235.035\n",
            "[14,   131] loss: 236.845\n",
            "[14,   132] loss: 238.654\n",
            "[14,   133] loss: 240.468\n",
            "[14,   134] loss: 242.275\n",
            "[14,   135] loss: 244.083\n",
            "[14,   136] loss: 245.884\n",
            "[14,   137] loss: 247.696\n",
            "[14,   138] loss: 249.508\n",
            "[14,   139] loss: 251.312\n",
            "[14,   140] loss: 253.121\n",
            "[14,   141] loss: 254.933\n",
            "[14,   142] loss: 256.736\n",
            "[14,   143] loss: 258.546\n",
            "[14,   144] loss: 260.355\n",
            "[14,   145] loss: 262.166\n",
            "[14,   146] loss: 263.978\n",
            "[14,   147] loss: 265.788\n",
            "[14,   148] loss: 267.601\n",
            "[14,   149] loss: 269.410\n",
            "[14,   150] loss: 271.211\n",
            "[14,   151] loss: 273.014\n",
            "[14,   152] loss: 274.819\n",
            "[14,   153] loss: 276.631\n",
            "[14,   154] loss: 278.434\n",
            "[14,   155] loss: 280.234\n",
            "[14,   156] loss: 282.050\n",
            "[14,   157] loss: 283.856\n",
            "[15,     2] loss: 3.615\n",
            "[15,     3] loss: 5.419\n",
            "[15,     4] loss: 7.234\n",
            "[15,     5] loss: 9.038\n",
            "[15,     6] loss: 10.849\n",
            "[15,     7] loss: 12.663\n",
            "[15,     8] loss: 14.475\n",
            "[15,     9] loss: 16.283\n",
            "[15,    10] loss: 18.096\n",
            "[15,    11] loss: 19.899\n",
            "[15,    12] loss: 21.705\n",
            "[15,    13] loss: 23.514\n",
            "[15,    14] loss: 25.319\n",
            "[15,    15] loss: 27.125\n",
            "[15,    16] loss: 28.927\n",
            "[15,    17] loss: 30.739\n",
            "[15,    18] loss: 32.552\n",
            "[15,    19] loss: 34.358\n",
            "[15,    20] loss: 36.160\n",
            "[15,    21] loss: 37.963\n",
            "[15,    22] loss: 39.779\n",
            "[15,    23] loss: 41.592\n",
            "[15,    24] loss: 43.405\n",
            "[15,    25] loss: 45.211\n",
            "[15,    26] loss: 47.023\n",
            "[15,    27] loss: 48.833\n",
            "[15,    28] loss: 50.644\n",
            "[15,    29] loss: 52.457\n",
            "[15,    30] loss: 54.267\n",
            "[15,    31] loss: 56.071\n",
            "[15,    32] loss: 57.876\n",
            "[15,    33] loss: 59.684\n",
            "[15,    34] loss: 61.490\n",
            "[15,    35] loss: 63.302\n",
            "[15,    36] loss: 65.103\n",
            "[15,    37] loss: 66.917\n",
            "[15,    38] loss: 68.730\n",
            "[15,    39] loss: 70.535\n",
            "[15,    40] loss: 72.342\n",
            "[15,    41] loss: 74.152\n",
            "[15,    42] loss: 75.958\n",
            "[15,    43] loss: 77.769\n",
            "[15,    44] loss: 79.579\n",
            "[15,    45] loss: 81.386\n",
            "[15,    46] loss: 83.192\n",
            "[15,    47] loss: 85.001\n",
            "[15,    48] loss: 86.812\n",
            "[15,    49] loss: 88.612\n",
            "[15,    50] loss: 90.423\n",
            "[15,    51] loss: 92.230\n",
            "[15,    52] loss: 94.037\n",
            "[15,    53] loss: 95.845\n",
            "[15,    54] loss: 97.653\n",
            "[15,    55] loss: 99.461\n",
            "[15,    56] loss: 101.267\n",
            "[15,    57] loss: 103.070\n",
            "[15,    58] loss: 104.877\n",
            "[15,    59] loss: 106.685\n",
            "[15,    60] loss: 108.492\n",
            "[15,    61] loss: 110.300\n",
            "[15,    62] loss: 112.111\n",
            "[15,    63] loss: 113.908\n",
            "[15,    64] loss: 115.715\n",
            "[15,    65] loss: 117.525\n",
            "[15,    66] loss: 119.331\n",
            "[15,    67] loss: 121.145\n",
            "[15,    68] loss: 122.955\n",
            "[15,    69] loss: 124.762\n",
            "[15,    70] loss: 126.565\n",
            "[15,    71] loss: 128.372\n",
            "[15,    72] loss: 130.180\n",
            "[15,    73] loss: 131.988\n",
            "[15,    74] loss: 133.801\n",
            "[15,    75] loss: 135.602\n",
            "[15,    76] loss: 137.408\n",
            "[15,    77] loss: 139.214\n",
            "[15,    78] loss: 141.030\n",
            "[15,    79] loss: 142.837\n",
            "[15,    80] loss: 144.645\n",
            "[15,    81] loss: 146.448\n",
            "[15,    82] loss: 148.262\n",
            "[15,    83] loss: 150.071\n",
            "[15,    84] loss: 151.880\n",
            "[15,    85] loss: 153.686\n",
            "[15,    86] loss: 155.497\n",
            "[15,    87] loss: 157.311\n",
            "[15,    88] loss: 159.126\n",
            "[15,    89] loss: 160.933\n",
            "[15,    90] loss: 162.744\n",
            "[15,    91] loss: 164.546\n",
            "[15,    92] loss: 166.355\n",
            "[15,    93] loss: 168.149\n",
            "[15,    94] loss: 169.960\n",
            "[15,    95] loss: 171.767\n",
            "[15,    96] loss: 173.571\n",
            "[15,    97] loss: 175.374\n",
            "[15,    98] loss: 177.181\n",
            "[15,    99] loss: 178.994\n",
            "[15,   100] loss: 180.810\n",
            "[15,   101] loss: 182.610\n",
            "[15,   102] loss: 184.421\n",
            "[15,   103] loss: 186.232\n",
            "[15,   104] loss: 188.041\n",
            "[15,   105] loss: 189.845\n",
            "[15,   106] loss: 191.657\n",
            "[15,   107] loss: 193.468\n",
            "[15,   108] loss: 195.282\n",
            "[15,   109] loss: 197.087\n",
            "[15,   110] loss: 198.893\n",
            "[15,   111] loss: 200.701\n",
            "[15,   112] loss: 202.511\n",
            "[15,   113] loss: 204.323\n",
            "[15,   114] loss: 206.128\n",
            "[15,   115] loss: 207.938\n",
            "[15,   116] loss: 209.747\n",
            "[15,   117] loss: 211.557\n",
            "[15,   118] loss: 213.362\n",
            "[15,   119] loss: 215.174\n",
            "[15,   120] loss: 216.980\n",
            "[15,   121] loss: 218.788\n",
            "[15,   122] loss: 220.598\n",
            "[15,   123] loss: 222.401\n",
            "[15,   124] loss: 224.211\n",
            "[15,   125] loss: 226.021\n",
            "[15,   126] loss: 227.830\n",
            "[15,   127] loss: 229.634\n",
            "[15,   128] loss: 231.439\n",
            "[15,   129] loss: 233.242\n",
            "[15,   130] loss: 235.051\n",
            "[15,   131] loss: 236.855\n",
            "[15,   132] loss: 238.663\n",
            "[15,   133] loss: 240.472\n",
            "[15,   134] loss: 242.280\n",
            "[15,   135] loss: 244.088\n",
            "[15,   136] loss: 245.889\n",
            "[15,   137] loss: 247.696\n",
            "[15,   138] loss: 249.503\n",
            "[15,   139] loss: 251.302\n",
            "[15,   140] loss: 253.113\n",
            "[15,   141] loss: 254.925\n",
            "[15,   142] loss: 256.727\n",
            "[15,   143] loss: 258.532\n",
            "[15,   144] loss: 260.340\n",
            "[15,   145] loss: 262.152\n",
            "[15,   146] loss: 263.966\n",
            "[15,   147] loss: 265.768\n",
            "[15,   148] loss: 267.578\n",
            "[15,   149] loss: 269.388\n",
            "[15,   150] loss: 271.194\n",
            "[15,   151] loss: 273.005\n",
            "[15,   152] loss: 274.814\n",
            "[15,   153] loss: 276.624\n",
            "[15,   154] loss: 278.429\n",
            "[15,   155] loss: 280.241\n",
            "[15,   156] loss: 282.053\n",
            "[15,   157] loss: 283.845\n",
            "[16,     2] loss: 3.608\n",
            "[16,     3] loss: 5.420\n",
            "[16,     4] loss: 7.227\n",
            "[16,     5] loss: 9.032\n",
            "[16,     6] loss: 10.838\n",
            "[16,     7] loss: 12.653\n",
            "[16,     8] loss: 14.464\n",
            "[16,     9] loss: 16.272\n",
            "[16,    10] loss: 18.079\n",
            "[16,    11] loss: 19.885\n",
            "[16,    12] loss: 21.698\n",
            "[16,    13] loss: 23.506\n",
            "[16,    14] loss: 25.318\n",
            "[16,    15] loss: 27.123\n",
            "[16,    16] loss: 28.934\n",
            "[16,    17] loss: 30.746\n",
            "[16,    18] loss: 32.555\n",
            "[16,    19] loss: 34.360\n",
            "[16,    20] loss: 36.164\n",
            "[16,    21] loss: 37.975\n",
            "[16,    22] loss: 39.783\n",
            "[16,    23] loss: 41.586\n",
            "[16,    24] loss: 43.400\n",
            "[16,    25] loss: 45.216\n",
            "[16,    26] loss: 47.028\n",
            "[16,    27] loss: 48.844\n",
            "[16,    28] loss: 50.649\n",
            "[16,    29] loss: 52.461\n",
            "[16,    30] loss: 54.268\n",
            "[16,    31] loss: 56.074\n",
            "[16,    32] loss: 57.882\n",
            "[16,    33] loss: 59.689\n",
            "[16,    34] loss: 61.498\n",
            "[16,    35] loss: 63.306\n",
            "[16,    36] loss: 65.110\n",
            "[16,    37] loss: 66.918\n",
            "[16,    38] loss: 68.726\n",
            "[16,    39] loss: 70.525\n",
            "[16,    40] loss: 72.332\n",
            "[16,    41] loss: 74.144\n",
            "[16,    42] loss: 75.952\n",
            "[16,    43] loss: 77.758\n",
            "[16,    44] loss: 79.570\n",
            "[16,    45] loss: 81.379\n",
            "[16,    46] loss: 83.186\n",
            "[16,    47] loss: 84.986\n",
            "[16,    48] loss: 86.789\n",
            "[16,    49] loss: 88.596\n",
            "[16,    50] loss: 90.408\n",
            "[16,    51] loss: 92.216\n",
            "[16,    52] loss: 94.020\n",
            "[16,    53] loss: 95.830\n",
            "[16,    54] loss: 97.640\n",
            "[16,    55] loss: 99.453\n",
            "[16,    56] loss: 101.266\n",
            "[16,    57] loss: 103.080\n",
            "[16,    58] loss: 104.893\n",
            "[16,    59] loss: 106.697\n",
            "[16,    60] loss: 108.504\n",
            "[16,    61] loss: 110.313\n",
            "[16,    62] loss: 112.117\n",
            "[16,    63] loss: 113.913\n",
            "[16,    64] loss: 115.721\n",
            "[16,    65] loss: 117.524\n",
            "[16,    66] loss: 119.329\n",
            "[16,    67] loss: 121.143\n",
            "[16,    68] loss: 122.949\n",
            "[16,    69] loss: 124.760\n",
            "[16,    70] loss: 126.572\n",
            "[16,    71] loss: 128.379\n",
            "[16,    72] loss: 130.190\n",
            "[16,    73] loss: 131.996\n",
            "[16,    74] loss: 133.809\n",
            "[16,    75] loss: 135.615\n",
            "[16,    76] loss: 137.421\n",
            "[16,    77] loss: 139.223\n",
            "[16,    78] loss: 141.029\n",
            "[16,    79] loss: 142.838\n",
            "[16,    80] loss: 144.648\n",
            "[16,    81] loss: 146.463\n",
            "[16,    82] loss: 148.268\n",
            "[16,    83] loss: 150.077\n",
            "[16,    84] loss: 151.878\n",
            "[16,    85] loss: 153.694\n",
            "[16,    86] loss: 155.501\n",
            "[16,    87] loss: 157.306\n",
            "[16,    88] loss: 159.112\n",
            "[16,    89] loss: 160.915\n",
            "[16,    90] loss: 162.727\n",
            "[16,    91] loss: 164.531\n",
            "[16,    92] loss: 166.338\n",
            "[16,    93] loss: 168.150\n",
            "[16,    94] loss: 169.961\n",
            "[16,    95] loss: 171.772\n",
            "[16,    96] loss: 173.582\n",
            "[16,    97] loss: 175.392\n",
            "[16,    98] loss: 177.193\n",
            "[16,    99] loss: 179.003\n",
            "[16,   100] loss: 180.809\n",
            "[16,   101] loss: 182.615\n",
            "[16,   102] loss: 184.424\n",
            "[16,   103] loss: 186.236\n",
            "[16,   104] loss: 188.042\n",
            "[16,   105] loss: 189.851\n",
            "[16,   106] loss: 191.660\n",
            "[16,   107] loss: 193.470\n",
            "[16,   108] loss: 195.275\n",
            "[16,   109] loss: 197.079\n",
            "[16,   110] loss: 198.882\n",
            "[16,   111] loss: 200.691\n",
            "[16,   112] loss: 202.497\n",
            "[16,   113] loss: 204.301\n",
            "[16,   114] loss: 206.110\n",
            "[16,   115] loss: 207.917\n",
            "[16,   116] loss: 209.722\n",
            "[16,   117] loss: 211.534\n",
            "[16,   118] loss: 213.346\n",
            "[16,   119] loss: 215.154\n",
            "[16,   120] loss: 216.958\n",
            "[16,   121] loss: 218.769\n",
            "[16,   122] loss: 220.575\n",
            "[16,   123] loss: 222.378\n",
            "[16,   124] loss: 224.180\n",
            "[16,   125] loss: 225.985\n",
            "[16,   126] loss: 227.791\n",
            "[16,   127] loss: 229.603\n",
            "[16,   128] loss: 231.407\n",
            "[16,   129] loss: 233.218\n",
            "[16,   130] loss: 235.027\n",
            "[16,   131] loss: 236.832\n",
            "[16,   132] loss: 238.647\n",
            "[16,   133] loss: 240.443\n",
            "[16,   134] loss: 242.251\n",
            "[16,   135] loss: 244.065\n",
            "[16,   136] loss: 245.871\n",
            "[16,   137] loss: 247.674\n",
            "[16,   138] loss: 249.483\n",
            "[16,   139] loss: 251.291\n",
            "[16,   140] loss: 253.103\n",
            "[16,   141] loss: 254.913\n",
            "[16,   142] loss: 256.722\n",
            "[16,   143] loss: 258.530\n",
            "[16,   144] loss: 260.341\n",
            "[16,   145] loss: 262.155\n",
            "[16,   146] loss: 263.963\n",
            "[16,   147] loss: 265.772\n",
            "[16,   148] loss: 267.579\n",
            "[16,   149] loss: 269.387\n",
            "[16,   150] loss: 271.198\n",
            "[16,   151] loss: 273.002\n",
            "[16,   152] loss: 274.811\n",
            "[16,   153] loss: 276.623\n",
            "[16,   154] loss: 278.434\n",
            "[16,   155] loss: 280.240\n",
            "[16,   156] loss: 282.052\n",
            "[16,   157] loss: 283.848\n",
            "[17,     2] loss: 3.615\n",
            "[17,     3] loss: 5.426\n",
            "[17,     4] loss: 7.231\n",
            "[17,     5] loss: 9.039\n",
            "[17,     6] loss: 10.845\n",
            "[17,     7] loss: 12.652\n",
            "[17,     8] loss: 14.458\n",
            "[17,     9] loss: 16.265\n",
            "[17,    10] loss: 18.080\n",
            "[17,    11] loss: 19.886\n",
            "[17,    12] loss: 21.693\n",
            "[17,    13] loss: 23.491\n",
            "[17,    14] loss: 25.306\n",
            "[17,    15] loss: 27.116\n",
            "[17,    16] loss: 28.927\n",
            "[17,    17] loss: 30.736\n",
            "[17,    18] loss: 32.542\n",
            "[17,    19] loss: 34.348\n",
            "[17,    20] loss: 36.155\n",
            "[17,    21] loss: 37.963\n",
            "[17,    22] loss: 39.775\n",
            "[17,    23] loss: 41.586\n",
            "[17,    24] loss: 43.391\n",
            "[17,    25] loss: 45.195\n",
            "[17,    26] loss: 47.005\n",
            "[17,    27] loss: 48.813\n",
            "[17,    28] loss: 50.623\n",
            "[17,    29] loss: 52.434\n",
            "[17,    30] loss: 54.234\n",
            "[17,    31] loss: 56.045\n",
            "[17,    32] loss: 57.855\n",
            "[17,    33] loss: 59.670\n",
            "[17,    34] loss: 61.480\n",
            "[17,    35] loss: 63.288\n",
            "[17,    36] loss: 65.097\n",
            "[17,    37] loss: 66.900\n",
            "[17,    38] loss: 68.708\n",
            "[17,    39] loss: 70.511\n",
            "[17,    40] loss: 72.328\n",
            "[17,    41] loss: 74.133\n",
            "[17,    42] loss: 75.943\n",
            "[17,    43] loss: 77.751\n",
            "[17,    44] loss: 79.561\n",
            "[17,    45] loss: 81.370\n",
            "[17,    46] loss: 83.172\n",
            "[17,    47] loss: 84.983\n",
            "[17,    48] loss: 86.797\n",
            "[17,    49] loss: 88.607\n",
            "[17,    50] loss: 90.419\n",
            "[17,    51] loss: 92.230\n",
            "[17,    52] loss: 94.028\n",
            "[17,    53] loss: 95.833\n",
            "[17,    54] loss: 97.636\n",
            "[17,    55] loss: 99.442\n",
            "[17,    56] loss: 101.255\n",
            "[17,    57] loss: 103.064\n",
            "[17,    58] loss: 104.868\n",
            "[17,    59] loss: 106.680\n",
            "[17,    60] loss: 108.493\n",
            "[17,    61] loss: 110.305\n",
            "[17,    62] loss: 112.113\n",
            "[17,    63] loss: 113.921\n",
            "[17,    64] loss: 115.724\n",
            "[17,    65] loss: 117.530\n",
            "[17,    66] loss: 119.334\n",
            "[17,    67] loss: 121.137\n",
            "[17,    68] loss: 122.941\n",
            "[17,    69] loss: 124.749\n",
            "[17,    70] loss: 126.566\n",
            "[17,    71] loss: 128.380\n",
            "[17,    72] loss: 130.191\n",
            "[17,    73] loss: 132.004\n",
            "[17,    74] loss: 133.815\n",
            "[17,    75] loss: 135.628\n",
            "[17,    76] loss: 137.426\n",
            "[17,    77] loss: 139.235\n",
            "[17,    78] loss: 141.044\n",
            "[17,    79] loss: 142.856\n",
            "[17,    80] loss: 144.658\n",
            "[17,    81] loss: 146.467\n",
            "[17,    82] loss: 148.281\n",
            "[17,    83] loss: 150.091\n",
            "[17,    84] loss: 151.890\n",
            "[17,    85] loss: 153.695\n",
            "[17,    86] loss: 155.503\n",
            "[17,    87] loss: 157.310\n",
            "[17,    88] loss: 159.119\n",
            "[17,    89] loss: 160.934\n",
            "[17,    90] loss: 162.738\n",
            "[17,    91] loss: 164.544\n",
            "[17,    92] loss: 166.354\n",
            "[17,    93] loss: 168.160\n",
            "[17,    94] loss: 169.963\n",
            "[17,    95] loss: 171.773\n",
            "[17,    96] loss: 173.587\n",
            "[17,    97] loss: 175.402\n",
            "[17,    98] loss: 177.213\n",
            "[17,    99] loss: 179.015\n",
            "[17,   100] loss: 180.816\n",
            "[17,   101] loss: 182.628\n",
            "[17,   102] loss: 184.431\n",
            "[17,   103] loss: 186.242\n",
            "[17,   104] loss: 188.049\n",
            "[17,   105] loss: 189.859\n",
            "[17,   106] loss: 191.660\n",
            "[17,   107] loss: 193.465\n",
            "[17,   108] loss: 195.263\n",
            "[17,   109] loss: 197.071\n",
            "[17,   110] loss: 198.886\n",
            "[17,   111] loss: 200.693\n",
            "[17,   112] loss: 202.499\n",
            "[17,   113] loss: 204.305\n",
            "[17,   114] loss: 206.111\n",
            "[17,   115] loss: 207.921\n",
            "[17,   116] loss: 209.719\n",
            "[17,   117] loss: 211.529\n",
            "[17,   118] loss: 213.335\n",
            "[17,   119] loss: 215.132\n",
            "[17,   120] loss: 216.943\n",
            "[17,   121] loss: 218.753\n",
            "[17,   122] loss: 220.559\n",
            "[17,   123] loss: 222.370\n",
            "[17,   124] loss: 224.176\n",
            "[17,   125] loss: 225.989\n",
            "[17,   126] loss: 227.791\n",
            "[17,   127] loss: 229.593\n",
            "[17,   128] loss: 231.400\n",
            "[17,   129] loss: 233.205\n",
            "[17,   130] loss: 235.015\n",
            "[17,   131] loss: 236.820\n",
            "[17,   132] loss: 238.632\n",
            "[17,   133] loss: 240.444\n",
            "[17,   134] loss: 242.258\n",
            "[17,   135] loss: 244.066\n",
            "[17,   136] loss: 245.879\n",
            "[17,   137] loss: 247.684\n",
            "[17,   138] loss: 249.496\n",
            "[17,   139] loss: 251.310\n",
            "[17,   140] loss: 253.117\n",
            "[17,   141] loss: 254.921\n",
            "[17,   142] loss: 256.728\n",
            "[17,   143] loss: 258.535\n",
            "[17,   144] loss: 260.341\n",
            "[17,   145] loss: 262.141\n",
            "[17,   146] loss: 263.946\n",
            "[17,   147] loss: 265.757\n",
            "[17,   148] loss: 267.566\n",
            "[17,   149] loss: 269.373\n",
            "[17,   150] loss: 271.186\n",
            "[17,   151] loss: 272.999\n",
            "[17,   152] loss: 274.811\n",
            "[17,   153] loss: 276.617\n",
            "[17,   154] loss: 278.422\n",
            "[17,   155] loss: 280.241\n",
            "[17,   156] loss: 282.046\n",
            "[17,   157] loss: 283.867\n",
            "[18,     2] loss: 3.600\n",
            "[18,     3] loss: 5.407\n",
            "[18,     4] loss: 7.217\n",
            "[18,     5] loss: 9.027\n",
            "[18,     6] loss: 10.837\n",
            "[18,     7] loss: 12.646\n",
            "[18,     8] loss: 14.450\n",
            "[18,     9] loss: 16.259\n",
            "[18,    10] loss: 18.065\n",
            "[18,    11] loss: 19.874\n",
            "[18,    12] loss: 21.681\n",
            "[18,    13] loss: 23.488\n",
            "[18,    14] loss: 25.303\n",
            "[18,    15] loss: 27.111\n",
            "[18,    16] loss: 28.924\n",
            "[18,    17] loss: 30.735\n",
            "[18,    18] loss: 32.538\n",
            "[18,    19] loss: 34.345\n",
            "[18,    20] loss: 36.149\n",
            "[18,    21] loss: 37.954\n",
            "[18,    22] loss: 39.764\n",
            "[18,    23] loss: 41.575\n",
            "[18,    24] loss: 43.389\n",
            "[18,    25] loss: 45.202\n",
            "[18,    26] loss: 47.015\n",
            "[18,    27] loss: 48.823\n",
            "[18,    28] loss: 50.635\n",
            "[18,    29] loss: 52.440\n",
            "[18,    30] loss: 54.251\n",
            "[18,    31] loss: 56.062\n",
            "[18,    32] loss: 57.866\n",
            "[18,    33] loss: 59.672\n",
            "[18,    34] loss: 61.482\n",
            "[18,    35] loss: 63.283\n",
            "[18,    36] loss: 65.094\n",
            "[18,    37] loss: 66.902\n",
            "[18,    38] loss: 68.711\n",
            "[18,    39] loss: 70.509\n",
            "[18,    40] loss: 72.323\n",
            "[18,    41] loss: 74.133\n",
            "[18,    42] loss: 75.945\n",
            "[18,    43] loss: 77.751\n",
            "[18,    44] loss: 79.562\n",
            "[18,    45] loss: 81.367\n",
            "[18,    46] loss: 83.174\n",
            "[18,    47] loss: 84.985\n",
            "[18,    48] loss: 86.794\n",
            "[18,    49] loss: 88.602\n",
            "[18,    50] loss: 90.411\n",
            "[18,    51] loss: 92.217\n",
            "[18,    52] loss: 94.029\n",
            "[18,    53] loss: 95.836\n",
            "[18,    54] loss: 97.645\n",
            "[18,    55] loss: 99.452\n",
            "[18,    56] loss: 101.255\n",
            "[18,    57] loss: 103.068\n",
            "[18,    58] loss: 104.877\n",
            "[18,    59] loss: 106.682\n",
            "[18,    60] loss: 108.486\n",
            "[18,    61] loss: 110.298\n",
            "[18,    62] loss: 112.106\n",
            "[18,    63] loss: 113.922\n",
            "[18,    64] loss: 115.729\n",
            "[18,    65] loss: 117.532\n",
            "[18,    66] loss: 119.344\n",
            "[18,    67] loss: 121.154\n",
            "[18,    68] loss: 122.963\n",
            "[18,    69] loss: 124.773\n",
            "[18,    70] loss: 126.578\n",
            "[18,    71] loss: 128.387\n",
            "[18,    72] loss: 130.193\n",
            "[18,    73] loss: 132.000\n",
            "[18,    74] loss: 133.808\n",
            "[18,    75] loss: 135.614\n",
            "[18,    76] loss: 137.427\n",
            "[18,    77] loss: 139.226\n",
            "[18,    78] loss: 141.036\n",
            "[18,    79] loss: 142.837\n",
            "[18,    80] loss: 144.647\n",
            "[18,    81] loss: 146.462\n",
            "[18,    82] loss: 148.268\n",
            "[18,    83] loss: 150.084\n",
            "[18,    84] loss: 151.890\n",
            "[18,    85] loss: 153.703\n",
            "[18,    86] loss: 155.505\n",
            "[18,    87] loss: 157.309\n",
            "[18,    88] loss: 159.114\n",
            "[18,    89] loss: 160.926\n",
            "[18,    90] loss: 162.738\n",
            "[18,    91] loss: 164.534\n",
            "[18,    92] loss: 166.348\n",
            "[18,    93] loss: 168.156\n",
            "[18,    94] loss: 169.964\n",
            "[18,    95] loss: 171.774\n",
            "[18,    96] loss: 173.585\n",
            "[18,    97] loss: 175.389\n",
            "[18,    98] loss: 177.188\n",
            "[18,    99] loss: 178.993\n",
            "[18,   100] loss: 180.793\n",
            "[18,   101] loss: 182.607\n",
            "[18,   102] loss: 184.420\n",
            "[18,   103] loss: 186.232\n",
            "[18,   104] loss: 188.042\n",
            "[18,   105] loss: 189.849\n",
            "[18,   106] loss: 191.656\n",
            "[18,   107] loss: 193.462\n",
            "[18,   108] loss: 195.271\n",
            "[18,   109] loss: 197.077\n",
            "[18,   110] loss: 198.892\n",
            "[18,   111] loss: 200.701\n",
            "[18,   112] loss: 202.512\n",
            "[18,   113] loss: 204.323\n",
            "[18,   114] loss: 206.131\n",
            "[18,   115] loss: 207.939\n",
            "[18,   116] loss: 209.748\n",
            "[18,   117] loss: 211.554\n",
            "[18,   118] loss: 213.366\n",
            "[18,   119] loss: 215.168\n",
            "[18,   120] loss: 216.975\n",
            "[18,   121] loss: 218.785\n",
            "[18,   122] loss: 220.594\n",
            "[18,   123] loss: 222.405\n",
            "[18,   124] loss: 224.212\n",
            "[18,   125] loss: 226.017\n",
            "[18,   126] loss: 227.816\n",
            "[18,   127] loss: 229.624\n",
            "[18,   128] loss: 231.432\n",
            "[18,   129] loss: 233.243\n",
            "[18,   130] loss: 235.039\n",
            "[18,   131] loss: 236.843\n",
            "[18,   132] loss: 238.647\n",
            "[18,   133] loss: 240.458\n",
            "[18,   134] loss: 242.266\n",
            "[18,   135] loss: 244.072\n",
            "[18,   136] loss: 245.875\n",
            "[18,   137] loss: 247.687\n",
            "[18,   138] loss: 249.497\n",
            "[18,   139] loss: 251.306\n",
            "[18,   140] loss: 253.112\n",
            "[18,   141] loss: 254.917\n",
            "[18,   142] loss: 256.729\n",
            "[18,   143] loss: 258.542\n",
            "[18,   144] loss: 260.353\n",
            "[18,   145] loss: 262.163\n",
            "[18,   146] loss: 263.972\n",
            "[18,   147] loss: 265.777\n",
            "[18,   148] loss: 267.587\n",
            "[18,   149] loss: 269.392\n",
            "[18,   150] loss: 271.195\n",
            "[18,   151] loss: 273.002\n",
            "[18,   152] loss: 274.807\n",
            "[18,   153] loss: 276.619\n",
            "[18,   154] loss: 278.429\n",
            "[18,   155] loss: 280.242\n",
            "[18,   156] loss: 282.052\n",
            "[18,   157] loss: 283.848\n",
            "[19,     2] loss: 3.627\n",
            "[19,     3] loss: 5.431\n",
            "[19,     4] loss: 7.244\n",
            "[19,     5] loss: 9.047\n",
            "[19,     6] loss: 10.853\n",
            "[19,     7] loss: 12.663\n",
            "[19,     8] loss: 14.473\n",
            "[19,     9] loss: 16.275\n",
            "[19,    10] loss: 18.085\n",
            "[19,    11] loss: 19.897\n",
            "[19,    12] loss: 21.706\n",
            "[19,    13] loss: 23.517\n",
            "[19,    14] loss: 25.328\n",
            "[19,    15] loss: 27.129\n",
            "[19,    16] loss: 28.938\n",
            "[19,    17] loss: 30.748\n",
            "[19,    18] loss: 32.558\n",
            "[19,    19] loss: 34.364\n",
            "[19,    20] loss: 36.173\n",
            "[19,    21] loss: 37.983\n",
            "[19,    22] loss: 39.793\n",
            "[19,    23] loss: 41.605\n",
            "[19,    24] loss: 43.410\n",
            "[19,    25] loss: 45.216\n",
            "[19,    26] loss: 47.030\n",
            "[19,    27] loss: 48.839\n",
            "[19,    28] loss: 50.645\n",
            "[19,    29] loss: 52.450\n",
            "[19,    30] loss: 54.261\n",
            "[19,    31] loss: 56.075\n",
            "[19,    32] loss: 57.881\n",
            "[19,    33] loss: 59.687\n",
            "[19,    34] loss: 61.495\n",
            "[19,    35] loss: 63.297\n",
            "[19,    36] loss: 65.105\n",
            "[19,    37] loss: 66.907\n",
            "[19,    38] loss: 68.716\n",
            "[19,    39] loss: 70.524\n",
            "[19,    40] loss: 72.331\n",
            "[19,    41] loss: 74.144\n",
            "[19,    42] loss: 75.950\n",
            "[19,    43] loss: 77.762\n",
            "[19,    44] loss: 79.572\n",
            "[19,    45] loss: 81.383\n",
            "[19,    46] loss: 83.193\n",
            "[19,    47] loss: 85.005\n",
            "[19,    48] loss: 86.810\n",
            "[19,    49] loss: 88.620\n",
            "[19,    50] loss: 90.433\n",
            "[19,    51] loss: 92.239\n",
            "[19,    52] loss: 94.049\n",
            "[19,    53] loss: 95.852\n",
            "[19,    54] loss: 97.657\n",
            "[19,    55] loss: 99.464\n",
            "[19,    56] loss: 101.266\n",
            "[19,    57] loss: 103.079\n",
            "[19,    58] loss: 104.892\n",
            "[19,    59] loss: 106.697\n",
            "[19,    60] loss: 108.504\n",
            "[19,    61] loss: 110.302\n",
            "[19,    62] loss: 112.110\n",
            "[19,    63] loss: 113.921\n",
            "[19,    64] loss: 115.722\n",
            "[19,    65] loss: 117.532\n",
            "[19,    66] loss: 119.342\n",
            "[19,    67] loss: 121.151\n",
            "[19,    68] loss: 122.960\n",
            "[19,    69] loss: 124.766\n",
            "[19,    70] loss: 126.574\n",
            "[19,    71] loss: 128.385\n",
            "[19,    72] loss: 130.198\n",
            "[19,    73] loss: 132.008\n",
            "[19,    74] loss: 133.817\n",
            "[19,    75] loss: 135.628\n",
            "[19,    76] loss: 137.434\n",
            "[19,    77] loss: 139.248\n",
            "[19,    78] loss: 141.061\n",
            "[19,    79] loss: 142.867\n",
            "[19,    80] loss: 144.674\n",
            "[19,    81] loss: 146.487\n",
            "[19,    82] loss: 148.298\n",
            "[19,    83] loss: 150.105\n",
            "[19,    84] loss: 151.912\n",
            "[19,    85] loss: 153.724\n",
            "[19,    86] loss: 155.528\n",
            "[19,    87] loss: 157.338\n",
            "[19,    88] loss: 159.150\n",
            "[19,    89] loss: 160.960\n",
            "[19,    90] loss: 162.769\n",
            "[19,    91] loss: 164.579\n",
            "[19,    92] loss: 166.389\n",
            "[19,    93] loss: 168.196\n",
            "[19,    94] loss: 170.001\n",
            "[19,    95] loss: 171.810\n",
            "[19,    96] loss: 173.618\n",
            "[19,    97] loss: 175.431\n",
            "[19,    98] loss: 177.237\n",
            "[19,    99] loss: 179.045\n",
            "[19,   100] loss: 180.854\n",
            "[19,   101] loss: 182.659\n",
            "[19,   102] loss: 184.475\n",
            "[19,   103] loss: 186.283\n",
            "[19,   104] loss: 188.087\n",
            "[19,   105] loss: 189.896\n",
            "[19,   106] loss: 191.709\n",
            "[19,   107] loss: 193.523\n",
            "[19,   108] loss: 195.333\n",
            "[19,   109] loss: 197.141\n",
            "[19,   110] loss: 198.944\n",
            "[19,   111] loss: 200.745\n",
            "[19,   112] loss: 202.546\n",
            "[19,   113] loss: 204.348\n",
            "[19,   114] loss: 206.153\n",
            "[19,   115] loss: 207.965\n",
            "[19,   116] loss: 209.769\n",
            "[19,   117] loss: 211.583\n",
            "[19,   118] loss: 213.391\n",
            "[19,   119] loss: 215.191\n",
            "[19,   120] loss: 216.994\n",
            "[19,   121] loss: 218.798\n",
            "[19,   122] loss: 220.610\n",
            "[19,   123] loss: 222.417\n",
            "[19,   124] loss: 224.229\n",
            "[19,   125] loss: 226.033\n",
            "[19,   126] loss: 227.840\n",
            "[19,   127] loss: 229.643\n",
            "[19,   128] loss: 231.447\n",
            "[19,   129] loss: 233.254\n",
            "[19,   130] loss: 235.065\n",
            "[19,   131] loss: 236.872\n",
            "[19,   132] loss: 238.684\n",
            "[19,   133] loss: 240.484\n",
            "[19,   134] loss: 242.295\n",
            "[19,   135] loss: 244.100\n",
            "[19,   136] loss: 245.909\n",
            "[19,   137] loss: 247.715\n",
            "[19,   138] loss: 249.522\n",
            "[19,   139] loss: 251.331\n",
            "[19,   140] loss: 253.142\n",
            "[19,   141] loss: 254.943\n",
            "[19,   142] loss: 256.751\n",
            "[19,   143] loss: 258.555\n",
            "[19,   144] loss: 260.361\n",
            "[19,   145] loss: 262.169\n",
            "[19,   146] loss: 263.985\n",
            "[19,   147] loss: 265.796\n",
            "[19,   148] loss: 267.600\n",
            "[19,   149] loss: 269.407\n",
            "[19,   150] loss: 271.213\n",
            "[19,   151] loss: 273.022\n",
            "[19,   152] loss: 274.829\n",
            "[19,   153] loss: 276.634\n",
            "[19,   154] loss: 278.435\n",
            "[19,   155] loss: 280.236\n",
            "[19,   156] loss: 282.047\n",
            "[19,   157] loss: 283.865\n",
            "[20,     2] loss: 3.613\n",
            "[20,     3] loss: 5.408\n",
            "[20,     4] loss: 7.215\n",
            "[20,     5] loss: 9.023\n",
            "[20,     6] loss: 10.834\n",
            "[20,     7] loss: 12.637\n",
            "[20,     8] loss: 14.443\n",
            "[20,     9] loss: 16.249\n",
            "[20,    10] loss: 18.055\n",
            "[20,    11] loss: 19.863\n",
            "[20,    12] loss: 21.677\n",
            "[20,    13] loss: 23.477\n",
            "[20,    14] loss: 25.288\n",
            "[20,    15] loss: 27.090\n",
            "[20,    16] loss: 28.900\n",
            "[20,    17] loss: 30.715\n",
            "[20,    18] loss: 32.529\n",
            "[20,    19] loss: 34.327\n",
            "[20,    20] loss: 36.135\n",
            "[20,    21] loss: 37.938\n",
            "[20,    22] loss: 39.753\n",
            "[20,    23] loss: 41.561\n",
            "[20,    24] loss: 43.366\n",
            "[20,    25] loss: 45.178\n",
            "[20,    26] loss: 46.981\n",
            "[20,    27] loss: 48.790\n",
            "[20,    28] loss: 50.602\n",
            "[20,    29] loss: 52.416\n",
            "[20,    30] loss: 54.228\n",
            "[20,    31] loss: 56.027\n",
            "[20,    32] loss: 57.839\n",
            "[20,    33] loss: 59.645\n",
            "[20,    34] loss: 61.452\n",
            "[20,    35] loss: 63.265\n",
            "[20,    36] loss: 65.067\n",
            "[20,    37] loss: 66.881\n",
            "[20,    38] loss: 68.687\n",
            "[20,    39] loss: 70.498\n",
            "[20,    40] loss: 72.303\n",
            "[20,    41] loss: 74.109\n",
            "[20,    42] loss: 75.921\n",
            "[20,    43] loss: 77.735\n",
            "[20,    44] loss: 79.542\n",
            "[20,    45] loss: 81.352\n",
            "[20,    46] loss: 83.156\n",
            "[20,    47] loss: 84.964\n",
            "[20,    48] loss: 86.769\n",
            "[20,    49] loss: 88.580\n",
            "[20,    50] loss: 90.391\n",
            "[20,    51] loss: 92.203\n",
            "[20,    52] loss: 94.005\n",
            "[20,    53] loss: 95.811\n",
            "[20,    54] loss: 97.620\n",
            "[20,    55] loss: 99.424\n",
            "[20,    56] loss: 101.241\n",
            "[20,    57] loss: 103.053\n",
            "[20,    58] loss: 104.857\n",
            "[20,    59] loss: 106.668\n",
            "[20,    60] loss: 108.476\n",
            "[20,    61] loss: 110.284\n",
            "[20,    62] loss: 112.099\n",
            "[20,    63] loss: 113.902\n",
            "[20,    64] loss: 115.713\n",
            "[20,    65] loss: 117.517\n",
            "[20,    66] loss: 119.323\n",
            "[20,    67] loss: 121.137\n",
            "[20,    68] loss: 122.932\n",
            "[20,    69] loss: 124.738\n",
            "[20,    70] loss: 126.554\n",
            "[20,    71] loss: 128.369\n",
            "[20,    72] loss: 130.175\n",
            "[20,    73] loss: 131.982\n",
            "[20,    74] loss: 133.795\n",
            "[20,    75] loss: 135.603\n",
            "[20,    76] loss: 137.410\n",
            "[20,    77] loss: 139.217\n",
            "[20,    78] loss: 141.024\n",
            "[20,    79] loss: 142.834\n",
            "[20,    80] loss: 144.640\n",
            "[20,    81] loss: 146.443\n",
            "[20,    82] loss: 148.247\n",
            "[20,    83] loss: 150.057\n",
            "[20,    84] loss: 151.865\n",
            "[20,    85] loss: 153.672\n",
            "[20,    86] loss: 155.483\n",
            "[20,    87] loss: 157.294\n",
            "[20,    88] loss: 159.100\n",
            "[20,    89] loss: 160.904\n",
            "[20,    90] loss: 162.712\n",
            "[20,    91] loss: 164.520\n",
            "[20,    92] loss: 166.327\n",
            "[20,    93] loss: 168.132\n",
            "[20,    94] loss: 169.942\n",
            "[20,    95] loss: 171.751\n",
            "[20,    96] loss: 173.565\n",
            "[20,    97] loss: 175.374\n",
            "[20,    98] loss: 177.181\n",
            "[20,    99] loss: 178.985\n",
            "[20,   100] loss: 180.794\n",
            "[20,   101] loss: 182.602\n",
            "[20,   102] loss: 184.410\n",
            "[20,   103] loss: 186.213\n",
            "[20,   104] loss: 188.024\n",
            "[20,   105] loss: 189.834\n",
            "[20,   106] loss: 191.642\n",
            "[20,   107] loss: 193.459\n",
            "[20,   108] loss: 195.267\n",
            "[20,   109] loss: 197.086\n",
            "[20,   110] loss: 198.893\n",
            "[20,   111] loss: 200.693\n",
            "[20,   112] loss: 202.507\n",
            "[20,   113] loss: 204.313\n",
            "[20,   114] loss: 206.120\n",
            "[20,   115] loss: 207.932\n",
            "[20,   116] loss: 209.731\n",
            "[20,   117] loss: 211.534\n",
            "[20,   118] loss: 213.343\n",
            "[20,   119] loss: 215.145\n",
            "[20,   120] loss: 216.957\n",
            "[20,   121] loss: 218.767\n",
            "[20,   122] loss: 220.578\n",
            "[20,   123] loss: 222.387\n",
            "[20,   124] loss: 224.196\n",
            "[20,   125] loss: 226.000\n",
            "[20,   126] loss: 227.813\n",
            "[20,   127] loss: 229.625\n",
            "[20,   128] loss: 231.434\n",
            "[20,   129] loss: 233.237\n",
            "[20,   130] loss: 235.043\n",
            "[20,   131] loss: 236.850\n",
            "[20,   132] loss: 238.653\n",
            "[20,   133] loss: 240.455\n",
            "[20,   134] loss: 242.262\n",
            "[20,   135] loss: 244.064\n",
            "[20,   136] loss: 245.877\n",
            "[20,   137] loss: 247.683\n",
            "[20,   138] loss: 249.493\n",
            "[20,   139] loss: 251.303\n",
            "[20,   140] loss: 253.113\n",
            "[20,   141] loss: 254.911\n",
            "[20,   142] loss: 256.718\n",
            "[20,   143] loss: 258.526\n",
            "[20,   144] loss: 260.333\n",
            "[20,   145] loss: 262.151\n",
            "[20,   146] loss: 263.963\n",
            "[20,   147] loss: 265.772\n",
            "[20,   148] loss: 267.581\n",
            "[20,   149] loss: 269.389\n",
            "[20,   150] loss: 271.199\n",
            "[20,   151] loss: 272.997\n",
            "[20,   152] loss: 274.805\n",
            "[20,   153] loss: 276.612\n",
            "[20,   154] loss: 278.426\n",
            "[20,   155] loss: 280.239\n",
            "[20,   156] loss: 282.048\n",
            "[20,   157] loss: 283.861\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pTBdqsnM0Lw"
      },
      "source": [
        "print(results['train_time']/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_TbtIDTMymK"
      },
      "source": [
        "plt.plot(range(EPOCHS),np.array(results['train_acc']))\n",
        "plt.plot(range(EPOCHS),np.array(results['valid_acc']))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_z2DHUyLycg"
      },
      "source": [
        "# InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sQBRRh-kcCH5",
        "outputId": "2c3c8c16-1f4a-4d30-c166-f742dac0b3f9"
      },
      "source": [
        "inception = models.inception_v3(pretrained=True)\n",
        "inception.aux_logits= False\n",
        "for param in inception.parameters():\n",
        "  param.requires_grad_(False)\n",
        "inception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57b394e7474142cf9a53402ff7264ac7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Inception3(\n",
              "  (Conv2d_1a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2b_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2d_3b_1x1): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_4a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Mixed_5b): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5c): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5d): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6a): InceptionB(\n",
              "    (branch3x3): BasicConv2d(\n",
              "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6b): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6c): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6d): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6e): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (AuxLogits): InceptionAux(\n",
              "    (conv0): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv1): BasicConv2d(\n",
              "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (Mixed_7a): InceptionD(\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7b): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7c): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNDiCb1PcWk-"
      },
      "source": [
        "inception.fc = nn.Sequential(nn.Linear(2048,256),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.4),\n",
        "                            nn.Linear(256,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNVwNeeDdCbH",
        "outputId": "5d318bd3-e0c4-4ada-9d0b-e991fd5f1ad4"
      },
      "source": [
        "model = inception\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Inception3(\n",
              "  (Conv2d_1a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2b_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2d_3b_1x1): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_4a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Mixed_5b): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5c): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5d): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6a): InceptionB(\n",
              "    (branch3x3): BasicConv2d(\n",
              "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6b): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6c): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6d): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6e): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (AuxLogits): InceptionAux(\n",
              "    (conv0): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv1): BasicConv2d(\n",
              "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (Mixed_7a): InceptionD(\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7b): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7c): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.4, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxvz-wTZcv3u"
      },
      "source": [
        "LR = 0.001\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbBVhunYc3Z3"
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MW5GOSkdTw7",
        "outputId": "767ce37a-4710-43ee-9a6d-139c1549d258"
      },
      "source": [
        "results = train_model(model,train_loader,valid_loader,criterion,optimizer,EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,     2] loss: 1.823\n",
            "[1,     3] loss: 2.716\n",
            "[1,     4] loss: 3.419\n",
            "[1,     5] loss: 4.134\n",
            "[1,     6] loss: 4.893\n",
            "[1,     7] loss: 5.515\n",
            "[1,     8] loss: 6.660\n",
            "[1,     9] loss: 7.440\n",
            "[1,    10] loss: 8.321\n",
            "[1,    11] loss: 9.011\n",
            "[1,    12] loss: 9.819\n",
            "[1,    13] loss: 10.681\n",
            "[1,    14] loss: 11.722\n",
            "[1,    15] loss: 12.675\n",
            "[1,    16] loss: 13.398\n",
            "[1,    17] loss: 14.185\n",
            "[1,    18] loss: 14.975\n",
            "[1,    19] loss: 15.880\n",
            "[1,    20] loss: 16.767\n",
            "[1,    21] loss: 17.530\n",
            "[1,    22] loss: 18.389\n",
            "[1,    23] loss: 19.082\n",
            "[1,    24] loss: 19.974\n",
            "[1,    25] loss: 20.768\n",
            "[1,    26] loss: 21.656\n",
            "[1,    27] loss: 22.194\n",
            "[1,    28] loss: 23.087\n",
            "[1,    29] loss: 23.795\n",
            "[1,    30] loss: 24.448\n",
            "[1,    31] loss: 25.366\n",
            "[1,    32] loss: 25.876\n",
            "[1,    33] loss: 26.922\n",
            "[1,    34] loss: 27.873\n",
            "[1,    35] loss: 28.851\n",
            "[1,    36] loss: 29.374\n",
            "[1,    37] loss: 30.165\n",
            "[1,    38] loss: 30.837\n",
            "[1,    39] loss: 31.561\n",
            "[1,    40] loss: 32.107\n",
            "[1,    41] loss: 32.998\n",
            "[1,    42] loss: 33.542\n",
            "[1,    43] loss: 34.469\n",
            "[1,    44] loss: 35.381\n",
            "[1,    45] loss: 36.080\n",
            "[1,    46] loss: 36.733\n",
            "[1,    47] loss: 37.446\n",
            "[1,    48] loss: 38.085\n",
            "[1,    49] loss: 38.627\n",
            "[1,    50] loss: 39.287\n",
            "[1,    51] loss: 40.178\n",
            "[1,    52] loss: 41.012\n",
            "[1,    53] loss: 41.664\n",
            "[1,    54] loss: 42.374\n",
            "[1,    55] loss: 42.980\n",
            "[1,    56] loss: 43.870\n",
            "[1,    57] loss: 44.687\n",
            "[1,    58] loss: 45.270\n",
            "[1,    59] loss: 45.751\n",
            "[1,    60] loss: 46.246\n",
            "[1,    61] loss: 46.994\n",
            "[1,    62] loss: 47.864\n",
            "[1,    63] loss: 48.440\n",
            "[1,    64] loss: 49.045\n",
            "[1,    65] loss: 49.800\n",
            "[1,    66] loss: 50.585\n",
            "[1,    67] loss: 51.341\n",
            "[1,    68] loss: 52.088\n",
            "[1,    69] loss: 52.785\n",
            "[1,    70] loss: 53.717\n",
            "[1,    71] loss: 54.460\n",
            "[1,    72] loss: 55.044\n",
            "[1,    73] loss: 55.585\n",
            "[1,    74] loss: 56.228\n",
            "[1,    75] loss: 56.586\n",
            "[1,    76] loss: 57.434\n",
            "[1,    77] loss: 58.139\n",
            "[1,    78] loss: 58.598\n",
            "[1,    79] loss: 59.438\n",
            "[1,    80] loss: 60.149\n",
            "[1,    81] loss: 60.780\n",
            "[1,    82] loss: 61.605\n",
            "[1,    83] loss: 62.321\n",
            "[1,    84] loss: 62.935\n",
            "[1,    85] loss: 63.840\n",
            "[1,    86] loss: 64.313\n",
            "[1,    87] loss: 64.765\n",
            "[1,    88] loss: 65.617\n",
            "[1,    89] loss: 66.614\n",
            "[1,    90] loss: 67.176\n",
            "[1,    91] loss: 67.978\n",
            "[1,    92] loss: 69.070\n",
            "[1,    93] loss: 69.903\n",
            "[1,    94] loss: 70.584\n",
            "[1,    95] loss: 71.243\n",
            "[1,    96] loss: 71.952\n",
            "[1,    97] loss: 72.546\n",
            "[1,    98] loss: 73.210\n",
            "[1,    99] loss: 73.953\n",
            "[1,   100] loss: 74.521\n",
            "[1,   101] loss: 75.143\n",
            "[1,   102] loss: 75.840\n",
            "[1,   103] loss: 76.716\n",
            "[1,   104] loss: 77.427\n",
            "[1,   105] loss: 78.130\n",
            "[1,   106] loss: 78.875\n",
            "[1,   107] loss: 79.514\n",
            "[1,   108] loss: 80.369\n",
            "[1,   109] loss: 81.016\n",
            "[1,   110] loss: 81.762\n",
            "[1,   111] loss: 82.375\n",
            "[1,   112] loss: 83.090\n",
            "[1,   113] loss: 83.659\n",
            "[1,   114] loss: 84.278\n",
            "[1,   115] loss: 84.897\n",
            "[1,   116] loss: 85.558\n",
            "[1,   117] loss: 86.369\n",
            "[1,   118] loss: 87.169\n",
            "[1,   119] loss: 87.952\n",
            "[1,   120] loss: 88.716\n",
            "[1,   121] loss: 89.420\n",
            "[1,   122] loss: 90.018\n",
            "[1,   123] loss: 90.622\n",
            "[1,   124] loss: 91.334\n",
            "[1,   125] loss: 91.807\n",
            "[1,   126] loss: 92.543\n",
            "[1,   127] loss: 93.476\n",
            "[1,   128] loss: 94.278\n",
            "[1,   129] loss: 95.039\n",
            "[1,   130] loss: 95.706\n",
            "[1,   131] loss: 96.684\n",
            "[1,   132] loss: 97.254\n",
            "[1,   133] loss: 98.193\n",
            "[1,   134] loss: 99.250\n",
            "[1,   135] loss: 99.764\n",
            "[1,   136] loss: 100.461\n",
            "[1,   137] loss: 101.145\n",
            "[1,   138] loss: 102.108\n",
            "[1,   139] loss: 102.794\n",
            "[1,   140] loss: 103.457\n",
            "[1,   141] loss: 104.289\n",
            "[1,   142] loss: 104.876\n",
            "[1,   143] loss: 105.914\n",
            "[1,   144] loss: 106.727\n",
            "[1,   145] loss: 107.394\n",
            "[1,   146] loss: 108.002\n",
            "[1,   147] loss: 108.743\n",
            "[1,   148] loss: 109.328\n",
            "[1,   149] loss: 109.735\n",
            "[1,   150] loss: 110.216\n",
            "[1,   151] loss: 111.243\n",
            "[1,   152] loss: 112.035\n",
            "[1,   153] loss: 112.874\n",
            "[1,   154] loss: 113.840\n",
            "[1,   155] loss: 114.279\n",
            "[1,   156] loss: 114.824\n",
            "[1,   157] loss: 115.119\n",
            "[2,     2] loss: 0.975\n",
            "[2,     3] loss: 1.589\n",
            "[2,     4] loss: 2.008\n",
            "[2,     5] loss: 2.461\n",
            "[2,     6] loss: 2.982\n",
            "[2,     7] loss: 3.497\n",
            "[2,     8] loss: 3.997\n",
            "[2,     9] loss: 4.454\n",
            "[2,    10] loss: 5.305\n",
            "[2,    11] loss: 5.850\n",
            "[2,    12] loss: 6.871\n",
            "[2,    13] loss: 7.414\n",
            "[2,    14] loss: 7.914\n",
            "[2,    15] loss: 8.534\n",
            "[2,    16] loss: 9.533\n",
            "[2,    17] loss: 10.066\n",
            "[2,    18] loss: 10.840\n",
            "[2,    19] loss: 11.508\n",
            "[2,    20] loss: 12.324\n",
            "[2,    21] loss: 12.942\n",
            "[2,    22] loss: 13.799\n",
            "[2,    23] loss: 14.671\n",
            "[2,    24] loss: 15.420\n",
            "[2,    25] loss: 16.110\n",
            "[2,    26] loss: 16.818\n",
            "[2,    27] loss: 17.488\n",
            "[2,    28] loss: 18.228\n",
            "[2,    29] loss: 19.267\n",
            "[2,    30] loss: 19.643\n",
            "[2,    31] loss: 20.273\n",
            "[2,    32] loss: 20.993\n",
            "[2,    33] loss: 21.508\n",
            "[2,    34] loss: 22.293\n",
            "[2,    35] loss: 22.869\n",
            "[2,    36] loss: 23.590\n",
            "[2,    37] loss: 24.061\n",
            "[2,    38] loss: 24.663\n",
            "[2,    39] loss: 25.475\n",
            "[2,    40] loss: 26.060\n",
            "[2,    41] loss: 26.517\n",
            "[2,    42] loss: 27.309\n",
            "[2,    43] loss: 28.094\n",
            "[2,    44] loss: 28.765\n",
            "[2,    45] loss: 29.236\n",
            "[2,    46] loss: 29.834\n",
            "[2,    47] loss: 30.709\n",
            "[2,    48] loss: 31.408\n",
            "[2,    49] loss: 31.816\n",
            "[2,    50] loss: 32.437\n",
            "[2,    51] loss: 33.096\n",
            "[2,    52] loss: 33.932\n",
            "[2,    53] loss: 34.585\n",
            "[2,    54] loss: 35.435\n",
            "[2,    55] loss: 36.058\n",
            "[2,    56] loss: 36.593\n",
            "[2,    57] loss: 37.333\n",
            "[2,    58] loss: 37.793\n",
            "[2,    59] loss: 38.723\n",
            "[2,    60] loss: 39.316\n",
            "[2,    61] loss: 39.981\n",
            "[2,    62] loss: 40.649\n",
            "[2,    63] loss: 41.210\n",
            "[2,    64] loss: 42.000\n",
            "[2,    65] loss: 42.615\n",
            "[2,    66] loss: 43.182\n",
            "[2,    67] loss: 43.827\n",
            "[2,    68] loss: 44.538\n",
            "[2,    69] loss: 45.289\n",
            "[2,    70] loss: 46.014\n",
            "[2,    71] loss: 46.641\n",
            "[2,    72] loss: 47.371\n",
            "[2,    73] loss: 47.847\n",
            "[2,    74] loss: 48.679\n",
            "[2,    75] loss: 48.990\n",
            "[2,    76] loss: 49.538\n",
            "[2,    77] loss: 50.303\n",
            "[2,    78] loss: 50.861\n",
            "[2,    79] loss: 51.419\n",
            "[2,    80] loss: 51.841\n",
            "[2,    81] loss: 52.619\n",
            "[2,    82] loss: 53.195\n",
            "[2,    83] loss: 53.756\n",
            "[2,    84] loss: 54.547\n",
            "[2,    85] loss: 55.198\n",
            "[2,    86] loss: 55.871\n",
            "[2,    87] loss: 56.399\n",
            "[2,    88] loss: 57.018\n",
            "[2,    89] loss: 57.796\n",
            "[2,    90] loss: 58.511\n",
            "[2,    91] loss: 59.142\n",
            "[2,    92] loss: 59.698\n",
            "[2,    93] loss: 60.731\n",
            "[2,    94] loss: 61.340\n",
            "[2,    95] loss: 61.742\n",
            "[2,    96] loss: 62.727\n",
            "[2,    97] loss: 63.421\n",
            "[2,    98] loss: 64.362\n",
            "[2,    99] loss: 65.053\n",
            "[2,   100] loss: 65.686\n",
            "[2,   101] loss: 66.377\n",
            "[2,   102] loss: 66.984\n",
            "[2,   103] loss: 67.595\n",
            "[2,   104] loss: 68.243\n",
            "[2,   105] loss: 69.080\n",
            "[2,   106] loss: 69.462\n",
            "[2,   107] loss: 69.907\n",
            "[2,   108] loss: 70.275\n",
            "[2,   109] loss: 70.884\n",
            "[2,   110] loss: 71.588\n",
            "[2,   111] loss: 72.078\n",
            "[2,   112] loss: 72.447\n",
            "[2,   113] loss: 73.113\n",
            "[2,   114] loss: 73.674\n",
            "[2,   115] loss: 74.069\n",
            "[2,   116] loss: 74.496\n",
            "[2,   117] loss: 75.491\n",
            "[2,   118] loss: 75.926\n",
            "[2,   119] loss: 76.415\n",
            "[2,   120] loss: 77.046\n",
            "[2,   121] loss: 77.575\n",
            "[2,   122] loss: 78.111\n",
            "[2,   123] loss: 78.817\n",
            "[2,   124] loss: 79.547\n",
            "[2,   125] loss: 80.039\n",
            "[2,   126] loss: 80.564\n",
            "[2,   127] loss: 81.018\n",
            "[2,   128] loss: 81.750\n",
            "[2,   129] loss: 82.190\n",
            "[2,   130] loss: 82.935\n",
            "[2,   131] loss: 83.332\n",
            "[2,   132] loss: 83.976\n",
            "[2,   133] loss: 84.759\n",
            "[2,   134] loss: 85.482\n",
            "[2,   135] loss: 86.039\n",
            "[2,   136] loss: 86.613\n",
            "[2,   137] loss: 87.135\n",
            "[2,   138] loss: 87.684\n",
            "[2,   139] loss: 88.313\n",
            "[2,   140] loss: 88.872\n",
            "[2,   141] loss: 89.469\n",
            "[2,   142] loss: 90.176\n",
            "[2,   143] loss: 90.966\n",
            "[2,   144] loss: 91.426\n",
            "[2,   145] loss: 91.973\n",
            "[2,   146] loss: 92.615\n",
            "[2,   147] loss: 93.448\n",
            "[2,   148] loss: 94.126\n",
            "[2,   149] loss: 94.976\n",
            "[2,   150] loss: 95.588\n",
            "[2,   151] loss: 96.154\n",
            "[2,   152] loss: 96.794\n",
            "[2,   153] loss: 97.526\n",
            "[2,   154] loss: 98.150\n",
            "[2,   155] loss: 98.837\n",
            "[2,   156] loss: 99.382\n",
            "[2,   157] loss: 100.044\n",
            "[3,     2] loss: 1.359\n",
            "[3,     3] loss: 2.013\n",
            "[3,     4] loss: 2.607\n",
            "[3,     5] loss: 3.063\n",
            "[3,     6] loss: 3.872\n",
            "[3,     7] loss: 4.381\n",
            "[3,     8] loss: 4.781\n",
            "[3,     9] loss: 5.362\n",
            "[3,    10] loss: 5.898\n",
            "[3,    11] loss: 6.793\n",
            "[3,    12] loss: 7.340\n",
            "[3,    13] loss: 7.768\n",
            "[3,    14] loss: 8.398\n",
            "[3,    15] loss: 8.961\n",
            "[3,    16] loss: 9.218\n",
            "[3,    17] loss: 9.922\n",
            "[3,    18] loss: 10.513\n",
            "[3,    19] loss: 10.860\n",
            "[3,    20] loss: 11.229\n",
            "[3,    21] loss: 11.768\n",
            "[3,    22] loss: 12.561\n",
            "[3,    23] loss: 13.271\n",
            "[3,    24] loss: 13.933\n",
            "[3,    25] loss: 14.257\n",
            "[3,    26] loss: 14.775\n",
            "[3,    27] loss: 15.041\n",
            "[3,    28] loss: 15.974\n",
            "[3,    29] loss: 16.538\n",
            "[3,    30] loss: 17.285\n",
            "[3,    31] loss: 18.017\n",
            "[3,    32] loss: 18.414\n",
            "[3,    33] loss: 18.977\n",
            "[3,    34] loss: 19.563\n",
            "[3,    35] loss: 20.355\n",
            "[3,    36] loss: 21.073\n",
            "[3,    37] loss: 21.519\n",
            "[3,    38] loss: 22.049\n",
            "[3,    39] loss: 22.602\n",
            "[3,    40] loss: 23.437\n",
            "[3,    41] loss: 23.998\n",
            "[3,    42] loss: 24.570\n",
            "[3,    43] loss: 25.290\n",
            "[3,    44] loss: 25.736\n",
            "[3,    45] loss: 26.156\n",
            "[3,    46] loss: 26.864\n",
            "[3,    47] loss: 27.419\n",
            "[3,    48] loss: 27.940\n",
            "[3,    49] loss: 28.237\n",
            "[3,    50] loss: 28.924\n",
            "[3,    51] loss: 29.379\n",
            "[3,    52] loss: 30.124\n",
            "[3,    53] loss: 30.826\n",
            "[3,    54] loss: 31.707\n",
            "[3,    55] loss: 32.274\n",
            "[3,    56] loss: 32.888\n",
            "[3,    57] loss: 33.542\n",
            "[3,    58] loss: 33.921\n",
            "[3,    59] loss: 34.532\n",
            "[3,    60] loss: 35.525\n",
            "[3,    61] loss: 36.065\n",
            "[3,    62] loss: 36.796\n",
            "[3,    63] loss: 37.343\n",
            "[3,    64] loss: 37.791\n",
            "[3,    65] loss: 38.334\n",
            "[3,    66] loss: 39.062\n",
            "[3,    67] loss: 39.398\n",
            "[3,    68] loss: 40.154\n",
            "[3,    69] loss: 40.967\n",
            "[3,    70] loss: 41.687\n",
            "[3,    71] loss: 42.176\n",
            "[3,    72] loss: 42.883\n",
            "[3,    73] loss: 43.465\n",
            "[3,    74] loss: 44.150\n",
            "[3,    75] loss: 45.017\n",
            "[3,    76] loss: 45.544\n",
            "[3,    77] loss: 46.194\n",
            "[3,    78] loss: 46.601\n",
            "[3,    79] loss: 47.132\n",
            "[3,    80] loss: 47.667\n",
            "[3,    81] loss: 48.439\n",
            "[3,    82] loss: 48.940\n",
            "[3,    83] loss: 49.621\n",
            "[3,    84] loss: 50.242\n",
            "[3,    85] loss: 50.772\n",
            "[3,    86] loss: 51.699\n",
            "[3,    87] loss: 52.201\n",
            "[3,    88] loss: 52.745\n",
            "[3,    89] loss: 53.116\n",
            "[3,    90] loss: 53.831\n",
            "[3,    91] loss: 54.130\n",
            "[3,    92] loss: 54.492\n",
            "[3,    93] loss: 54.838\n",
            "[3,    94] loss: 55.288\n",
            "[3,    95] loss: 55.900\n",
            "[3,    96] loss: 56.293\n",
            "[3,    97] loss: 56.811\n",
            "[3,    98] loss: 57.338\n",
            "[3,    99] loss: 57.870\n",
            "[3,   100] loss: 58.204\n",
            "[3,   101] loss: 59.036\n",
            "[3,   102] loss: 59.616\n",
            "[3,   103] loss: 60.134\n",
            "[3,   104] loss: 60.618\n",
            "[3,   105] loss: 61.352\n",
            "[3,   106] loss: 61.650\n",
            "[3,   107] loss: 62.279\n",
            "[3,   108] loss: 62.628\n",
            "[3,   109] loss: 63.421\n",
            "[3,   110] loss: 64.146\n",
            "[3,   111] loss: 64.738\n",
            "[3,   112] loss: 65.086\n",
            "[3,   113] loss: 65.523\n",
            "[3,   114] loss: 65.998\n",
            "[3,   115] loss: 66.795\n",
            "[3,   116] loss: 67.482\n",
            "[3,   117] loss: 67.946\n",
            "[3,   118] loss: 68.733\n",
            "[3,   119] loss: 69.424\n",
            "[3,   120] loss: 69.869\n",
            "[3,   121] loss: 70.517\n",
            "[3,   122] loss: 71.283\n",
            "[3,   123] loss: 71.912\n",
            "[3,   124] loss: 72.332\n",
            "[3,   125] loss: 72.836\n",
            "[3,   126] loss: 73.301\n",
            "[3,   127] loss: 73.991\n",
            "[3,   128] loss: 74.716\n",
            "[3,   129] loss: 75.311\n",
            "[3,   130] loss: 76.081\n",
            "[3,   131] loss: 76.629\n",
            "[3,   132] loss: 77.113\n",
            "[3,   133] loss: 77.829\n",
            "[3,   134] loss: 78.118\n",
            "[3,   135] loss: 78.872\n",
            "[3,   136] loss: 79.532\n",
            "[3,   137] loss: 80.341\n",
            "[3,   138] loss: 80.703\n",
            "[3,   139] loss: 81.136\n",
            "[3,   140] loss: 81.747\n",
            "[3,   141] loss: 82.220\n",
            "[3,   142] loss: 82.875\n",
            "[3,   143] loss: 83.534\n",
            "[3,   144] loss: 83.903\n",
            "[3,   145] loss: 84.354\n",
            "[3,   146] loss: 84.930\n",
            "[3,   147] loss: 85.353\n",
            "[3,   148] loss: 85.879\n",
            "[3,   149] loss: 86.354\n",
            "[3,   150] loss: 86.614\n",
            "[3,   151] loss: 87.091\n",
            "[3,   152] loss: 87.793\n",
            "[3,   153] loss: 88.781\n",
            "[3,   154] loss: 89.291\n",
            "[3,   155] loss: 89.889\n",
            "[3,   156] loss: 90.579\n",
            "[3,   157] loss: 91.243\n",
            "[4,     2] loss: 0.885\n",
            "[4,     3] loss: 1.306\n",
            "[4,     4] loss: 2.151\n",
            "[4,     5] loss: 2.627\n",
            "[4,     6] loss: 3.286\n",
            "[4,     7] loss: 3.936\n",
            "[4,     8] loss: 4.455\n",
            "[4,     9] loss: 5.063\n",
            "[4,    10] loss: 5.583\n",
            "[4,    11] loss: 6.056\n",
            "[4,    12] loss: 6.289\n",
            "[4,    13] loss: 7.053\n",
            "[4,    14] loss: 7.616\n",
            "[4,    15] loss: 8.003\n",
            "[4,    16] loss: 8.561\n",
            "[4,    17] loss: 9.266\n",
            "[4,    18] loss: 9.994\n",
            "[4,    19] loss: 10.435\n",
            "[4,    20] loss: 10.861\n",
            "[4,    21] loss: 11.296\n",
            "[4,    22] loss: 11.749\n",
            "[4,    23] loss: 12.177\n",
            "[4,    24] loss: 12.637\n",
            "[4,    25] loss: 13.044\n",
            "[4,    26] loss: 14.007\n",
            "[4,    27] loss: 14.709\n",
            "[4,    28] loss: 15.388\n",
            "[4,    29] loss: 15.889\n",
            "[4,    30] loss: 16.312\n",
            "[4,    31] loss: 17.082\n",
            "[4,    32] loss: 17.488\n",
            "[4,    33] loss: 17.966\n",
            "[4,    34] loss: 18.399\n",
            "[4,    35] loss: 18.758\n",
            "[4,    36] loss: 19.322\n",
            "[4,    37] loss: 19.874\n",
            "[4,    38] loss: 20.404\n",
            "[4,    39] loss: 20.858\n",
            "[4,    40] loss: 21.411\n",
            "[4,    41] loss: 21.836\n",
            "[4,    42] loss: 22.411\n",
            "[4,    43] loss: 23.097\n",
            "[4,    44] loss: 23.412\n",
            "[4,    45] loss: 23.847\n",
            "[4,    46] loss: 24.317\n",
            "[4,    47] loss: 25.112\n",
            "[4,    48] loss: 25.537\n",
            "[4,    49] loss: 25.959\n",
            "[4,    50] loss: 26.441\n",
            "[4,    51] loss: 26.962\n",
            "[4,    52] loss: 27.478\n",
            "[4,    53] loss: 27.826\n",
            "[4,    54] loss: 28.376\n",
            "[4,    55] loss: 28.913\n",
            "[4,    56] loss: 29.361\n",
            "[4,    57] loss: 29.939\n",
            "[4,    58] loss: 30.666\n",
            "[4,    59] loss: 31.063\n",
            "[4,    60] loss: 31.558\n",
            "[4,    61] loss: 31.981\n",
            "[4,    62] loss: 32.395\n",
            "[4,    63] loss: 32.904\n",
            "[4,    64] loss: 33.399\n",
            "[4,    65] loss: 34.181\n",
            "[4,    66] loss: 35.016\n",
            "[4,    67] loss: 35.344\n",
            "[4,    68] loss: 35.737\n",
            "[4,    69] loss: 36.251\n",
            "[4,    70] loss: 36.539\n",
            "[4,    71] loss: 36.893\n",
            "[4,    72] loss: 37.170\n",
            "[4,    73] loss: 37.802\n",
            "[4,    74] loss: 38.462\n",
            "[4,    75] loss: 38.935\n",
            "[4,    76] loss: 39.378\n",
            "[4,    77] loss: 40.235\n",
            "[4,    78] loss: 40.857\n",
            "[4,    79] loss: 41.577\n",
            "[4,    80] loss: 42.409\n",
            "[4,    81] loss: 42.836\n",
            "[4,    82] loss: 43.316\n",
            "[4,    83] loss: 43.832\n",
            "[4,    84] loss: 44.339\n",
            "[4,    85] loss: 45.105\n",
            "[4,    86] loss: 45.662\n",
            "[4,    87] loss: 46.482\n",
            "[4,    88] loss: 47.294\n",
            "[4,    89] loss: 48.058\n",
            "[4,    90] loss: 48.573\n",
            "[4,    91] loss: 49.094\n",
            "[4,    92] loss: 49.537\n",
            "[4,    93] loss: 50.012\n",
            "[4,    94] loss: 50.517\n",
            "[4,    95] loss: 51.023\n",
            "[4,    96] loss: 51.536\n",
            "[4,    97] loss: 52.075\n",
            "[4,    98] loss: 52.666\n",
            "[4,    99] loss: 52.938\n",
            "[4,   100] loss: 53.346\n",
            "[4,   101] loss: 54.192\n",
            "[4,   102] loss: 54.735\n",
            "[4,   103] loss: 55.336\n",
            "[4,   104] loss: 55.843\n",
            "[4,   105] loss: 56.299\n",
            "[4,   106] loss: 56.929\n",
            "[4,   107] loss: 57.371\n",
            "[4,   108] loss: 57.772\n",
            "[4,   109] loss: 58.353\n",
            "[4,   110] loss: 58.748\n",
            "[4,   111] loss: 59.265\n",
            "[4,   112] loss: 59.769\n",
            "[4,   113] loss: 60.336\n",
            "[4,   114] loss: 60.741\n",
            "[4,   115] loss: 61.135\n",
            "[4,   116] loss: 61.641\n",
            "[4,   117] loss: 62.029\n",
            "[4,   118] loss: 62.467\n",
            "[4,   119] loss: 62.781\n",
            "[4,   120] loss: 63.176\n",
            "[4,   121] loss: 63.584\n",
            "[4,   122] loss: 63.905\n",
            "[4,   123] loss: 64.547\n",
            "[4,   124] loss: 65.195\n",
            "[4,   125] loss: 65.810\n",
            "[4,   126] loss: 66.481\n",
            "[4,   127] loss: 66.896\n",
            "[4,   128] loss: 67.400\n",
            "[4,   129] loss: 67.677\n",
            "[4,   130] loss: 68.111\n",
            "[4,   131] loss: 68.772\n",
            "[4,   132] loss: 69.297\n",
            "[4,   133] loss: 69.724\n",
            "[4,   134] loss: 70.458\n",
            "[4,   135] loss: 71.006\n",
            "[4,   136] loss: 71.412\n",
            "[4,   137] loss: 71.769\n",
            "[4,   138] loss: 72.214\n",
            "[4,   139] loss: 72.740\n",
            "[4,   140] loss: 73.235\n",
            "[4,   141] loss: 73.621\n",
            "[4,   142] loss: 74.113\n",
            "[4,   143] loss: 74.723\n",
            "[4,   144] loss: 75.275\n",
            "[4,   145] loss: 75.772\n",
            "[4,   146] loss: 76.277\n",
            "[4,   147] loss: 77.123\n",
            "[4,   148] loss: 77.776\n",
            "[4,   149] loss: 78.596\n",
            "[4,   150] loss: 78.949\n",
            "[4,   151] loss: 79.522\n",
            "[4,   152] loss: 80.104\n",
            "[4,   153] loss: 80.680\n",
            "[4,   154] loss: 81.379\n",
            "[4,   155] loss: 81.834\n",
            "[4,   156] loss: 82.310\n",
            "[4,   157] loss: 82.524\n",
            "[5,     2] loss: 1.046\n",
            "[5,     3] loss: 1.554\n",
            "[5,     4] loss: 2.122\n",
            "[5,     5] loss: 2.604\n",
            "[5,     6] loss: 3.138\n",
            "[5,     7] loss: 3.447\n",
            "[5,     8] loss: 3.903\n",
            "[5,     9] loss: 4.304\n",
            "[5,    10] loss: 5.021\n",
            "[5,    11] loss: 5.442\n",
            "[5,    12] loss: 6.056\n",
            "[5,    13] loss: 6.765\n",
            "[5,    14] loss: 7.185\n",
            "[5,    15] loss: 7.688\n",
            "[5,    16] loss: 8.325\n",
            "[5,    17] loss: 8.791\n",
            "[5,    18] loss: 9.190\n",
            "[5,    19] loss: 9.830\n",
            "[5,    20] loss: 10.245\n",
            "[5,    21] loss: 10.668\n",
            "[5,    22] loss: 11.425\n",
            "[5,    23] loss: 11.831\n",
            "[5,    24] loss: 12.358\n",
            "[5,    25] loss: 12.934\n",
            "[5,    26] loss: 13.460\n",
            "[5,    27] loss: 13.755\n",
            "[5,    28] loss: 14.254\n",
            "[5,    29] loss: 14.618\n",
            "[5,    30] loss: 15.161\n",
            "[5,    31] loss: 15.543\n",
            "[5,    32] loss: 16.153\n",
            "[5,    33] loss: 16.669\n",
            "[5,    34] loss: 17.041\n",
            "[5,    35] loss: 17.677\n",
            "[5,    36] loss: 18.346\n",
            "[5,    37] loss: 18.643\n",
            "[5,    38] loss: 18.941\n",
            "[5,    39] loss: 19.264\n",
            "[5,    40] loss: 19.737\n",
            "[5,    41] loss: 20.098\n",
            "[5,    42] loss: 20.531\n",
            "[5,    43] loss: 21.101\n",
            "[5,    44] loss: 21.526\n",
            "[5,    45] loss: 21.976\n",
            "[5,    46] loss: 22.303\n",
            "[5,    47] loss: 22.968\n",
            "[5,    48] loss: 23.335\n",
            "[5,    49] loss: 23.705\n",
            "[5,    50] loss: 24.087\n",
            "[5,    51] loss: 24.486\n",
            "[5,    52] loss: 24.898\n",
            "[5,    53] loss: 25.331\n",
            "[5,    54] loss: 26.056\n",
            "[5,    55] loss: 26.380\n",
            "[5,    56] loss: 26.730\n",
            "[5,    57] loss: 27.253\n",
            "[5,    58] loss: 27.879\n",
            "[5,    59] loss: 28.167\n",
            "[5,    60] loss: 28.588\n",
            "[5,    61] loss: 29.032\n",
            "[5,    62] loss: 29.541\n",
            "[5,    63] loss: 30.041\n",
            "[5,    64] loss: 30.517\n",
            "[5,    65] loss: 30.892\n",
            "[5,    66] loss: 31.580\n",
            "[5,    67] loss: 32.068\n",
            "[5,    68] loss: 32.554\n",
            "[5,    69] loss: 33.065\n",
            "[5,    70] loss: 33.561\n",
            "[5,    71] loss: 34.006\n",
            "[5,    72] loss: 34.566\n",
            "[5,    73] loss: 35.005\n",
            "[5,    74] loss: 35.704\n",
            "[5,    75] loss: 36.275\n",
            "[5,    76] loss: 36.897\n",
            "[5,    77] loss: 37.406\n",
            "[5,    78] loss: 37.835\n",
            "[5,    79] loss: 38.521\n",
            "[5,    80] loss: 38.938\n",
            "[5,    81] loss: 39.408\n",
            "[5,    82] loss: 39.919\n",
            "[5,    83] loss: 40.179\n",
            "[5,    84] loss: 40.574\n",
            "[5,    85] loss: 41.104\n",
            "[5,    86] loss: 41.486\n",
            "[5,    87] loss: 41.920\n",
            "[5,    88] loss: 42.548\n",
            "[5,    89] loss: 42.880\n",
            "[5,    90] loss: 43.413\n",
            "[5,    91] loss: 43.901\n",
            "[5,    92] loss: 44.376\n",
            "[5,    93] loss: 44.762\n",
            "[5,    94] loss: 45.328\n",
            "[5,    95] loss: 45.704\n",
            "[5,    96] loss: 46.464\n",
            "[5,    97] loss: 46.857\n",
            "[5,    98] loss: 47.178\n",
            "[5,    99] loss: 47.775\n",
            "[5,   100] loss: 48.163\n",
            "[5,   101] loss: 48.438\n",
            "[5,   102] loss: 48.871\n",
            "[5,   103] loss: 49.447\n",
            "[5,   104] loss: 49.881\n",
            "[5,   105] loss: 50.587\n",
            "[5,   106] loss: 51.273\n",
            "[5,   107] loss: 51.792\n",
            "[5,   108] loss: 52.216\n",
            "[5,   109] loss: 52.680\n",
            "[5,   110] loss: 53.076\n",
            "[5,   111] loss: 53.922\n",
            "[5,   112] loss: 54.469\n",
            "[5,   113] loss: 54.831\n",
            "[5,   114] loss: 55.325\n",
            "[5,   115] loss: 55.925\n",
            "[5,   116] loss: 56.410\n",
            "[5,   117] loss: 56.764\n",
            "[5,   118] loss: 57.373\n",
            "[5,   119] loss: 57.993\n",
            "[5,   120] loss: 58.416\n",
            "[5,   121] loss: 58.964\n",
            "[5,   122] loss: 59.414\n",
            "[5,   123] loss: 60.250\n",
            "[5,   124] loss: 60.870\n",
            "[5,   125] loss: 61.190\n",
            "[5,   126] loss: 61.653\n",
            "[5,   127] loss: 62.301\n",
            "[5,   128] loss: 62.748\n",
            "[5,   129] loss: 63.449\n",
            "[5,   130] loss: 63.859\n",
            "[5,   131] loss: 64.538\n",
            "[5,   132] loss: 64.792\n",
            "[5,   133] loss: 65.154\n",
            "[5,   134] loss: 65.454\n",
            "[5,   135] loss: 66.059\n",
            "[5,   136] loss: 66.565\n",
            "[5,   137] loss: 67.211\n",
            "[5,   138] loss: 67.652\n",
            "[5,   139] loss: 68.169\n",
            "[5,   140] loss: 69.028\n",
            "[5,   141] loss: 69.812\n",
            "[5,   142] loss: 70.131\n",
            "[5,   143] loss: 70.802\n",
            "[5,   144] loss: 71.274\n",
            "[5,   145] loss: 71.981\n",
            "[5,   146] loss: 72.854\n",
            "[5,   147] loss: 73.587\n",
            "[5,   148] loss: 74.099\n",
            "[5,   149] loss: 74.607\n",
            "[5,   150] loss: 74.959\n",
            "[5,   151] loss: 75.408\n",
            "[5,   152] loss: 76.152\n",
            "[5,   153] loss: 76.673\n",
            "[5,   154] loss: 77.050\n",
            "[5,   155] loss: 77.542\n",
            "[5,   156] loss: 78.439\n",
            "[5,   157] loss: 79.141\n",
            "[6,     2] loss: 1.130\n",
            "[6,     3] loss: 1.568\n",
            "[6,     4] loss: 1.875\n",
            "[6,     5] loss: 2.272\n",
            "[6,     6] loss: 2.638\n",
            "[6,     7] loss: 2.936\n",
            "[6,     8] loss: 3.484\n",
            "[6,     9] loss: 3.827\n",
            "[6,    10] loss: 4.420\n",
            "[6,    11] loss: 4.780\n",
            "[6,    12] loss: 5.210\n",
            "[6,    13] loss: 5.475\n",
            "[6,    14] loss: 5.869\n",
            "[6,    15] loss: 6.146\n",
            "[6,    16] loss: 6.781\n",
            "[6,    17] loss: 7.267\n",
            "[6,    18] loss: 7.644\n",
            "[6,    19] loss: 8.398\n",
            "[6,    20] loss: 8.749\n",
            "[6,    21] loss: 9.551\n",
            "[6,    22] loss: 10.049\n",
            "[6,    23] loss: 10.398\n",
            "[6,    24] loss: 10.669\n",
            "[6,    25] loss: 11.167\n",
            "[6,    26] loss: 11.544\n",
            "[6,    27] loss: 12.010\n",
            "[6,    28] loss: 12.374\n",
            "[6,    29] loss: 12.881\n",
            "[6,    30] loss: 13.430\n",
            "[6,    31] loss: 13.794\n",
            "[6,    32] loss: 14.236\n",
            "[6,    33] loss: 14.542\n",
            "[6,    34] loss: 14.909\n",
            "[6,    35] loss: 15.269\n",
            "[6,    36] loss: 15.679\n",
            "[6,    37] loss: 15.982\n",
            "[6,    38] loss: 16.238\n",
            "[6,    39] loss: 16.865\n",
            "[6,    40] loss: 17.223\n",
            "[6,    41] loss: 17.679\n",
            "[6,    42] loss: 18.268\n",
            "[6,    43] loss: 18.526\n",
            "[6,    44] loss: 18.995\n",
            "[6,    45] loss: 19.392\n",
            "[6,    46] loss: 19.761\n",
            "[6,    47] loss: 20.260\n",
            "[6,    48] loss: 20.870\n",
            "[6,    49] loss: 21.293\n",
            "[6,    50] loss: 21.574\n",
            "[6,    51] loss: 21.724\n",
            "[6,    52] loss: 22.128\n",
            "[6,    53] loss: 22.354\n",
            "[6,    54] loss: 22.683\n",
            "[6,    55] loss: 23.106\n",
            "[6,    56] loss: 23.630\n",
            "[6,    57] loss: 24.080\n",
            "[6,    58] loss: 24.856\n",
            "[6,    59] loss: 25.373\n",
            "[6,    60] loss: 25.744\n",
            "[6,    61] loss: 26.333\n",
            "[6,    62] loss: 26.826\n",
            "[6,    63] loss: 27.263\n",
            "[6,    64] loss: 27.577\n",
            "[6,    65] loss: 27.862\n",
            "[6,    66] loss: 28.123\n",
            "[6,    67] loss: 28.421\n",
            "[6,    68] loss: 28.864\n",
            "[6,    69] loss: 29.514\n",
            "[6,    70] loss: 30.064\n",
            "[6,    71] loss: 30.317\n",
            "[6,    72] loss: 30.623\n",
            "[6,    73] loss: 31.143\n",
            "[6,    74] loss: 31.645\n",
            "[6,    75] loss: 32.078\n",
            "[6,    76] loss: 32.648\n",
            "[6,    77] loss: 33.014\n",
            "[6,    78] loss: 33.655\n",
            "[6,    79] loss: 33.893\n",
            "[6,    80] loss: 34.400\n",
            "[6,    81] loss: 34.639\n",
            "[6,    82] loss: 34.943\n",
            "[6,    83] loss: 35.320\n",
            "[6,    84] loss: 35.780\n",
            "[6,    85] loss: 36.202\n",
            "[6,    86] loss: 36.786\n",
            "[6,    87] loss: 37.378\n",
            "[6,    88] loss: 37.711\n",
            "[6,    89] loss: 38.085\n",
            "[6,    90] loss: 38.632\n",
            "[6,    91] loss: 39.460\n",
            "[6,    92] loss: 39.921\n",
            "[6,    93] loss: 40.198\n",
            "[6,    94] loss: 40.500\n",
            "[6,    95] loss: 40.724\n",
            "[6,    96] loss: 41.367\n",
            "[6,    97] loss: 41.764\n",
            "[6,    98] loss: 42.164\n",
            "[6,    99] loss: 42.663\n",
            "[6,   100] loss: 42.929\n",
            "[6,   101] loss: 43.293\n",
            "[6,   102] loss: 43.651\n",
            "[6,   103] loss: 44.018\n",
            "[6,   104] loss: 44.289\n",
            "[6,   105] loss: 44.769\n",
            "[6,   106] loss: 45.208\n",
            "[6,   107] loss: 45.569\n",
            "[6,   108] loss: 46.139\n",
            "[6,   109] loss: 46.362\n",
            "[6,   110] loss: 46.820\n",
            "[6,   111] loss: 47.149\n",
            "[6,   112] loss: 47.723\n",
            "[6,   113] loss: 47.948\n",
            "[6,   114] loss: 48.362\n",
            "[6,   115] loss: 48.800\n",
            "[6,   116] loss: 49.273\n",
            "[6,   117] loss: 49.528\n",
            "[6,   118] loss: 49.911\n",
            "[6,   119] loss: 50.242\n",
            "[6,   120] loss: 50.559\n",
            "[6,   121] loss: 50.940\n",
            "[6,   122] loss: 51.486\n",
            "[6,   123] loss: 52.099\n",
            "[6,   124] loss: 52.703\n",
            "[6,   125] loss: 53.163\n",
            "[6,   126] loss: 53.608\n",
            "[6,   127] loss: 53.834\n",
            "[6,   128] loss: 54.416\n",
            "[6,   129] loss: 55.031\n",
            "[6,   130] loss: 55.662\n",
            "[6,   131] loss: 55.972\n",
            "[6,   132] loss: 56.543\n",
            "[6,   133] loss: 57.369\n",
            "[6,   134] loss: 58.499\n",
            "[6,   135] loss: 58.735\n",
            "[6,   136] loss: 59.113\n",
            "[6,   137] loss: 59.749\n",
            "[6,   138] loss: 60.185\n",
            "[6,   139] loss: 60.755\n",
            "[6,   140] loss: 61.167\n",
            "[6,   141] loss: 61.709\n",
            "[6,   142] loss: 62.112\n",
            "[6,   143] loss: 62.694\n",
            "[6,   144] loss: 63.150\n",
            "[6,   145] loss: 63.695\n",
            "[6,   146] loss: 64.044\n",
            "[6,   147] loss: 64.390\n",
            "[6,   148] loss: 64.806\n",
            "[6,   149] loss: 65.211\n",
            "[6,   150] loss: 65.799\n",
            "[6,   151] loss: 66.341\n",
            "[6,   152] loss: 66.859\n",
            "[6,   153] loss: 67.427\n",
            "[6,   154] loss: 67.999\n",
            "[6,   155] loss: 68.276\n",
            "[6,   156] loss: 68.804\n",
            "[6,   157] loss: 69.029\n",
            "[7,     2] loss: 0.705\n",
            "[7,     3] loss: 1.105\n",
            "[7,     4] loss: 1.646\n",
            "[7,     5] loss: 2.097\n",
            "[7,     6] loss: 2.517\n",
            "[7,     7] loss: 2.909\n",
            "[7,     8] loss: 3.233\n",
            "[7,     9] loss: 3.566\n",
            "[7,    10] loss: 3.808\n",
            "[7,    11] loss: 4.408\n",
            "[7,    12] loss: 4.722\n",
            "[7,    13] loss: 5.057\n",
            "[7,    14] loss: 5.422\n",
            "[7,    15] loss: 5.939\n",
            "[7,    16] loss: 6.149\n",
            "[7,    17] loss: 6.351\n",
            "[7,    18] loss: 6.944\n",
            "[7,    19] loss: 7.495\n",
            "[7,    20] loss: 7.796\n",
            "[7,    21] loss: 8.098\n",
            "[7,    22] loss: 8.478\n",
            "[7,    23] loss: 8.884\n",
            "[7,    24] loss: 9.221\n",
            "[7,    25] loss: 9.607\n",
            "[7,    26] loss: 9.885\n",
            "[7,    27] loss: 10.115\n",
            "[7,    28] loss: 10.731\n",
            "[7,    29] loss: 11.048\n",
            "[7,    30] loss: 11.420\n",
            "[7,    31] loss: 12.040\n",
            "[7,    32] loss: 12.485\n",
            "[7,    33] loss: 12.676\n",
            "[7,    34] loss: 13.053\n",
            "[7,    35] loss: 13.325\n",
            "[7,    36] loss: 13.902\n",
            "[7,    37] loss: 14.474\n",
            "[7,    38] loss: 14.854\n",
            "[7,    39] loss: 15.172\n",
            "[7,    40] loss: 15.526\n",
            "[7,    41] loss: 15.878\n",
            "[7,    42] loss: 16.290\n",
            "[7,    43] loss: 16.464\n",
            "[7,    44] loss: 16.674\n",
            "[7,    45] loss: 17.130\n",
            "[7,    46] loss: 17.506\n",
            "[7,    47] loss: 17.762\n",
            "[7,    48] loss: 18.147\n",
            "[7,    49] loss: 18.484\n",
            "[7,    50] loss: 18.780\n",
            "[7,    51] loss: 19.151\n",
            "[7,    52] loss: 19.457\n",
            "[7,    53] loss: 20.012\n",
            "[7,    54] loss: 20.455\n",
            "[7,    55] loss: 20.809\n",
            "[7,    56] loss: 21.372\n",
            "[7,    57] loss: 21.825\n",
            "[7,    58] loss: 22.239\n",
            "[7,    59] loss: 22.616\n",
            "[7,    60] loss: 22.961\n",
            "[7,    61] loss: 23.415\n",
            "[7,    62] loss: 23.728\n",
            "[7,    63] loss: 24.309\n",
            "[7,    64] loss: 24.766\n",
            "[7,    65] loss: 25.216\n",
            "[7,    66] loss: 25.542\n",
            "[7,    67] loss: 26.032\n",
            "[7,    68] loss: 26.477\n",
            "[7,    69] loss: 26.861\n",
            "[7,    70] loss: 27.242\n",
            "[7,    71] loss: 27.663\n",
            "[7,    72] loss: 27.892\n",
            "[7,    73] loss: 28.242\n",
            "[7,    74] loss: 28.428\n",
            "[7,    75] loss: 28.730\n",
            "[7,    76] loss: 29.052\n",
            "[7,    77] loss: 29.214\n",
            "[7,    78] loss: 29.527\n",
            "[7,    79] loss: 29.920\n",
            "[7,    80] loss: 30.262\n",
            "[7,    81] loss: 30.580\n",
            "[7,    82] loss: 31.165\n",
            "[7,    83] loss: 31.560\n",
            "[7,    84] loss: 31.883\n",
            "[7,    85] loss: 32.119\n",
            "[7,    86] loss: 32.502\n",
            "[7,    87] loss: 32.908\n",
            "[7,    88] loss: 33.225\n",
            "[7,    89] loss: 33.750\n",
            "[7,    90] loss: 34.253\n",
            "[7,    91] loss: 34.694\n",
            "[7,    92] loss: 35.090\n",
            "[7,    93] loss: 35.537\n",
            "[7,    94] loss: 36.095\n",
            "[7,    95] loss: 36.268\n",
            "[7,    96] loss: 36.566\n",
            "[7,    97] loss: 36.882\n",
            "[7,    98] loss: 37.639\n",
            "[7,    99] loss: 38.063\n",
            "[7,   100] loss: 38.529\n",
            "[7,   101] loss: 38.987\n",
            "[7,   102] loss: 39.480\n",
            "[7,   103] loss: 40.028\n",
            "[7,   104] loss: 40.539\n",
            "[7,   105] loss: 41.150\n",
            "[7,   106] loss: 41.449\n",
            "[7,   107] loss: 41.788\n",
            "[7,   108] loss: 42.229\n",
            "[7,   109] loss: 42.697\n",
            "[7,   110] loss: 43.114\n",
            "[7,   111] loss: 43.525\n",
            "[7,   112] loss: 43.903\n",
            "[7,   113] loss: 44.187\n",
            "[7,   114] loss: 44.471\n",
            "[7,   115] loss: 44.778\n",
            "[7,   116] loss: 45.259\n",
            "[7,   117] loss: 45.555\n",
            "[7,   118] loss: 45.874\n",
            "[7,   119] loss: 46.043\n",
            "[7,   120] loss: 46.299\n",
            "[7,   121] loss: 46.536\n",
            "[7,   122] loss: 46.939\n",
            "[7,   123] loss: 47.478\n",
            "[7,   124] loss: 47.858\n",
            "[7,   125] loss: 48.340\n",
            "[7,   126] loss: 48.702\n",
            "[7,   127] loss: 49.174\n",
            "[7,   128] loss: 49.543\n",
            "[7,   129] loss: 49.858\n",
            "[7,   130] loss: 50.319\n",
            "[7,   131] loss: 51.015\n",
            "[7,   132] loss: 51.359\n",
            "[7,   133] loss: 51.623\n",
            "[7,   134] loss: 52.146\n",
            "[7,   135] loss: 52.698\n",
            "[7,   136] loss: 53.114\n",
            "[7,   137] loss: 53.517\n",
            "[7,   138] loss: 53.864\n",
            "[7,   139] loss: 54.455\n",
            "[7,   140] loss: 54.938\n",
            "[7,   141] loss: 55.430\n",
            "[7,   142] loss: 55.815\n",
            "[7,   143] loss: 56.147\n",
            "[7,   144] loss: 56.501\n",
            "[7,   145] loss: 56.990\n",
            "[7,   146] loss: 57.410\n",
            "[7,   147] loss: 57.814\n",
            "[7,   148] loss: 58.434\n",
            "[7,   149] loss: 58.973\n",
            "[7,   150] loss: 59.457\n",
            "[7,   151] loss: 60.027\n",
            "[7,   152] loss: 60.395\n",
            "[7,   153] loss: 61.143\n",
            "[7,   154] loss: 61.311\n",
            "[7,   155] loss: 61.643\n",
            "[7,   156] loss: 62.134\n",
            "[7,   157] loss: 62.278\n",
            "[8,     2] loss: 1.025\n",
            "[8,     3] loss: 1.515\n",
            "[8,     4] loss: 1.772\n",
            "[8,     5] loss: 2.133\n",
            "[8,     6] loss: 2.515\n",
            "[8,     7] loss: 2.964\n",
            "[8,     8] loss: 3.248\n",
            "[8,     9] loss: 3.496\n",
            "[8,    10] loss: 3.774\n",
            "[8,    11] loss: 4.002\n",
            "[8,    12] loss: 4.511\n",
            "[8,    13] loss: 4.926\n",
            "[8,    14] loss: 5.233\n",
            "[8,    15] loss: 5.911\n",
            "[8,    16] loss: 6.228\n",
            "[8,    17] loss: 6.522\n",
            "[8,    18] loss: 6.790\n",
            "[8,    19] loss: 7.005\n",
            "[8,    20] loss: 7.276\n",
            "[8,    21] loss: 7.519\n",
            "[8,    22] loss: 7.762\n",
            "[8,    23] loss: 8.198\n",
            "[8,    24] loss: 8.335\n",
            "[8,    25] loss: 8.709\n",
            "[8,    26] loss: 9.223\n",
            "[8,    27] loss: 9.461\n",
            "[8,    28] loss: 9.778\n",
            "[8,    29] loss: 10.155\n",
            "[8,    30] loss: 10.525\n",
            "[8,    31] loss: 10.918\n",
            "[8,    32] loss: 11.150\n",
            "[8,    33] loss: 11.421\n",
            "[8,    34] loss: 11.757\n",
            "[8,    35] loss: 12.034\n",
            "[8,    36] loss: 12.340\n",
            "[8,    37] loss: 12.631\n",
            "[8,    38] loss: 13.155\n",
            "[8,    39] loss: 13.443\n",
            "[8,    40] loss: 13.618\n",
            "[8,    41] loss: 13.830\n",
            "[8,    42] loss: 14.200\n",
            "[8,    43] loss: 14.355\n",
            "[8,    44] loss: 14.678\n",
            "[8,    45] loss: 15.036\n",
            "[8,    46] loss: 15.404\n",
            "[8,    47] loss: 15.727\n",
            "[8,    48] loss: 16.038\n",
            "[8,    49] loss: 16.404\n",
            "[8,    50] loss: 16.639\n",
            "[8,    51] loss: 17.182\n",
            "[8,    52] loss: 17.533\n",
            "[8,    53] loss: 18.045\n",
            "[8,    54] loss: 18.270\n",
            "[8,    55] loss: 18.561\n",
            "[8,    56] loss: 18.754\n",
            "[8,    57] loss: 19.304\n",
            "[8,    58] loss: 19.647\n",
            "[8,    59] loss: 20.003\n",
            "[8,    60] loss: 20.206\n",
            "[8,    61] loss: 20.560\n",
            "[8,    62] loss: 20.924\n",
            "[8,    63] loss: 21.274\n",
            "[8,    64] loss: 21.547\n",
            "[8,    65] loss: 22.016\n",
            "[8,    66] loss: 22.407\n",
            "[8,    67] loss: 22.628\n",
            "[8,    68] loss: 22.889\n",
            "[8,    69] loss: 23.168\n",
            "[8,    70] loss: 23.565\n",
            "[8,    71] loss: 23.986\n",
            "[8,    72] loss: 24.356\n",
            "[8,    73] loss: 24.706\n",
            "[8,    74] loss: 25.158\n",
            "[8,    75] loss: 25.485\n",
            "[8,    76] loss: 26.395\n",
            "[8,    77] loss: 26.948\n",
            "[8,    78] loss: 27.204\n",
            "[8,    79] loss: 27.513\n",
            "[8,    80] loss: 27.810\n",
            "[8,    81] loss: 28.261\n",
            "[8,    82] loss: 28.595\n",
            "[8,    83] loss: 29.002\n",
            "[8,    84] loss: 29.218\n",
            "[8,    85] loss: 29.899\n",
            "[8,    86] loss: 30.566\n",
            "[8,    87] loss: 31.254\n",
            "[8,    88] loss: 31.630\n",
            "[8,    89] loss: 31.922\n",
            "[8,    90] loss: 32.291\n",
            "[8,    91] loss: 32.584\n",
            "[8,    92] loss: 32.896\n",
            "[8,    93] loss: 33.212\n",
            "[8,    94] loss: 33.922\n",
            "[8,    95] loss: 34.606\n",
            "[8,    96] loss: 34.963\n",
            "[8,    97] loss: 35.393\n",
            "[8,    98] loss: 35.827\n",
            "[8,    99] loss: 36.121\n",
            "[8,   100] loss: 36.728\n",
            "[8,   101] loss: 37.053\n",
            "[8,   102] loss: 37.315\n",
            "[8,   103] loss: 37.620\n",
            "[8,   104] loss: 37.971\n",
            "[8,   105] loss: 38.535\n",
            "[8,   106] loss: 38.872\n",
            "[8,   107] loss: 39.480\n",
            "[8,   108] loss: 39.920\n",
            "[8,   109] loss: 40.418\n",
            "[8,   110] loss: 40.684\n",
            "[8,   111] loss: 41.198\n",
            "[8,   112] loss: 41.637\n",
            "[8,   113] loss: 42.121\n",
            "[8,   114] loss: 42.427\n",
            "[8,   115] loss: 42.714\n",
            "[8,   116] loss: 43.027\n",
            "[8,   117] loss: 43.351\n",
            "[8,   118] loss: 43.727\n",
            "[8,   119] loss: 44.208\n",
            "[8,   120] loss: 44.814\n",
            "[8,   121] loss: 45.037\n",
            "[8,   122] loss: 45.345\n",
            "[8,   123] loss: 45.552\n",
            "[8,   124] loss: 45.991\n",
            "[8,   125] loss: 46.274\n",
            "[8,   126] loss: 46.549\n",
            "[8,   127] loss: 47.230\n",
            "[8,   128] loss: 47.769\n",
            "[8,   129] loss: 48.075\n",
            "[8,   130] loss: 48.437\n",
            "[8,   131] loss: 48.789\n",
            "[8,   132] loss: 49.342\n",
            "[8,   133] loss: 49.673\n",
            "[8,   134] loss: 50.268\n",
            "[8,   135] loss: 50.900\n",
            "[8,   136] loss: 51.559\n",
            "[8,   137] loss: 51.886\n",
            "[8,   138] loss: 52.465\n",
            "[8,   139] loss: 52.960\n",
            "[8,   140] loss: 53.127\n",
            "[8,   141] loss: 53.619\n",
            "[8,   142] loss: 54.339\n",
            "[8,   143] loss: 55.128\n",
            "[8,   144] loss: 55.721\n",
            "[8,   145] loss: 56.361\n",
            "[8,   146] loss: 56.586\n",
            "[8,   147] loss: 56.875\n",
            "[8,   148] loss: 57.104\n",
            "[8,   149] loss: 57.641\n",
            "[8,   150] loss: 58.071\n",
            "[8,   151] loss: 58.408\n",
            "[8,   152] loss: 58.652\n",
            "[8,   153] loss: 59.269\n",
            "[8,   154] loss: 59.620\n",
            "[8,   155] loss: 59.924\n",
            "[8,   156] loss: 60.508\n",
            "[8,   157] loss: 61.159\n",
            "[9,     2] loss: 0.663\n",
            "[9,     3] loss: 0.956\n",
            "[9,     4] loss: 1.478\n",
            "[9,     5] loss: 2.099\n",
            "[9,     6] loss: 2.577\n",
            "[9,     7] loss: 2.950\n",
            "[9,     8] loss: 3.218\n",
            "[9,     9] loss: 3.391\n",
            "[9,    10] loss: 3.818\n",
            "[9,    11] loss: 4.088\n",
            "[9,    12] loss: 4.413\n",
            "[9,    13] loss: 4.662\n",
            "[9,    14] loss: 5.034\n",
            "[9,    15] loss: 5.577\n",
            "[9,    16] loss: 6.102\n",
            "[9,    17] loss: 6.309\n",
            "[9,    18] loss: 6.589\n",
            "[9,    19] loss: 6.799\n",
            "[9,    20] loss: 6.988\n",
            "[9,    21] loss: 7.455\n",
            "[9,    22] loss: 7.935\n",
            "[9,    23] loss: 8.233\n",
            "[9,    24] loss: 8.689\n",
            "[9,    25] loss: 8.859\n",
            "[9,    26] loss: 9.116\n",
            "[9,    27] loss: 9.327\n",
            "[9,    28] loss: 9.825\n",
            "[9,    29] loss: 10.044\n",
            "[9,    30] loss: 10.365\n",
            "[9,    31] loss: 10.672\n",
            "[9,    32] loss: 11.009\n",
            "[9,    33] loss: 11.418\n",
            "[9,    34] loss: 11.773\n",
            "[9,    35] loss: 12.076\n",
            "[9,    36] loss: 12.431\n",
            "[9,    37] loss: 12.725\n",
            "[9,    38] loss: 13.405\n",
            "[9,    39] loss: 13.733\n",
            "[9,    40] loss: 13.948\n",
            "[9,    41] loss: 14.574\n",
            "[9,    42] loss: 14.908\n",
            "[9,    43] loss: 15.186\n",
            "[9,    44] loss: 15.501\n",
            "[9,    45] loss: 15.913\n",
            "[9,    46] loss: 16.120\n",
            "[9,    47] loss: 16.311\n",
            "[9,    48] loss: 16.546\n",
            "[9,    49] loss: 16.944\n",
            "[9,    50] loss: 17.179\n",
            "[9,    51] loss: 17.638\n",
            "[9,    52] loss: 17.986\n",
            "[9,    53] loss: 18.654\n",
            "[9,    54] loss: 19.491\n",
            "[9,    55] loss: 19.733\n",
            "[9,    56] loss: 19.934\n",
            "[9,    57] loss: 20.565\n",
            "[9,    58] loss: 20.804\n",
            "[9,    59] loss: 21.322\n",
            "[9,    60] loss: 21.610\n",
            "[9,    61] loss: 21.869\n",
            "[9,    62] loss: 22.187\n",
            "[9,    63] loss: 22.587\n",
            "[9,    64] loss: 22.735\n",
            "[9,    65] loss: 23.257\n",
            "[9,    66] loss: 23.534\n",
            "[9,    67] loss: 23.989\n",
            "[9,    68] loss: 24.379\n",
            "[9,    69] loss: 24.691\n",
            "[9,    70] loss: 24.981\n",
            "[9,    71] loss: 25.320\n",
            "[9,    72] loss: 25.841\n",
            "[9,    73] loss: 26.268\n",
            "[9,    74] loss: 26.531\n",
            "[9,    75] loss: 26.999\n",
            "[9,    76] loss: 27.399\n",
            "[9,    77] loss: 27.662\n",
            "[9,    78] loss: 28.066\n",
            "[9,    79] loss: 28.348\n",
            "[9,    80] loss: 28.697\n",
            "[9,    81] loss: 29.062\n",
            "[9,    82] loss: 29.231\n",
            "[9,    83] loss: 29.533\n",
            "[9,    84] loss: 30.037\n",
            "[9,    85] loss: 30.340\n",
            "[9,    86] loss: 30.791\n",
            "[9,    87] loss: 31.169\n",
            "[9,    88] loss: 31.578\n",
            "[9,    89] loss: 32.249\n",
            "[9,    90] loss: 32.709\n",
            "[9,    91] loss: 32.983\n",
            "[9,    92] loss: 33.268\n",
            "[9,    93] loss: 33.688\n",
            "[9,    94] loss: 34.022\n",
            "[9,    95] loss: 34.548\n",
            "[9,    96] loss: 34.914\n",
            "[9,    97] loss: 35.580\n",
            "[9,    98] loss: 35.926\n",
            "[9,    99] loss: 36.049\n",
            "[9,   100] loss: 36.390\n",
            "[9,   101] loss: 37.172\n",
            "[9,   102] loss: 37.493\n",
            "[9,   103] loss: 37.767\n",
            "[9,   104] loss: 38.146\n",
            "[9,   105] loss: 38.533\n",
            "[9,   106] loss: 38.776\n",
            "[9,   107] loss: 39.060\n",
            "[9,   108] loss: 39.386\n",
            "[9,   109] loss: 39.762\n",
            "[9,   110] loss: 40.299\n",
            "[9,   111] loss: 40.805\n",
            "[9,   112] loss: 41.091\n",
            "[9,   113] loss: 41.353\n",
            "[9,   114] loss: 42.001\n",
            "[9,   115] loss: 42.260\n",
            "[9,   116] loss: 42.655\n",
            "[9,   117] loss: 43.012\n",
            "[9,   118] loss: 43.257\n",
            "[9,   119] loss: 43.602\n",
            "[9,   120] loss: 44.069\n",
            "[9,   121] loss: 44.307\n",
            "[9,   122] loss: 44.566\n",
            "[9,   123] loss: 44.938\n",
            "[9,   124] loss: 45.294\n",
            "[9,   125] loss: 45.646\n",
            "[9,   126] loss: 46.018\n",
            "[9,   127] loss: 46.328\n",
            "[9,   128] loss: 46.756\n",
            "[9,   129] loss: 47.212\n",
            "[9,   130] loss: 47.363\n",
            "[9,   131] loss: 47.778\n",
            "[9,   132] loss: 48.401\n",
            "[9,   133] loss: 48.671\n",
            "[9,   134] loss: 49.005\n",
            "[9,   135] loss: 49.273\n",
            "[9,   136] loss: 49.799\n",
            "[9,   137] loss: 50.002\n",
            "[9,   138] loss: 50.308\n",
            "[9,   139] loss: 50.719\n",
            "[9,   140] loss: 51.064\n",
            "[9,   141] loss: 51.333\n",
            "[9,   142] loss: 51.815\n",
            "[9,   143] loss: 52.325\n",
            "[9,   144] loss: 52.771\n",
            "[9,   145] loss: 52.950\n",
            "[9,   146] loss: 53.329\n",
            "[9,   147] loss: 53.682\n",
            "[9,   148] loss: 53.962\n",
            "[9,   149] loss: 54.307\n",
            "[9,   150] loss: 54.706\n",
            "[9,   151] loss: 54.943\n",
            "[9,   152] loss: 55.257\n",
            "[9,   153] loss: 55.524\n",
            "[9,   154] loss: 55.873\n",
            "[9,   155] loss: 56.177\n",
            "[9,   156] loss: 56.481\n",
            "[9,   157] loss: 57.012\n",
            "[10,     2] loss: 0.521\n",
            "[10,     3] loss: 0.879\n",
            "[10,     4] loss: 1.267\n",
            "[10,     5] loss: 1.508\n",
            "[10,     6] loss: 1.630\n",
            "[10,     7] loss: 1.962\n",
            "[10,     8] loss: 2.146\n",
            "[10,     9] loss: 2.577\n",
            "[10,    10] loss: 2.779\n",
            "[10,    11] loss: 2.968\n",
            "[10,    12] loss: 3.297\n",
            "[10,    13] loss: 3.753\n",
            "[10,    14] loss: 4.099\n",
            "[10,    15] loss: 4.350\n",
            "[10,    16] loss: 4.545\n",
            "[10,    17] loss: 4.818\n",
            "[10,    18] loss: 5.179\n",
            "[10,    19] loss: 5.525\n",
            "[10,    20] loss: 5.926\n",
            "[10,    21] loss: 6.179\n",
            "[10,    22] loss: 6.515\n",
            "[10,    23] loss: 6.758\n",
            "[10,    24] loss: 7.007\n",
            "[10,    25] loss: 7.210\n",
            "[10,    26] loss: 7.396\n",
            "[10,    27] loss: 7.874\n",
            "[10,    28] loss: 8.082\n",
            "[10,    29] loss: 8.457\n",
            "[10,    30] loss: 8.776\n",
            "[10,    31] loss: 9.051\n",
            "[10,    32] loss: 9.267\n",
            "[10,    33] loss: 9.539\n",
            "[10,    34] loss: 9.754\n",
            "[10,    35] loss: 9.964\n",
            "[10,    36] loss: 10.114\n",
            "[10,    37] loss: 10.360\n",
            "[10,    38] loss: 10.676\n",
            "[10,    39] loss: 10.997\n",
            "[10,    40] loss: 11.183\n",
            "[10,    41] loss: 11.472\n",
            "[10,    42] loss: 11.843\n",
            "[10,    43] loss: 12.184\n",
            "[10,    44] loss: 12.471\n",
            "[10,    45] loss: 12.718\n",
            "[10,    46] loss: 12.852\n",
            "[10,    47] loss: 13.062\n",
            "[10,    48] loss: 13.215\n",
            "[10,    49] loss: 13.568\n",
            "[10,    50] loss: 14.006\n",
            "[10,    51] loss: 14.392\n",
            "[10,    52] loss: 14.710\n",
            "[10,    53] loss: 14.965\n",
            "[10,    54] loss: 15.221\n",
            "[10,    55] loss: 15.870\n",
            "[10,    56] loss: 16.212\n",
            "[10,    57] loss: 16.465\n",
            "[10,    58] loss: 16.709\n",
            "[10,    59] loss: 16.947\n",
            "[10,    60] loss: 17.127\n",
            "[10,    61] loss: 17.385\n",
            "[10,    62] loss: 17.541\n",
            "[10,    63] loss: 17.778\n",
            "[10,    64] loss: 18.116\n",
            "[10,    65] loss: 18.340\n",
            "[10,    66] loss: 18.625\n",
            "[10,    67] loss: 18.934\n",
            "[10,    68] loss: 19.151\n",
            "[10,    69] loss: 19.361\n",
            "[10,    70] loss: 19.979\n",
            "[10,    71] loss: 20.375\n",
            "[10,    72] loss: 20.483\n",
            "[10,    73] loss: 20.721\n",
            "[10,    74] loss: 21.017\n",
            "[10,    75] loss: 21.239\n",
            "[10,    76] loss: 21.612\n",
            "[10,    77] loss: 22.210\n",
            "[10,    78] loss: 22.564\n",
            "[10,    79] loss: 22.903\n",
            "[10,    80] loss: 23.085\n",
            "[10,    81] loss: 23.343\n",
            "[10,    82] loss: 23.690\n",
            "[10,    83] loss: 23.910\n",
            "[10,    84] loss: 24.270\n",
            "[10,    85] loss: 24.583\n",
            "[10,    86] loss: 25.010\n",
            "[10,    87] loss: 25.272\n",
            "[10,    88] loss: 25.445\n",
            "[10,    89] loss: 25.658\n",
            "[10,    90] loss: 26.140\n",
            "[10,    91] loss: 26.477\n",
            "[10,    92] loss: 26.763\n",
            "[10,    93] loss: 27.020\n",
            "[10,    94] loss: 27.361\n",
            "[10,    95] loss: 27.573\n",
            "[10,    96] loss: 27.733\n",
            "[10,    97] loss: 27.984\n",
            "[10,    98] loss: 28.363\n",
            "[10,    99] loss: 28.865\n",
            "[10,   100] loss: 29.160\n",
            "[10,   101] loss: 29.497\n",
            "[10,   102] loss: 29.747\n",
            "[10,   103] loss: 29.955\n",
            "[10,   104] loss: 30.300\n",
            "[10,   105] loss: 30.590\n",
            "[10,   106] loss: 30.767\n",
            "[10,   107] loss: 30.919\n",
            "[10,   108] loss: 31.083\n",
            "[10,   109] loss: 31.283\n",
            "[10,   110] loss: 31.388\n",
            "[10,   111] loss: 31.745\n",
            "[10,   112] loss: 32.092\n",
            "[10,   113] loss: 32.429\n",
            "[10,   114] loss: 32.531\n",
            "[10,   115] loss: 32.824\n",
            "[10,   116] loss: 33.096\n",
            "[10,   117] loss: 33.457\n",
            "[10,   118] loss: 33.803\n",
            "[10,   119] loss: 34.206\n",
            "[10,   120] loss: 34.568\n",
            "[10,   121] loss: 35.020\n",
            "[10,   122] loss: 35.227\n",
            "[10,   123] loss: 35.539\n",
            "[10,   124] loss: 35.790\n",
            "[10,   125] loss: 36.069\n",
            "[10,   126] loss: 36.262\n",
            "[10,   127] loss: 36.425\n",
            "[10,   128] loss: 36.570\n",
            "[10,   129] loss: 36.804\n",
            "[10,   130] loss: 37.421\n",
            "[10,   131] loss: 37.800\n",
            "[10,   132] loss: 38.027\n",
            "[10,   133] loss: 38.402\n",
            "[10,   134] loss: 38.763\n",
            "[10,   135] loss: 39.181\n",
            "[10,   136] loss: 39.639\n",
            "[10,   137] loss: 39.801\n",
            "[10,   138] loss: 40.119\n",
            "[10,   139] loss: 40.540\n",
            "[10,   140] loss: 40.945\n",
            "[10,   141] loss: 41.145\n",
            "[10,   142] loss: 41.330\n",
            "[10,   143] loss: 41.410\n",
            "[10,   144] loss: 41.671\n",
            "[10,   145] loss: 42.118\n",
            "[10,   146] loss: 42.422\n",
            "[10,   147] loss: 42.832\n",
            "[10,   148] loss: 43.184\n",
            "[10,   149] loss: 43.444\n",
            "[10,   150] loss: 43.697\n",
            "[10,   151] loss: 44.162\n",
            "[10,   152] loss: 44.412\n",
            "[10,   153] loss: 44.953\n",
            "[10,   154] loss: 45.151\n",
            "[10,   155] loss: 45.467\n",
            "[10,   156] loss: 45.642\n",
            "[10,   157] loss: 45.736\n",
            "[11,     2] loss: 0.480\n",
            "[11,     3] loss: 0.773\n",
            "[11,     4] loss: 1.032\n",
            "[11,     5] loss: 1.217\n",
            "[11,     6] loss: 1.527\n",
            "[11,     7] loss: 1.965\n",
            "[11,     8] loss: 2.215\n",
            "[11,     9] loss: 2.457\n",
            "[11,    10] loss: 2.828\n",
            "[11,    11] loss: 3.140\n",
            "[11,    12] loss: 3.392\n",
            "[11,    13] loss: 3.653\n",
            "[11,    14] loss: 3.819\n",
            "[11,    15] loss: 4.210\n",
            "[11,    16] loss: 4.333\n",
            "[11,    17] loss: 4.567\n",
            "[11,    18] loss: 4.904\n",
            "[11,    19] loss: 5.090\n",
            "[11,    20] loss: 5.350\n",
            "[11,    21] loss: 5.614\n",
            "[11,    22] loss: 5.755\n",
            "[11,    23] loss: 6.175\n",
            "[11,    24] loss: 6.361\n",
            "[11,    25] loss: 6.539\n",
            "[11,    26] loss: 6.658\n",
            "[11,    27] loss: 6.915\n",
            "[11,    28] loss: 7.063\n",
            "[11,    29] loss: 7.346\n",
            "[11,    30] loss: 7.487\n",
            "[11,    31] loss: 7.690\n",
            "[11,    32] loss: 7.893\n",
            "[11,    33] loss: 8.148\n",
            "[11,    34] loss: 8.413\n",
            "[11,    35] loss: 8.720\n",
            "[11,    36] loss: 8.982\n",
            "[11,    37] loss: 9.100\n",
            "[11,    38] loss: 9.278\n",
            "[11,    39] loss: 9.531\n",
            "[11,    40] loss: 9.671\n",
            "[11,    41] loss: 9.871\n",
            "[11,    42] loss: 10.224\n",
            "[11,    43] loss: 10.350\n",
            "[11,    44] loss: 10.525\n",
            "[11,    45] loss: 10.778\n",
            "[11,    46] loss: 11.130\n",
            "[11,    47] loss: 11.348\n",
            "[11,    48] loss: 11.508\n",
            "[11,    49] loss: 11.722\n",
            "[11,    50] loss: 11.811\n",
            "[11,    51] loss: 12.122\n",
            "[11,    52] loss: 12.512\n",
            "[11,    53] loss: 12.658\n",
            "[11,    54] loss: 13.017\n",
            "[11,    55] loss: 13.186\n",
            "[11,    56] loss: 13.367\n",
            "[11,    57] loss: 13.732\n",
            "[11,    58] loss: 13.900\n",
            "[11,    59] loss: 14.116\n",
            "[11,    60] loss: 14.328\n",
            "[11,    61] loss: 14.718\n",
            "[11,    62] loss: 14.996\n",
            "[11,    63] loss: 15.101\n",
            "[11,    64] loss: 15.339\n",
            "[11,    65] loss: 15.702\n",
            "[11,    66] loss: 15.928\n",
            "[11,    67] loss: 16.235\n",
            "[11,    68] loss: 16.436\n",
            "[11,    69] loss: 16.851\n",
            "[11,    70] loss: 17.166\n",
            "[11,    71] loss: 17.314\n",
            "[11,    72] loss: 17.679\n",
            "[11,    73] loss: 18.130\n",
            "[11,    74] loss: 18.654\n",
            "[11,    75] loss: 18.916\n",
            "[11,    76] loss: 19.271\n",
            "[11,    77] loss: 19.527\n",
            "[11,    78] loss: 19.668\n",
            "[11,    79] loss: 19.972\n",
            "[11,    80] loss: 20.318\n",
            "[11,    81] loss: 20.566\n",
            "[11,    82] loss: 21.005\n",
            "[11,    83] loss: 21.405\n",
            "[11,    84] loss: 21.621\n",
            "[11,    85] loss: 21.870\n",
            "[11,    86] loss: 22.014\n",
            "[11,    87] loss: 22.212\n",
            "[11,    88] loss: 22.411\n",
            "[11,    89] loss: 22.979\n",
            "[11,    90] loss: 23.384\n",
            "[11,    91] loss: 23.543\n",
            "[11,    92] loss: 23.686\n",
            "[11,    93] loss: 24.068\n",
            "[11,    94] loss: 24.639\n",
            "[11,    95] loss: 25.093\n",
            "[11,    96] loss: 25.375\n",
            "[11,    97] loss: 25.711\n",
            "[11,    98] loss: 26.367\n",
            "[11,    99] loss: 26.831\n",
            "[11,   100] loss: 27.080\n",
            "[11,   101] loss: 27.381\n",
            "[11,   102] loss: 27.745\n",
            "[11,   103] loss: 27.954\n",
            "[11,   104] loss: 28.288\n",
            "[11,   105] loss: 28.592\n",
            "[11,   106] loss: 28.883\n",
            "[11,   107] loss: 29.110\n",
            "[11,   108] loss: 29.327\n",
            "[11,   109] loss: 29.684\n",
            "[11,   110] loss: 29.955\n",
            "[11,   111] loss: 30.281\n",
            "[11,   112] loss: 30.450\n",
            "[11,   113] loss: 30.751\n",
            "[11,   114] loss: 31.174\n",
            "[11,   115] loss: 31.438\n",
            "[11,   116] loss: 31.639\n",
            "[11,   117] loss: 31.807\n",
            "[11,   118] loss: 32.323\n",
            "[11,   119] loss: 32.535\n",
            "[11,   120] loss: 32.717\n",
            "[11,   121] loss: 32.914\n",
            "[11,   122] loss: 33.244\n",
            "[11,   123] loss: 33.516\n",
            "[11,   124] loss: 33.759\n",
            "[11,   125] loss: 34.077\n",
            "[11,   126] loss: 34.227\n",
            "[11,   127] loss: 34.498\n",
            "[11,   128] loss: 34.706\n",
            "[11,   129] loss: 34.871\n",
            "[11,   130] loss: 35.174\n",
            "[11,   131] loss: 35.443\n",
            "[11,   132] loss: 35.708\n",
            "[11,   133] loss: 35.868\n",
            "[11,   134] loss: 36.248\n",
            "[11,   135] loss: 36.622\n",
            "[11,   136] loss: 36.955\n",
            "[11,   137] loss: 37.170\n",
            "[11,   138] loss: 37.320\n",
            "[11,   139] loss: 37.918\n",
            "[11,   140] loss: 38.278\n",
            "[11,   141] loss: 38.437\n",
            "[11,   142] loss: 38.777\n",
            "[11,   143] loss: 39.087\n",
            "[11,   144] loss: 39.260\n",
            "[11,   145] loss: 39.421\n",
            "[11,   146] loss: 39.688\n",
            "[11,   147] loss: 39.895\n",
            "[11,   148] loss: 40.296\n",
            "[11,   149] loss: 40.613\n",
            "[11,   150] loss: 40.776\n",
            "[11,   151] loss: 41.107\n",
            "[11,   152] loss: 41.430\n",
            "[11,   153] loss: 41.651\n",
            "[11,   154] loss: 41.942\n",
            "[11,   155] loss: 42.282\n",
            "[11,   156] loss: 42.615\n",
            "[11,   157] loss: 42.833\n",
            "[12,     2] loss: 0.434\n",
            "[12,     3] loss: 0.619\n",
            "[12,     4] loss: 0.920\n",
            "[12,     5] loss: 1.070\n",
            "[12,     6] loss: 1.242\n",
            "[12,     7] loss: 1.415\n",
            "[12,     8] loss: 1.705\n",
            "[12,     9] loss: 1.986\n",
            "[12,    10] loss: 2.290\n",
            "[12,    11] loss: 2.476\n",
            "[12,    12] loss: 2.621\n",
            "[12,    13] loss: 2.876\n",
            "[12,    14] loss: 3.006\n",
            "[12,    15] loss: 3.240\n",
            "[12,    16] loss: 3.507\n",
            "[12,    17] loss: 3.786\n",
            "[12,    18] loss: 4.123\n",
            "[12,    19] loss: 4.581\n",
            "[12,    20] loss: 4.740\n",
            "[12,    21] loss: 4.955\n",
            "[12,    22] loss: 5.201\n",
            "[12,    23] loss: 5.498\n",
            "[12,    24] loss: 5.804\n",
            "[12,    25] loss: 6.002\n",
            "[12,    26] loss: 6.152\n",
            "[12,    27] loss: 6.406\n",
            "[12,    28] loss: 6.699\n",
            "[12,    29] loss: 6.908\n",
            "[12,    30] loss: 7.026\n",
            "[12,    31] loss: 7.190\n",
            "[12,    32] loss: 7.336\n",
            "[12,    33] loss: 7.571\n",
            "[12,    34] loss: 7.813\n",
            "[12,    35] loss: 7.952\n",
            "[12,    36] loss: 8.244\n",
            "[12,    37] loss: 8.375\n",
            "[12,    38] loss: 8.642\n",
            "[12,    39] loss: 8.729\n",
            "[12,    40] loss: 8.906\n",
            "[12,    41] loss: 9.113\n",
            "[12,    42] loss: 9.232\n",
            "[12,    43] loss: 9.535\n",
            "[12,    44] loss: 9.800\n",
            "[12,    45] loss: 9.976\n",
            "[12,    46] loss: 10.255\n",
            "[12,    47] loss: 10.445\n",
            "[12,    48] loss: 10.643\n",
            "[12,    49] loss: 10.872\n",
            "[12,    50] loss: 11.019\n",
            "[12,    51] loss: 11.203\n",
            "[12,    52] loss: 11.425\n",
            "[12,    53] loss: 11.628\n",
            "[12,    54] loss: 11.758\n",
            "[12,    55] loss: 12.070\n",
            "[12,    56] loss: 12.321\n",
            "[12,    57] loss: 12.434\n",
            "[12,    58] loss: 12.532\n",
            "[12,    59] loss: 12.791\n",
            "[12,    60] loss: 12.926\n",
            "[12,    61] loss: 13.164\n",
            "[12,    62] loss: 13.286\n",
            "[12,    63] loss: 13.584\n",
            "[12,    64] loss: 13.788\n",
            "[12,    65] loss: 13.981\n",
            "[12,    66] loss: 14.252\n",
            "[12,    67] loss: 14.379\n",
            "[12,    68] loss: 14.631\n",
            "[12,    69] loss: 14.908\n",
            "[12,    70] loss: 15.140\n",
            "[12,    71] loss: 15.360\n",
            "[12,    72] loss: 15.639\n",
            "[12,    73] loss: 15.903\n",
            "[12,    74] loss: 16.063\n",
            "[12,    75] loss: 16.212\n",
            "[12,    76] loss: 16.706\n",
            "[12,    77] loss: 16.998\n",
            "[12,    78] loss: 17.290\n",
            "[12,    79] loss: 17.451\n",
            "[12,    80] loss: 17.581\n",
            "[12,    81] loss: 17.783\n",
            "[12,    82] loss: 18.086\n",
            "[12,    83] loss: 18.258\n",
            "[12,    84] loss: 18.600\n",
            "[12,    85] loss: 18.729\n",
            "[12,    86] loss: 18.903\n",
            "[12,    87] loss: 19.221\n",
            "[12,    88] loss: 19.538\n",
            "[12,    89] loss: 19.800\n",
            "[12,    90] loss: 20.131\n",
            "[12,    91] loss: 20.215\n",
            "[12,    92] loss: 20.590\n",
            "[12,    93] loss: 20.797\n",
            "[12,    94] loss: 21.063\n",
            "[12,    95] loss: 21.468\n",
            "[12,    96] loss: 21.759\n",
            "[12,    97] loss: 22.059\n",
            "[12,    98] loss: 22.251\n",
            "[12,    99] loss: 22.506\n",
            "[12,   100] loss: 22.690\n",
            "[12,   101] loss: 23.263\n",
            "[12,   102] loss: 23.560\n",
            "[12,   103] loss: 23.777\n",
            "[12,   104] loss: 24.043\n",
            "[12,   105] loss: 24.255\n",
            "[12,   106] loss: 24.470\n",
            "[12,   107] loss: 24.732\n",
            "[12,   108] loss: 25.043\n",
            "[12,   109] loss: 25.247\n",
            "[12,   110] loss: 25.506\n",
            "[12,   111] loss: 25.820\n",
            "[12,   112] loss: 26.024\n",
            "[12,   113] loss: 26.236\n",
            "[12,   114] loss: 26.328\n",
            "[12,   115] loss: 26.583\n",
            "[12,   116] loss: 26.808\n",
            "[12,   117] loss: 27.032\n",
            "[12,   118] loss: 27.222\n",
            "[12,   119] loss: 27.624\n",
            "[12,   120] loss: 27.919\n",
            "[12,   121] loss: 28.134\n",
            "[12,   122] loss: 28.369\n",
            "[12,   123] loss: 28.582\n",
            "[12,   124] loss: 28.777\n",
            "[12,   125] loss: 29.105\n",
            "[12,   126] loss: 29.303\n",
            "[12,   127] loss: 29.452\n",
            "[12,   128] loss: 29.701\n",
            "[12,   129] loss: 29.863\n",
            "[12,   130] loss: 30.158\n",
            "[12,   131] loss: 30.627\n",
            "[12,   132] loss: 30.919\n",
            "[12,   133] loss: 31.154\n",
            "[12,   134] loss: 31.680\n",
            "[12,   135] loss: 31.902\n",
            "[12,   136] loss: 32.082\n",
            "[12,   137] loss: 32.330\n",
            "[12,   138] loss: 32.739\n",
            "[12,   139] loss: 32.976\n",
            "[12,   140] loss: 33.279\n",
            "[12,   141] loss: 33.427\n",
            "[12,   142] loss: 33.738\n",
            "[12,   143] loss: 34.016\n",
            "[12,   144] loss: 34.162\n",
            "[12,   145] loss: 34.374\n",
            "[12,   146] loss: 34.682\n",
            "[12,   147] loss: 34.770\n",
            "[12,   148] loss: 35.016\n",
            "[12,   149] loss: 35.257\n",
            "[12,   150] loss: 35.654\n",
            "[12,   151] loss: 35.910\n",
            "[12,   152] loss: 36.255\n",
            "[12,   153] loss: 36.766\n",
            "[12,   154] loss: 36.903\n",
            "[12,   155] loss: 37.128\n",
            "[12,   156] loss: 37.409\n",
            "[12,   157] loss: 37.517\n",
            "[13,     2] loss: 0.413\n",
            "[13,     3] loss: 0.598\n",
            "[13,     4] loss: 0.814\n",
            "[13,     5] loss: 1.082\n",
            "[13,     6] loss: 1.258\n",
            "[13,     7] loss: 1.433\n",
            "[13,     8] loss: 1.617\n",
            "[13,     9] loss: 1.855\n",
            "[13,    10] loss: 2.114\n",
            "[13,    11] loss: 2.392\n",
            "[13,    12] loss: 2.705\n",
            "[13,    13] loss: 2.823\n",
            "[13,    14] loss: 3.015\n",
            "[13,    15] loss: 3.166\n",
            "[13,    16] loss: 3.410\n",
            "[13,    17] loss: 3.551\n",
            "[13,    18] loss: 3.704\n",
            "[13,    19] loss: 3.824\n",
            "[13,    20] loss: 3.974\n",
            "[13,    21] loss: 4.170\n",
            "[13,    22] loss: 4.320\n",
            "[13,    23] loss: 4.408\n",
            "[13,    24] loss: 4.581\n",
            "[13,    25] loss: 4.933\n",
            "[13,    26] loss: 5.035\n",
            "[13,    27] loss: 5.122\n",
            "[13,    28] loss: 5.361\n",
            "[13,    29] loss: 5.586\n",
            "[13,    30] loss: 5.865\n",
            "[13,    31] loss: 5.992\n",
            "[13,    32] loss: 6.097\n",
            "[13,    33] loss: 6.247\n",
            "[13,    34] loss: 6.553\n",
            "[13,    35] loss: 6.700\n",
            "[13,    36] loss: 6.766\n",
            "[13,    37] loss: 6.878\n",
            "[13,    38] loss: 7.025\n",
            "[13,    39] loss: 7.240\n",
            "[13,    40] loss: 7.468\n",
            "[13,    41] loss: 7.697\n",
            "[13,    42] loss: 7.818\n",
            "[13,    43] loss: 8.041\n",
            "[13,    44] loss: 8.162\n",
            "[13,    45] loss: 8.260\n",
            "[13,    46] loss: 8.371\n",
            "[13,    47] loss: 8.619\n",
            "[13,    48] loss: 8.935\n",
            "[13,    49] loss: 9.269\n",
            "[13,    50] loss: 9.437\n",
            "[13,    51] loss: 9.758\n",
            "[13,    52] loss: 9.933\n",
            "[13,    53] loss: 10.099\n",
            "[13,    54] loss: 10.254\n",
            "[13,    55] loss: 10.531\n",
            "[13,    56] loss: 10.712\n",
            "[13,    57] loss: 10.968\n",
            "[13,    58] loss: 11.227\n",
            "[13,    59] loss: 11.353\n",
            "[13,    60] loss: 11.511\n",
            "[13,    61] loss: 11.807\n",
            "[13,    62] loss: 12.020\n",
            "[13,    63] loss: 12.180\n",
            "[13,    64] loss: 12.367\n",
            "[13,    65] loss: 12.640\n",
            "[13,    66] loss: 12.694\n",
            "[13,    67] loss: 12.938\n",
            "[13,    68] loss: 13.317\n",
            "[13,    69] loss: 13.731\n",
            "[13,    70] loss: 13.845\n",
            "[13,    71] loss: 13.984\n",
            "[13,    72] loss: 14.199\n",
            "[13,    73] loss: 14.444\n",
            "[13,    74] loss: 14.686\n",
            "[13,    75] loss: 15.032\n",
            "[13,    76] loss: 15.193\n",
            "[13,    77] loss: 15.334\n",
            "[13,    78] loss: 15.547\n",
            "[13,    79] loss: 15.696\n",
            "[13,    80] loss: 15.942\n",
            "[13,    81] loss: 16.134\n",
            "[13,    82] loss: 16.293\n",
            "[13,    83] loss: 16.411\n",
            "[13,    84] loss: 16.506\n",
            "[13,    85] loss: 16.718\n",
            "[13,    86] loss: 16.843\n",
            "[13,    87] loss: 16.953\n",
            "[13,    88] loss: 17.081\n",
            "[13,    89] loss: 17.196\n",
            "[13,    90] loss: 17.272\n",
            "[13,    91] loss: 17.453\n",
            "[13,    92] loss: 17.548\n",
            "[13,    93] loss: 17.779\n",
            "[13,    94] loss: 18.135\n",
            "[13,    95] loss: 18.258\n",
            "[13,    96] loss: 18.510\n",
            "[13,    97] loss: 18.719\n",
            "[13,    98] loss: 18.981\n",
            "[13,    99] loss: 19.099\n",
            "[13,   100] loss: 19.218\n",
            "[13,   101] loss: 19.362\n",
            "[13,   102] loss: 19.518\n",
            "[13,   103] loss: 19.583\n",
            "[13,   104] loss: 19.619\n",
            "[13,   105] loss: 19.827\n",
            "[13,   106] loss: 20.068\n",
            "[13,   107] loss: 20.207\n",
            "[13,   108] loss: 20.420\n",
            "[13,   109] loss: 20.507\n",
            "[13,   110] loss: 20.569\n",
            "[13,   111] loss: 20.676\n",
            "[13,   112] loss: 20.919\n",
            "[13,   113] loss: 21.095\n",
            "[13,   114] loss: 21.259\n",
            "[13,   115] loss: 21.542\n",
            "[13,   116] loss: 21.625\n",
            "[13,   117] loss: 21.712\n",
            "[13,   118] loss: 21.907\n",
            "[13,   119] loss: 22.122\n",
            "[13,   120] loss: 22.326\n",
            "[13,   121] loss: 22.423\n",
            "[13,   122] loss: 22.622\n",
            "[13,   123] loss: 22.710\n",
            "[13,   124] loss: 22.973\n",
            "[13,   125] loss: 23.280\n",
            "[13,   126] loss: 23.508\n",
            "[13,   127] loss: 23.646\n",
            "[13,   128] loss: 23.921\n",
            "[13,   129] loss: 24.075\n",
            "[13,   130] loss: 24.334\n",
            "[13,   131] loss: 24.396\n",
            "[13,   132] loss: 24.606\n",
            "[13,   133] loss: 24.893\n",
            "[13,   134] loss: 25.208\n",
            "[13,   135] loss: 25.530\n",
            "[13,   136] loss: 25.630\n",
            "[13,   137] loss: 25.882\n",
            "[13,   138] loss: 25.973\n",
            "[13,   139] loss: 26.295\n",
            "[13,   140] loss: 26.638\n",
            "[13,   141] loss: 26.859\n",
            "[13,   142] loss: 27.187\n",
            "[13,   143] loss: 27.540\n",
            "[13,   144] loss: 27.810\n",
            "[13,   145] loss: 27.948\n",
            "[13,   146] loss: 28.125\n",
            "[13,   147] loss: 28.320\n",
            "[13,   148] loss: 28.499\n",
            "[13,   149] loss: 28.784\n",
            "[13,   150] loss: 29.017\n",
            "[13,   151] loss: 29.165\n",
            "[13,   152] loss: 29.358\n",
            "[13,   153] loss: 29.620\n",
            "[13,   154] loss: 29.764\n",
            "[13,   155] loss: 30.076\n",
            "[13,   156] loss: 30.287\n",
            "[13,   157] loss: 30.550\n",
            "[14,     2] loss: 0.193\n",
            "[14,     3] loss: 0.354\n",
            "[14,     4] loss: 0.494\n",
            "[14,     5] loss: 0.574\n",
            "[14,     6] loss: 0.744\n",
            "[14,     7] loss: 0.911\n",
            "[14,     8] loss: 0.991\n",
            "[14,     9] loss: 1.098\n",
            "[14,    10] loss: 1.215\n",
            "[14,    11] loss: 1.315\n",
            "[14,    12] loss: 1.465\n",
            "[14,    13] loss: 1.638\n",
            "[14,    14] loss: 1.750\n",
            "[14,    15] loss: 1.937\n",
            "[14,    16] loss: 2.083\n",
            "[14,    17] loss: 2.200\n",
            "[14,    18] loss: 2.371\n",
            "[14,    19] loss: 2.557\n",
            "[14,    20] loss: 2.748\n",
            "[14,    21] loss: 2.917\n",
            "[14,    22] loss: 2.965\n",
            "[14,    23] loss: 3.139\n",
            "[14,    24] loss: 3.285\n",
            "[14,    25] loss: 3.376\n",
            "[14,    26] loss: 3.552\n",
            "[14,    27] loss: 3.826\n",
            "[14,    28] loss: 3.872\n",
            "[14,    29] loss: 4.032\n",
            "[14,    30] loss: 4.157\n",
            "[14,    31] loss: 4.261\n",
            "[14,    32] loss: 4.360\n",
            "[14,    33] loss: 4.461\n",
            "[14,    34] loss: 4.537\n",
            "[14,    35] loss: 4.721\n",
            "[14,    36] loss: 4.884\n",
            "[14,    37] loss: 5.015\n",
            "[14,    38] loss: 5.173\n",
            "[14,    39] loss: 5.490\n",
            "[14,    40] loss: 5.633\n",
            "[14,    41] loss: 5.801\n",
            "[14,    42] loss: 5.881\n",
            "[14,    43] loss: 6.027\n",
            "[14,    44] loss: 6.139\n",
            "[14,    45] loss: 6.193\n",
            "[14,    46] loss: 6.473\n",
            "[14,    47] loss: 6.596\n",
            "[14,    48] loss: 6.685\n",
            "[14,    49] loss: 6.819\n",
            "[14,    50] loss: 7.017\n",
            "[14,    51] loss: 7.112\n",
            "[14,    52] loss: 7.261\n",
            "[14,    53] loss: 7.385\n",
            "[14,    54] loss: 7.512\n",
            "[14,    55] loss: 7.695\n",
            "[14,    56] loss: 7.820\n",
            "[14,    57] loss: 7.990\n",
            "[14,    58] loss: 8.192\n",
            "[14,    59] loss: 8.289\n",
            "[14,    60] loss: 8.503\n",
            "[14,    61] loss: 8.664\n",
            "[14,    62] loss: 8.819\n",
            "[14,    63] loss: 8.987\n",
            "[14,    64] loss: 9.109\n",
            "[14,    65] loss: 9.278\n",
            "[14,    66] loss: 9.432\n",
            "[14,    67] loss: 9.529\n",
            "[14,    68] loss: 9.645\n",
            "[14,    69] loss: 9.806\n",
            "[14,    70] loss: 9.895\n",
            "[14,    71] loss: 10.057\n",
            "[14,    72] loss: 10.236\n",
            "[14,    73] loss: 10.314\n",
            "[14,    74] loss: 10.451\n",
            "[14,    75] loss: 10.507\n",
            "[14,    76] loss: 10.625\n",
            "[14,    77] loss: 10.783\n",
            "[14,    78] loss: 10.925\n",
            "[14,    79] loss: 11.132\n",
            "[14,    80] loss: 11.252\n",
            "[14,    81] loss: 11.503\n",
            "[14,    82] loss: 11.604\n",
            "[14,    83] loss: 11.760\n",
            "[14,    84] loss: 11.881\n",
            "[14,    85] loss: 12.108\n",
            "[14,    86] loss: 12.360\n",
            "[14,    87] loss: 12.547\n",
            "[14,    88] loss: 12.695\n",
            "[14,    89] loss: 12.998\n",
            "[14,    90] loss: 13.255\n",
            "[14,    91] loss: 13.529\n",
            "[14,    92] loss: 13.800\n",
            "[14,    93] loss: 13.874\n",
            "[14,    94] loss: 14.199\n",
            "[14,    95] loss: 14.367\n",
            "[14,    96] loss: 14.491\n",
            "[14,    97] loss: 14.634\n",
            "[14,    98] loss: 14.837\n",
            "[14,    99] loss: 15.077\n",
            "[14,   100] loss: 15.374\n",
            "[14,   101] loss: 15.553\n",
            "[14,   102] loss: 15.664\n",
            "[14,   103] loss: 15.807\n",
            "[14,   104] loss: 16.042\n",
            "[14,   105] loss: 16.237\n",
            "[14,   106] loss: 16.497\n",
            "[14,   107] loss: 16.720\n",
            "[14,   108] loss: 16.852\n",
            "[14,   109] loss: 17.026\n",
            "[14,   110] loss: 17.180\n",
            "[14,   111] loss: 17.372\n",
            "[14,   112] loss: 17.514\n",
            "[14,   113] loss: 17.724\n",
            "[14,   114] loss: 18.068\n",
            "[14,   115] loss: 18.183\n",
            "[14,   116] loss: 18.388\n",
            "[14,   117] loss: 18.641\n",
            "[14,   118] loss: 18.835\n",
            "[14,   119] loss: 18.998\n",
            "[14,   120] loss: 19.257\n",
            "[14,   121] loss: 19.578\n",
            "[14,   122] loss: 19.711\n",
            "[14,   123] loss: 19.805\n",
            "[14,   124] loss: 20.033\n",
            "[14,   125] loss: 20.227\n",
            "[14,   126] loss: 20.500\n",
            "[14,   127] loss: 20.992\n",
            "[14,   128] loss: 21.243\n",
            "[14,   129] loss: 21.352\n",
            "[14,   130] loss: 21.491\n",
            "[14,   131] loss: 21.696\n",
            "[14,   132] loss: 21.908\n",
            "[14,   133] loss: 22.114\n",
            "[14,   134] loss: 22.358\n",
            "[14,   135] loss: 22.505\n",
            "[14,   136] loss: 22.647\n",
            "[14,   137] loss: 22.811\n",
            "[14,   138] loss: 22.907\n",
            "[14,   139] loss: 23.080\n",
            "[14,   140] loss: 23.523\n",
            "[14,   141] loss: 23.701\n",
            "[14,   142] loss: 23.892\n",
            "[14,   143] loss: 24.045\n",
            "[14,   144] loss: 24.248\n",
            "[14,   145] loss: 24.421\n",
            "[14,   146] loss: 24.773\n",
            "[14,   147] loss: 24.987\n",
            "[14,   148] loss: 25.150\n",
            "[14,   149] loss: 25.329\n",
            "[14,   150] loss: 25.559\n",
            "[14,   151] loss: 25.710\n",
            "[14,   152] loss: 25.936\n",
            "[14,   153] loss: 26.033\n",
            "[14,   154] loss: 26.322\n",
            "[14,   155] loss: 26.621\n",
            "[14,   156] loss: 26.812\n",
            "[14,   157] loss: 26.883\n",
            "[15,     2] loss: 0.224\n",
            "[15,     3] loss: 0.339\n",
            "[15,     4] loss: 0.437\n",
            "[15,     5] loss: 0.553\n",
            "[15,     6] loss: 0.701\n",
            "[15,     7] loss: 0.916\n",
            "[15,     8] loss: 1.075\n",
            "[15,     9] loss: 1.224\n",
            "[15,    10] loss: 1.305\n",
            "[15,    11] loss: 1.620\n",
            "[15,    12] loss: 1.753\n",
            "[15,    13] loss: 1.829\n",
            "[15,    14] loss: 1.899\n",
            "[15,    15] loss: 2.137\n",
            "[15,    16] loss: 2.271\n",
            "[15,    17] loss: 2.389\n",
            "[15,    18] loss: 2.450\n",
            "[15,    19] loss: 2.594\n",
            "[15,    20] loss: 2.739\n",
            "[15,    21] loss: 2.842\n",
            "[15,    22] loss: 2.913\n",
            "[15,    23] loss: 2.983\n",
            "[15,    24] loss: 3.094\n",
            "[15,    25] loss: 3.148\n",
            "[15,    26] loss: 3.244\n",
            "[15,    27] loss: 3.381\n",
            "[15,    28] loss: 3.446\n",
            "[15,    29] loss: 3.601\n",
            "[15,    30] loss: 3.758\n",
            "[15,    31] loss: 3.867\n",
            "[15,    32] loss: 4.033\n",
            "[15,    33] loss: 4.079\n",
            "[15,    34] loss: 4.171\n",
            "[15,    35] loss: 4.377\n",
            "[15,    36] loss: 4.457\n",
            "[15,    37] loss: 4.626\n",
            "[15,    38] loss: 4.750\n",
            "[15,    39] loss: 4.951\n",
            "[15,    40] loss: 5.048\n",
            "[15,    41] loss: 5.326\n",
            "[15,    42] loss: 5.559\n",
            "[15,    43] loss: 5.606\n",
            "[15,    44] loss: 5.716\n",
            "[15,    45] loss: 5.797\n",
            "[15,    46] loss: 5.975\n",
            "[15,    47] loss: 6.017\n",
            "[15,    48] loss: 6.280\n",
            "[15,    49] loss: 6.416\n",
            "[15,    50] loss: 6.576\n",
            "[15,    51] loss: 6.673\n",
            "[15,    52] loss: 6.810\n",
            "[15,    53] loss: 6.897\n",
            "[15,    54] loss: 7.043\n",
            "[15,    55] loss: 7.128\n",
            "[15,    56] loss: 7.246\n",
            "[15,    57] loss: 7.369\n",
            "[15,    58] loss: 7.653\n",
            "[15,    59] loss: 7.784\n",
            "[15,    60] loss: 7.907\n",
            "[15,    61] loss: 7.978\n",
            "[15,    62] loss: 8.137\n",
            "[15,    63] loss: 8.283\n",
            "[15,    64] loss: 8.378\n",
            "[15,    65] loss: 8.480\n",
            "[15,    66] loss: 8.581\n",
            "[15,    67] loss: 8.674\n",
            "[15,    68] loss: 8.833\n",
            "[15,    69] loss: 8.919\n",
            "[15,    70] loss: 9.096\n",
            "[15,    71] loss: 9.202\n",
            "[15,    72] loss: 9.413\n",
            "[15,    73] loss: 9.455\n",
            "[15,    74] loss: 9.622\n",
            "[15,    75] loss: 9.718\n",
            "[15,    76] loss: 9.863\n",
            "[15,    77] loss: 10.016\n",
            "[15,    78] loss: 10.089\n",
            "[15,    79] loss: 10.217\n",
            "[15,    80] loss: 10.368\n",
            "[15,    81] loss: 10.496\n",
            "[15,    82] loss: 10.599\n",
            "[15,    83] loss: 10.748\n",
            "[15,    84] loss: 10.858\n",
            "[15,    85] loss: 10.962\n",
            "[15,    86] loss: 11.111\n",
            "[15,    87] loss: 11.178\n",
            "[15,    88] loss: 11.262\n",
            "[15,    89] loss: 11.395\n",
            "[15,    90] loss: 11.518\n",
            "[15,    91] loss: 11.592\n",
            "[15,    92] loss: 11.758\n",
            "[15,    93] loss: 11.837\n",
            "[15,    94] loss: 11.994\n",
            "[15,    95] loss: 12.214\n",
            "[15,    96] loss: 12.328\n",
            "[15,    97] loss: 12.393\n",
            "[15,    98] loss: 12.536\n",
            "[15,    99] loss: 12.614\n",
            "[15,   100] loss: 12.744\n",
            "[15,   101] loss: 12.863\n",
            "[15,   102] loss: 12.925\n",
            "[15,   103] loss: 13.048\n",
            "[15,   104] loss: 13.112\n",
            "[15,   105] loss: 13.272\n",
            "[15,   106] loss: 13.375\n",
            "[15,   107] loss: 13.478\n",
            "[15,   108] loss: 13.608\n",
            "[15,   109] loss: 13.719\n",
            "[15,   110] loss: 13.817\n",
            "[15,   111] loss: 13.862\n",
            "[15,   112] loss: 13.967\n",
            "[15,   113] loss: 14.214\n",
            "[15,   114] loss: 14.358\n",
            "[15,   115] loss: 14.562\n",
            "[15,   116] loss: 15.100\n",
            "[15,   117] loss: 15.150\n",
            "[15,   118] loss: 15.296\n",
            "[15,   119] loss: 15.373\n",
            "[15,   120] loss: 15.594\n",
            "[15,   121] loss: 15.781\n",
            "[15,   122] loss: 16.044\n",
            "[15,   123] loss: 16.286\n",
            "[15,   124] loss: 16.405\n",
            "[15,   125] loss: 16.491\n",
            "[15,   126] loss: 16.705\n",
            "[15,   127] loss: 17.096\n",
            "[15,   128] loss: 17.293\n",
            "[15,   129] loss: 17.504\n",
            "[15,   130] loss: 17.636\n",
            "[15,   131] loss: 17.874\n",
            "[15,   132] loss: 18.215\n",
            "[15,   133] loss: 18.304\n",
            "[15,   134] loss: 18.466\n",
            "[15,   135] loss: 18.606\n",
            "[15,   136] loss: 18.717\n",
            "[15,   137] loss: 19.047\n",
            "[15,   138] loss: 19.255\n",
            "[15,   139] loss: 19.585\n",
            "[15,   140] loss: 19.690\n",
            "[15,   141] loss: 19.837\n",
            "[15,   142] loss: 20.099\n",
            "[15,   143] loss: 20.446\n",
            "[15,   144] loss: 20.572\n",
            "[15,   145] loss: 20.721\n",
            "[15,   146] loss: 20.801\n",
            "[15,   147] loss: 20.947\n",
            "[15,   148] loss: 21.113\n",
            "[15,   149] loss: 21.276\n",
            "[15,   150] loss: 21.414\n",
            "[15,   151] loss: 21.688\n",
            "[15,   152] loss: 22.103\n",
            "[15,   153] loss: 22.324\n",
            "[15,   154] loss: 22.552\n",
            "[15,   155] loss: 22.712\n",
            "[15,   156] loss: 22.963\n",
            "[15,   157] loss: 23.382\n",
            "[16,     2] loss: 0.329\n",
            "[16,     3] loss: 0.597\n",
            "[16,     4] loss: 0.920\n",
            "[16,     5] loss: 1.228\n",
            "[16,     6] loss: 1.447\n",
            "[16,     7] loss: 1.561\n",
            "[16,     8] loss: 1.751\n",
            "[16,     9] loss: 1.930\n",
            "[16,    10] loss: 2.302\n",
            "[16,    11] loss: 2.457\n",
            "[16,    12] loss: 2.656\n",
            "[16,    13] loss: 2.872\n",
            "[16,    14] loss: 3.028\n",
            "[16,    15] loss: 3.186\n",
            "[16,    16] loss: 3.382\n",
            "[16,    17] loss: 3.546\n",
            "[16,    18] loss: 3.622\n",
            "[16,    19] loss: 3.742\n",
            "[16,    20] loss: 3.891\n",
            "[16,    21] loss: 4.128\n",
            "[16,    22] loss: 4.381\n",
            "[16,    23] loss: 4.512\n",
            "[16,    24] loss: 4.675\n",
            "[16,    25] loss: 4.806\n",
            "[16,    26] loss: 4.919\n",
            "[16,    27] loss: 5.135\n",
            "[16,    28] loss: 5.448\n",
            "[16,    29] loss: 5.554\n",
            "[16,    30] loss: 5.688\n",
            "[16,    31] loss: 5.891\n",
            "[16,    32] loss: 6.092\n",
            "[16,    33] loss: 6.207\n",
            "[16,    34] loss: 6.297\n",
            "[16,    35] loss: 6.400\n",
            "[16,    36] loss: 6.525\n",
            "[16,    37] loss: 6.905\n",
            "[16,    38] loss: 7.003\n",
            "[16,    39] loss: 7.352\n",
            "[16,    40] loss: 7.624\n",
            "[16,    41] loss: 7.790\n",
            "[16,    42] loss: 7.938\n",
            "[16,    43] loss: 8.095\n",
            "[16,    44] loss: 8.167\n",
            "[16,    45] loss: 8.400\n",
            "[16,    46] loss: 8.618\n",
            "[16,    47] loss: 8.820\n",
            "[16,    48] loss: 8.981\n",
            "[16,    49] loss: 9.087\n",
            "[16,    50] loss: 9.226\n",
            "[16,    51] loss: 9.398\n",
            "[16,    52] loss: 9.700\n",
            "[16,    53] loss: 9.954\n",
            "[16,    54] loss: 10.191\n",
            "[16,    55] loss: 10.377\n",
            "[16,    56] loss: 10.744\n",
            "[16,    57] loss: 10.886\n",
            "[16,    58] loss: 11.000\n",
            "[16,    59] loss: 11.126\n",
            "[16,    60] loss: 11.278\n",
            "[16,    61] loss: 11.389\n",
            "[16,    62] loss: 11.508\n",
            "[16,    63] loss: 11.654\n",
            "[16,    64] loss: 11.741\n",
            "[16,    65] loss: 11.885\n",
            "[16,    66] loss: 12.007\n",
            "[16,    67] loss: 12.416\n",
            "[16,    68] loss: 12.549\n",
            "[16,    69] loss: 12.727\n",
            "[16,    70] loss: 12.842\n",
            "[16,    71] loss: 12.906\n",
            "[16,    72] loss: 13.134\n",
            "[16,    73] loss: 13.256\n",
            "[16,    74] loss: 13.624\n",
            "[16,    75] loss: 13.812\n",
            "[16,    76] loss: 14.043\n",
            "[16,    77] loss: 14.239\n",
            "[16,    78] loss: 14.500\n",
            "[16,    79] loss: 14.791\n",
            "[16,    80] loss: 15.003\n",
            "[16,    81] loss: 15.356\n",
            "[16,    82] loss: 15.435\n",
            "[16,    83] loss: 15.508\n",
            "[16,    84] loss: 15.629\n",
            "[16,    85] loss: 15.899\n",
            "[16,    86] loss: 16.020\n",
            "[16,    87] loss: 16.196\n",
            "[16,    88] loss: 16.307\n",
            "[16,    89] loss: 16.395\n",
            "[16,    90] loss: 16.523\n",
            "[16,    91] loss: 16.720\n",
            "[16,    92] loss: 16.838\n",
            "[16,    93] loss: 17.076\n",
            "[16,    94] loss: 17.189\n",
            "[16,    95] loss: 17.264\n",
            "[16,    96] loss: 17.352\n",
            "[16,    97] loss: 17.454\n",
            "[16,    98] loss: 17.658\n",
            "[16,    99] loss: 17.856\n",
            "[16,   100] loss: 18.034\n",
            "[16,   101] loss: 18.121\n",
            "[16,   102] loss: 18.196\n",
            "[16,   103] loss: 18.350\n",
            "[16,   104] loss: 18.525\n",
            "[16,   105] loss: 18.707\n",
            "[16,   106] loss: 18.937\n",
            "[16,   107] loss: 19.096\n",
            "[16,   108] loss: 19.179\n",
            "[16,   109] loss: 19.282\n",
            "[16,   110] loss: 19.555\n",
            "[16,   111] loss: 19.717\n",
            "[16,   112] loss: 19.789\n",
            "[16,   113] loss: 19.989\n",
            "[16,   114] loss: 20.141\n",
            "[16,   115] loss: 20.302\n",
            "[16,   116] loss: 20.586\n",
            "[16,   117] loss: 20.636\n",
            "[16,   118] loss: 20.704\n",
            "[16,   119] loss: 20.885\n",
            "[16,   120] loss: 20.984\n",
            "[16,   121] loss: 21.132\n",
            "[16,   122] loss: 21.265\n",
            "[16,   123] loss: 21.431\n",
            "[16,   124] loss: 21.537\n",
            "[16,   125] loss: 21.762\n",
            "[16,   126] loss: 21.898\n",
            "[16,   127] loss: 22.021\n",
            "[16,   128] loss: 22.120\n",
            "[16,   129] loss: 22.231\n",
            "[16,   130] loss: 22.306\n",
            "[16,   131] loss: 22.529\n",
            "[16,   132] loss: 22.643\n",
            "[16,   133] loss: 22.776\n",
            "[16,   134] loss: 22.887\n",
            "[16,   135] loss: 23.057\n",
            "[16,   136] loss: 23.227\n",
            "[16,   137] loss: 23.378\n",
            "[16,   138] loss: 23.543\n",
            "[16,   139] loss: 23.623\n",
            "[16,   140] loss: 23.698\n",
            "[16,   141] loss: 23.893\n",
            "[16,   142] loss: 24.086\n",
            "[16,   143] loss: 24.139\n",
            "[16,   144] loss: 24.282\n",
            "[16,   145] loss: 24.367\n",
            "[16,   146] loss: 24.439\n",
            "[16,   147] loss: 24.656\n",
            "[16,   148] loss: 24.763\n",
            "[16,   149] loss: 24.832\n",
            "[16,   150] loss: 25.086\n",
            "[16,   151] loss: 25.271\n",
            "[16,   152] loss: 25.396\n",
            "[16,   153] loss: 25.497\n",
            "[16,   154] loss: 25.645\n",
            "[16,   155] loss: 25.902\n",
            "[16,   156] loss: 26.068\n",
            "[16,   157] loss: 26.105\n",
            "[17,     2] loss: 0.321\n",
            "[17,     3] loss: 0.382\n",
            "[17,     4] loss: 0.462\n",
            "[17,     5] loss: 0.585\n",
            "[17,     6] loss: 0.650\n",
            "[17,     7] loss: 0.760\n",
            "[17,     8] loss: 0.890\n",
            "[17,     9] loss: 0.988\n",
            "[17,    10] loss: 1.100\n",
            "[17,    11] loss: 1.161\n",
            "[17,    12] loss: 1.256\n",
            "[17,    13] loss: 1.339\n",
            "[17,    14] loss: 1.410\n",
            "[17,    15] loss: 1.519\n",
            "[17,    16] loss: 1.613\n",
            "[17,    17] loss: 1.650\n",
            "[17,    18] loss: 1.708\n",
            "[17,    19] loss: 1.826\n",
            "[17,    20] loss: 1.878\n",
            "[17,    21] loss: 1.991\n",
            "[17,    22] loss: 2.126\n",
            "[17,    23] loss: 2.177\n",
            "[17,    24] loss: 2.370\n",
            "[17,    25] loss: 2.455\n",
            "[17,    26] loss: 2.549\n",
            "[17,    27] loss: 2.663\n",
            "[17,    28] loss: 2.798\n",
            "[17,    29] loss: 2.866\n",
            "[17,    30] loss: 2.963\n",
            "[17,    31] loss: 3.059\n",
            "[17,    32] loss: 3.182\n",
            "[17,    33] loss: 3.357\n",
            "[17,    34] loss: 3.534\n",
            "[17,    35] loss: 3.684\n",
            "[17,    36] loss: 3.808\n",
            "[17,    37] loss: 3.879\n",
            "[17,    38] loss: 3.931\n",
            "[17,    39] loss: 4.086\n",
            "[17,    40] loss: 4.137\n",
            "[17,    41] loss: 4.200\n",
            "[17,    42] loss: 4.316\n",
            "[17,    43] loss: 4.426\n",
            "[17,    44] loss: 4.507\n",
            "[17,    45] loss: 4.580\n",
            "[17,    46] loss: 4.616\n",
            "[17,    47] loss: 4.713\n",
            "[17,    48] loss: 4.796\n",
            "[17,    49] loss: 4.850\n",
            "[17,    50] loss: 4.939\n",
            "[17,    51] loss: 5.016\n",
            "[17,    52] loss: 5.093\n",
            "[17,    53] loss: 5.260\n",
            "[17,    54] loss: 5.340\n",
            "[17,    55] loss: 5.444\n",
            "[17,    56] loss: 5.517\n",
            "[17,    57] loss: 5.612\n",
            "[17,    58] loss: 5.682\n",
            "[17,    59] loss: 5.747\n",
            "[17,    60] loss: 5.802\n",
            "[17,    61] loss: 5.928\n",
            "[17,    62] loss: 6.009\n",
            "[17,    63] loss: 6.071\n",
            "[17,    64] loss: 6.168\n",
            "[17,    65] loss: 6.222\n",
            "[17,    66] loss: 6.294\n",
            "[17,    67] loss: 6.353\n",
            "[17,    68] loss: 6.440\n",
            "[17,    69] loss: 6.493\n",
            "[17,    70] loss: 6.562\n",
            "[17,    71] loss: 6.648\n",
            "[17,    72] loss: 6.707\n",
            "[17,    73] loss: 6.781\n",
            "[17,    74] loss: 6.836\n",
            "[17,    75] loss: 6.970\n",
            "[17,    76] loss: 7.076\n",
            "[17,    77] loss: 7.113\n",
            "[17,    78] loss: 7.197\n",
            "[17,    79] loss: 7.302\n",
            "[17,    80] loss: 7.450\n",
            "[17,    81] loss: 7.521\n",
            "[17,    82] loss: 7.553\n",
            "[17,    83] loss: 7.614\n",
            "[17,    84] loss: 7.706\n",
            "[17,    85] loss: 7.809\n",
            "[17,    86] loss: 7.915\n",
            "[17,    87] loss: 7.979\n",
            "[17,    88] loss: 8.137\n",
            "[17,    89] loss: 8.177\n",
            "[17,    90] loss: 8.245\n",
            "[17,    91] loss: 8.310\n",
            "[17,    92] loss: 8.395\n",
            "[17,    93] loss: 8.553\n",
            "[17,    94] loss: 8.642\n",
            "[17,    95] loss: 8.727\n",
            "[17,    96] loss: 8.845\n",
            "[17,    97] loss: 8.912\n",
            "[17,    98] loss: 8.962\n",
            "[17,    99] loss: 9.104\n",
            "[17,   100] loss: 9.197\n",
            "[17,   101] loss: 9.335\n",
            "[17,   102] loss: 9.437\n",
            "[17,   103] loss: 9.502\n",
            "[17,   104] loss: 9.600\n",
            "[17,   105] loss: 9.729\n",
            "[17,   106] loss: 9.832\n",
            "[17,   107] loss: 9.952\n",
            "[17,   108] loss: 10.034\n",
            "[17,   109] loss: 10.099\n",
            "[17,   110] loss: 10.184\n",
            "[17,   111] loss: 10.296\n",
            "[17,   112] loss: 10.407\n",
            "[17,   113] loss: 10.480\n",
            "[17,   114] loss: 10.534\n",
            "[17,   115] loss: 10.590\n",
            "[17,   116] loss: 10.667\n",
            "[17,   117] loss: 10.794\n",
            "[17,   118] loss: 10.842\n",
            "[17,   119] loss: 10.883\n",
            "[17,   120] loss: 10.927\n",
            "[17,   121] loss: 10.982\n",
            "[17,   122] loss: 11.166\n",
            "[17,   123] loss: 11.229\n",
            "[17,   124] loss: 11.260\n",
            "[17,   125] loss: 11.399\n",
            "[17,   126] loss: 11.477\n",
            "[17,   127] loss: 11.530\n",
            "[17,   128] loss: 11.593\n",
            "[17,   129] loss: 11.730\n",
            "[17,   130] loss: 11.856\n",
            "[17,   131] loss: 11.878\n",
            "[17,   132] loss: 11.966\n",
            "[17,   133] loss: 12.016\n",
            "[17,   134] loss: 12.093\n",
            "[17,   135] loss: 12.230\n",
            "[17,   136] loss: 12.424\n",
            "[17,   137] loss: 12.491\n",
            "[17,   138] loss: 12.534\n",
            "[17,   139] loss: 12.672\n",
            "[17,   140] loss: 12.804\n",
            "[17,   141] loss: 12.847\n",
            "[17,   142] loss: 12.922\n",
            "[17,   143] loss: 13.016\n",
            "[17,   144] loss: 13.114\n",
            "[17,   145] loss: 13.255\n",
            "[17,   146] loss: 13.294\n",
            "[17,   147] loss: 13.366\n",
            "[17,   148] loss: 13.462\n",
            "[17,   149] loss: 13.543\n",
            "[17,   150] loss: 13.631\n",
            "[17,   151] loss: 13.666\n",
            "[17,   152] loss: 13.751\n",
            "[17,   153] loss: 13.823\n",
            "[17,   154] loss: 13.936\n",
            "[17,   155] loss: 14.174\n",
            "[17,   156] loss: 14.261\n",
            "[17,   157] loss: 14.299\n",
            "[18,     2] loss: 0.237\n",
            "[18,     3] loss: 0.275\n",
            "[18,     4] loss: 0.312\n",
            "[18,     5] loss: 0.411\n",
            "[18,     6] loss: 0.530\n",
            "[18,     7] loss: 0.608\n",
            "[18,     8] loss: 0.650\n",
            "[18,     9] loss: 0.701\n",
            "[18,    10] loss: 0.771\n",
            "[18,    11] loss: 0.824\n",
            "[18,    12] loss: 0.877\n",
            "[18,    13] loss: 0.912\n",
            "[18,    14] loss: 0.951\n",
            "[18,    15] loss: 1.043\n",
            "[18,    16] loss: 1.121\n",
            "[18,    17] loss: 1.152\n",
            "[18,    18] loss: 1.201\n",
            "[18,    19] loss: 1.238\n",
            "[18,    20] loss: 1.288\n",
            "[18,    21] loss: 1.348\n",
            "[18,    22] loss: 1.389\n",
            "[18,    23] loss: 1.420\n",
            "[18,    24] loss: 1.476\n",
            "[18,    25] loss: 1.543\n",
            "[18,    26] loss: 1.596\n",
            "[18,    27] loss: 1.628\n",
            "[18,    28] loss: 1.660\n",
            "[18,    29] loss: 1.792\n",
            "[18,    30] loss: 1.838\n",
            "[18,    31] loss: 1.913\n",
            "[18,    32] loss: 2.011\n",
            "[18,    33] loss: 2.069\n",
            "[18,    34] loss: 2.123\n",
            "[18,    35] loss: 2.225\n",
            "[18,    36] loss: 2.246\n",
            "[18,    37] loss: 2.304\n",
            "[18,    38] loss: 2.342\n",
            "[18,    39] loss: 2.380\n",
            "[18,    40] loss: 2.426\n",
            "[18,    41] loss: 2.475\n",
            "[18,    42] loss: 2.530\n",
            "[18,    43] loss: 2.575\n",
            "[18,    44] loss: 2.636\n",
            "[18,    45] loss: 2.670\n",
            "[18,    46] loss: 2.691\n",
            "[18,    47] loss: 2.747\n",
            "[18,    48] loss: 2.837\n",
            "[18,    49] loss: 2.946\n",
            "[18,    50] loss: 3.023\n",
            "[18,    51] loss: 3.046\n",
            "[18,    52] loss: 3.116\n",
            "[18,    53] loss: 3.141\n",
            "[18,    54] loss: 3.241\n",
            "[18,    55] loss: 3.323\n",
            "[18,    56] loss: 3.454\n",
            "[18,    57] loss: 3.626\n",
            "[18,    58] loss: 3.678\n",
            "[18,    59] loss: 3.836\n",
            "[18,    60] loss: 3.880\n",
            "[18,    61] loss: 3.942\n",
            "[18,    62] loss: 3.971\n",
            "[18,    63] loss: 4.039\n",
            "[18,    64] loss: 4.083\n",
            "[18,    65] loss: 4.122\n",
            "[18,    66] loss: 4.201\n",
            "[18,    67] loss: 4.249\n",
            "[18,    68] loss: 4.325\n",
            "[18,    69] loss: 4.339\n",
            "[18,    70] loss: 4.439\n",
            "[18,    71] loss: 4.473\n",
            "[18,    72] loss: 4.530\n",
            "[18,    73] loss: 4.573\n",
            "[18,    74] loss: 4.689\n",
            "[18,    75] loss: 4.765\n",
            "[18,    76] loss: 4.846\n",
            "[18,    77] loss: 4.943\n",
            "[18,    78] loss: 5.181\n",
            "[18,    79] loss: 5.230\n",
            "[18,    80] loss: 5.300\n",
            "[18,    81] loss: 5.385\n",
            "[18,    82] loss: 5.446\n",
            "[18,    83] loss: 5.584\n",
            "[18,    84] loss: 5.726\n",
            "[18,    85] loss: 5.797\n",
            "[18,    86] loss: 5.863\n",
            "[18,    87] loss: 5.898\n",
            "[18,    88] loss: 5.976\n",
            "[18,    89] loss: 6.089\n",
            "[18,    90] loss: 6.230\n",
            "[18,    91] loss: 6.332\n",
            "[18,    92] loss: 6.405\n",
            "[18,    93] loss: 6.428\n",
            "[18,    94] loss: 6.480\n",
            "[18,    95] loss: 6.506\n",
            "[18,    96] loss: 6.582\n",
            "[18,    97] loss: 6.627\n",
            "[18,    98] loss: 6.679\n",
            "[18,    99] loss: 6.838\n",
            "[18,   100] loss: 6.914\n",
            "[18,   101] loss: 6.976\n",
            "[18,   102] loss: 7.050\n",
            "[18,   103] loss: 7.126\n",
            "[18,   104] loss: 7.151\n",
            "[18,   105] loss: 7.200\n",
            "[18,   106] loss: 7.292\n",
            "[18,   107] loss: 7.408\n",
            "[18,   108] loss: 7.472\n",
            "[18,   109] loss: 7.568\n",
            "[18,   110] loss: 7.606\n",
            "[18,   111] loss: 7.644\n",
            "[18,   112] loss: 7.849\n",
            "[18,   113] loss: 7.904\n",
            "[18,   114] loss: 7.939\n",
            "[18,   115] loss: 7.998\n",
            "[18,   116] loss: 8.116\n",
            "[18,   117] loss: 8.167\n",
            "[18,   118] loss: 8.259\n",
            "[18,   119] loss: 8.312\n",
            "[18,   120] loss: 8.409\n",
            "[18,   121] loss: 8.630\n",
            "[18,   122] loss: 8.734\n",
            "[18,   123] loss: 8.923\n",
            "[18,   124] loss: 8.998\n",
            "[18,   125] loss: 9.080\n",
            "[18,   126] loss: 9.121\n",
            "[18,   127] loss: 9.220\n",
            "[18,   128] loss: 9.265\n",
            "[18,   129] loss: 9.456\n",
            "[18,   130] loss: 9.544\n",
            "[18,   131] loss: 9.598\n",
            "[18,   132] loss: 9.672\n",
            "[18,   133] loss: 9.731\n",
            "[18,   134] loss: 9.812\n",
            "[18,   135] loss: 9.995\n",
            "[18,   136] loss: 10.104\n",
            "[18,   137] loss: 10.166\n",
            "[18,   138] loss: 10.254\n",
            "[18,   139] loss: 10.469\n",
            "[18,   140] loss: 10.506\n",
            "[18,   141] loss: 10.579\n",
            "[18,   142] loss: 10.665\n",
            "[18,   143] loss: 10.774\n",
            "[18,   144] loss: 10.910\n",
            "[18,   145] loss: 10.981\n",
            "[18,   146] loss: 11.178\n",
            "[18,   147] loss: 11.227\n",
            "[18,   148] loss: 11.250\n",
            "[18,   149] loss: 11.313\n",
            "[18,   150] loss: 11.373\n",
            "[18,   151] loss: 11.388\n",
            "[18,   152] loss: 11.471\n",
            "[18,   153] loss: 11.531\n",
            "[18,   154] loss: 11.640\n",
            "[18,   155] loss: 11.704\n",
            "[18,   156] loss: 11.809\n",
            "[18,   157] loss: 12.051\n",
            "[19,     2] loss: 0.113\n",
            "[19,     3] loss: 0.499\n",
            "[19,     4] loss: 0.576\n",
            "[19,     5] loss: 0.631\n",
            "[19,     6] loss: 0.715\n",
            "[19,     7] loss: 0.844\n",
            "[19,     8] loss: 1.065\n",
            "[19,     9] loss: 1.133\n",
            "[19,    10] loss: 1.259\n",
            "[19,    11] loss: 1.343\n",
            "[19,    12] loss: 1.514\n",
            "[19,    13] loss: 1.635\n",
            "[19,    14] loss: 1.755\n",
            "[19,    15] loss: 1.896\n",
            "[19,    16] loss: 1.947\n",
            "[19,    17] loss: 2.044\n",
            "[19,    18] loss: 2.072\n",
            "[19,    19] loss: 2.144\n",
            "[19,    20] loss: 2.221\n",
            "[19,    21] loss: 2.317\n",
            "[19,    22] loss: 2.377\n",
            "[19,    23] loss: 2.476\n",
            "[19,    24] loss: 2.500\n",
            "[19,    25] loss: 2.537\n",
            "[19,    26] loss: 2.662\n",
            "[19,    27] loss: 2.735\n",
            "[19,    28] loss: 2.812\n",
            "[19,    29] loss: 2.880\n",
            "[19,    30] loss: 2.962\n",
            "[19,    31] loss: 3.025\n",
            "[19,    32] loss: 3.092\n",
            "[19,    33] loss: 3.144\n",
            "[19,    34] loss: 3.193\n",
            "[19,    35] loss: 3.259\n",
            "[19,    36] loss: 3.358\n",
            "[19,    37] loss: 3.399\n",
            "[19,    38] loss: 3.524\n",
            "[19,    39] loss: 3.589\n",
            "[19,    40] loss: 3.637\n",
            "[19,    41] loss: 3.705\n",
            "[19,    42] loss: 3.764\n",
            "[19,    43] loss: 3.786\n",
            "[19,    44] loss: 3.831\n",
            "[19,    45] loss: 3.867\n",
            "[19,    46] loss: 3.982\n",
            "[19,    47] loss: 4.082\n",
            "[19,    48] loss: 4.138\n",
            "[19,    49] loss: 4.292\n",
            "[19,    50] loss: 4.354\n",
            "[19,    51] loss: 4.429\n",
            "[19,    52] loss: 4.495\n",
            "[19,    53] loss: 4.575\n",
            "[19,    54] loss: 4.617\n",
            "[19,    55] loss: 4.871\n",
            "[19,    56] loss: 4.912\n",
            "[19,    57] loss: 4.994\n",
            "[19,    58] loss: 5.119\n",
            "[19,    59] loss: 5.154\n",
            "[19,    60] loss: 5.270\n",
            "[19,    61] loss: 5.361\n",
            "[19,    62] loss: 5.424\n",
            "[19,    63] loss: 5.464\n",
            "[19,    64] loss: 5.768\n",
            "[19,    65] loss: 5.830\n",
            "[19,    66] loss: 5.861\n",
            "[19,    67] loss: 5.954\n",
            "[19,    68] loss: 6.064\n",
            "[19,    69] loss: 6.270\n",
            "[19,    70] loss: 6.359\n",
            "[19,    71] loss: 6.411\n",
            "[19,    72] loss: 6.521\n",
            "[19,    73] loss: 6.574\n",
            "[19,    74] loss: 6.686\n",
            "[19,    75] loss: 6.717\n",
            "[19,    76] loss: 6.786\n",
            "[19,    77] loss: 6.899\n",
            "[19,    78] loss: 6.945\n",
            "[19,    79] loss: 6.985\n",
            "[19,    80] loss: 7.013\n",
            "[19,    81] loss: 7.085\n",
            "[19,    82] loss: 7.176\n",
            "[19,    83] loss: 7.208\n",
            "[19,    84] loss: 7.280\n",
            "[19,    85] loss: 7.360\n",
            "[19,    86] loss: 7.415\n",
            "[19,    87] loss: 7.559\n",
            "[19,    88] loss: 7.609\n",
            "[19,    89] loss: 7.681\n",
            "[19,    90] loss: 7.728\n",
            "[19,    91] loss: 7.804\n",
            "[19,    92] loss: 7.864\n",
            "[19,    93] loss: 7.958\n",
            "[19,    94] loss: 8.185\n",
            "[19,    95] loss: 8.228\n",
            "[19,    96] loss: 8.310\n",
            "[19,    97] loss: 8.548\n",
            "[19,    98] loss: 8.663\n",
            "[19,    99] loss: 8.712\n",
            "[19,   100] loss: 8.747\n",
            "[19,   101] loss: 8.896\n",
            "[19,   102] loss: 8.939\n",
            "[19,   103] loss: 9.030\n",
            "[19,   104] loss: 9.058\n",
            "[19,   105] loss: 9.175\n",
            "[19,   106] loss: 9.193\n",
            "[19,   107] loss: 9.264\n",
            "[19,   108] loss: 9.300\n",
            "[19,   109] loss: 9.337\n",
            "[19,   110] loss: 9.422\n",
            "[19,   111] loss: 9.456\n",
            "[19,   112] loss: 9.488\n",
            "[19,   113] loss: 9.560\n",
            "[19,   114] loss: 9.671\n",
            "[19,   115] loss: 9.716\n",
            "[19,   116] loss: 9.776\n",
            "[19,   117] loss: 9.827\n",
            "[19,   118] loss: 9.978\n",
            "[19,   119] loss: 10.040\n",
            "[19,   120] loss: 10.105\n",
            "[19,   121] loss: 10.180\n",
            "[19,   122] loss: 10.210\n",
            "[19,   123] loss: 10.367\n",
            "[19,   124] loss: 10.410\n",
            "[19,   125] loss: 10.463\n",
            "[19,   126] loss: 10.503\n",
            "[19,   127] loss: 10.597\n",
            "[19,   128] loss: 10.710\n",
            "[19,   129] loss: 10.759\n",
            "[19,   130] loss: 10.872\n",
            "[19,   131] loss: 10.922\n",
            "[19,   132] loss: 11.023\n",
            "[19,   133] loss: 11.523\n",
            "[19,   134] loss: 11.602\n",
            "[19,   135] loss: 11.678\n",
            "[19,   136] loss: 11.770\n",
            "[19,   137] loss: 12.015\n",
            "[19,   138] loss: 12.092\n",
            "[19,   139] loss: 12.231\n",
            "[19,   140] loss: 12.294\n",
            "[19,   141] loss: 12.338\n",
            "[19,   142] loss: 12.409\n",
            "[19,   143] loss: 12.490\n",
            "[19,   144] loss: 12.561\n",
            "[19,   145] loss: 12.664\n",
            "[19,   146] loss: 12.709\n",
            "[19,   147] loss: 12.791\n",
            "[19,   148] loss: 12.903\n",
            "[19,   149] loss: 12.951\n",
            "[19,   150] loss: 13.032\n",
            "[19,   151] loss: 13.201\n",
            "[19,   152] loss: 13.284\n",
            "[19,   153] loss: 13.387\n",
            "[19,   154] loss: 13.470\n",
            "[19,   155] loss: 13.574\n",
            "[19,   156] loss: 13.705\n",
            "[19,   157] loss: 13.813\n",
            "[20,     2] loss: 0.131\n",
            "[20,     3] loss: 0.173\n",
            "[20,     4] loss: 0.209\n",
            "[20,     5] loss: 0.266\n",
            "[20,     6] loss: 0.296\n",
            "[20,     7] loss: 0.372\n",
            "[20,     8] loss: 0.460\n",
            "[20,     9] loss: 0.497\n",
            "[20,    10] loss: 0.642\n",
            "[20,    11] loss: 0.680\n",
            "[20,    12] loss: 0.715\n",
            "[20,    13] loss: 0.744\n",
            "[20,    14] loss: 0.786\n",
            "[20,    15] loss: 0.958\n",
            "[20,    16] loss: 1.028\n",
            "[20,    17] loss: 1.075\n",
            "[20,    18] loss: 1.103\n",
            "[20,    19] loss: 1.131\n",
            "[20,    20] loss: 1.221\n",
            "[20,    21] loss: 1.332\n",
            "[20,    22] loss: 1.401\n",
            "[20,    23] loss: 1.455\n",
            "[20,    24] loss: 1.472\n",
            "[20,    25] loss: 1.523\n",
            "[20,    26] loss: 1.579\n",
            "[20,    27] loss: 1.620\n",
            "[20,    28] loss: 1.732\n",
            "[20,    29] loss: 1.765\n",
            "[20,    30] loss: 1.907\n",
            "[20,    31] loss: 1.949\n",
            "[20,    32] loss: 1.972\n",
            "[20,    33] loss: 2.011\n",
            "[20,    34] loss: 2.137\n",
            "[20,    35] loss: 2.169\n",
            "[20,    36] loss: 2.202\n",
            "[20,    37] loss: 2.250\n",
            "[20,    38] loss: 2.309\n",
            "[20,    39] loss: 2.361\n",
            "[20,    40] loss: 2.426\n",
            "[20,    41] loss: 2.524\n",
            "[20,    42] loss: 2.548\n",
            "[20,    43] loss: 2.592\n",
            "[20,    44] loss: 2.654\n",
            "[20,    45] loss: 2.694\n",
            "[20,    46] loss: 2.755\n",
            "[20,    47] loss: 2.845\n",
            "[20,    48] loss: 2.925\n",
            "[20,    49] loss: 2.951\n",
            "[20,    50] loss: 3.018\n",
            "[20,    51] loss: 3.159\n",
            "[20,    52] loss: 3.220\n",
            "[20,    53] loss: 3.252\n",
            "[20,    54] loss: 3.280\n",
            "[20,    55] loss: 3.335\n",
            "[20,    56] loss: 3.488\n",
            "[20,    57] loss: 3.511\n",
            "[20,    58] loss: 3.584\n",
            "[20,    59] loss: 3.626\n",
            "[20,    60] loss: 3.657\n",
            "[20,    61] loss: 3.730\n",
            "[20,    62] loss: 3.801\n",
            "[20,    63] loss: 3.855\n",
            "[20,    64] loss: 3.905\n",
            "[20,    65] loss: 3.947\n",
            "[20,    66] loss: 3.990\n",
            "[20,    67] loss: 4.088\n",
            "[20,    68] loss: 4.195\n",
            "[20,    69] loss: 4.232\n",
            "[20,    70] loss: 4.266\n",
            "[20,    71] loss: 4.323\n",
            "[20,    72] loss: 4.356\n",
            "[20,    73] loss: 4.403\n",
            "[20,    74] loss: 4.491\n",
            "[20,    75] loss: 4.531\n",
            "[20,    76] loss: 4.593\n",
            "[20,    77] loss: 4.637\n",
            "[20,    78] loss: 4.675\n",
            "[20,    79] loss: 4.790\n",
            "[20,    80] loss: 4.840\n",
            "[20,    81] loss: 4.874\n",
            "[20,    82] loss: 4.929\n",
            "[20,    83] loss: 4.955\n",
            "[20,    84] loss: 4.978\n",
            "[20,    85] loss: 5.007\n",
            "[20,    86] loss: 5.043\n",
            "[20,    87] loss: 5.157\n",
            "[20,    88] loss: 5.172\n",
            "[20,    89] loss: 5.228\n",
            "[20,    90] loss: 5.245\n",
            "[20,    91] loss: 5.295\n",
            "[20,    92] loss: 5.313\n",
            "[20,    93] loss: 5.451\n",
            "[20,    94] loss: 5.536\n",
            "[20,    95] loss: 5.573\n",
            "[20,    96] loss: 5.614\n",
            "[20,    97] loss: 5.638\n",
            "[20,    98] loss: 5.775\n",
            "[20,    99] loss: 5.866\n",
            "[20,   100] loss: 5.915\n",
            "[20,   101] loss: 5.975\n",
            "[20,   102] loss: 5.997\n",
            "[20,   103] loss: 6.055\n",
            "[20,   104] loss: 6.097\n",
            "[20,   105] loss: 6.137\n",
            "[20,   106] loss: 6.215\n",
            "[20,   107] loss: 6.275\n",
            "[20,   108] loss: 6.493\n",
            "[20,   109] loss: 6.513\n",
            "[20,   110] loss: 6.549\n",
            "[20,   111] loss: 6.598\n",
            "[20,   112] loss: 6.639\n",
            "[20,   113] loss: 6.909\n",
            "[20,   114] loss: 7.033\n",
            "[20,   115] loss: 7.090\n",
            "[20,   116] loss: 7.159\n",
            "[20,   117] loss: 7.185\n",
            "[20,   118] loss: 7.277\n",
            "[20,   119] loss: 7.346\n",
            "[20,   120] loss: 7.412\n",
            "[20,   121] loss: 7.459\n",
            "[20,   122] loss: 7.540\n",
            "[20,   123] loss: 7.606\n",
            "[20,   124] loss: 7.634\n",
            "[20,   125] loss: 7.712\n",
            "[20,   126] loss: 7.743\n",
            "[20,   127] loss: 7.811\n",
            "[20,   128] loss: 7.852\n",
            "[20,   129] loss: 7.878\n",
            "[20,   130] loss: 7.928\n",
            "[20,   131] loss: 8.003\n",
            "[20,   132] loss: 8.033\n",
            "[20,   133] loss: 8.092\n",
            "[20,   134] loss: 8.189\n",
            "[20,   135] loss: 8.237\n",
            "[20,   136] loss: 8.265\n",
            "[20,   137] loss: 8.308\n",
            "[20,   138] loss: 8.433\n",
            "[20,   139] loss: 8.495\n",
            "[20,   140] loss: 8.536\n",
            "[20,   141] loss: 8.553\n",
            "[20,   142] loss: 8.679\n",
            "[20,   143] loss: 8.754\n",
            "[20,   144] loss: 8.952\n",
            "[20,   145] loss: 9.019\n",
            "[20,   146] loss: 9.158\n",
            "[20,   147] loss: 9.297\n",
            "[20,   148] loss: 9.386\n",
            "[20,   149] loss: 9.498\n",
            "[20,   150] loss: 9.591\n",
            "[20,   151] loss: 9.653\n",
            "[20,   152] loss: 9.791\n",
            "[20,   153] loss: 10.023\n",
            "[20,   154] loss: 10.066\n",
            "[20,   155] loss: 10.211\n",
            "[20,   156] loss: 10.243\n",
            "[20,   157] loss: 10.353\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_8uhYfv5la",
        "outputId": "87e4984e-3731-4dad-a2dd-ca13906abf11"
      },
      "source": [
        "print(results['train_time']/60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.56276630957921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "DyW8iV0q-MLm",
        "outputId": "6757b0d2-c543-48f9-d309-0b166d61c053"
      },
      "source": [
        "plt.plot(range(EPOCHS),np.array(results['train_acc']))\n",
        "plt.plot(range(EPOCHS),np.array(results['valid_acc']))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NecIxj0Jv814"
      },
      "source": [
        "plt.plot(np.array(results['train_acc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dUc4kU5wCIp"
      },
      "source": [
        "plt.plot(np.array(results['valid_acc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkheqa_wCsD"
      },
      "source": [
        "plt.plot(np.array(results['epoch_loss']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-X24riNw7B_",
        "outputId": "0815bef5-fb3a-4407-f746-805574263123"
      },
      "source": [
        "accuracy(model,train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(93.8600, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKC-7P9gw-0g",
        "outputId": "5bcbdfc9-d5f3-403e-88d0-9e1fdbd7a56d"
      },
      "source": [
        "accuracy(model,valid_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(70.5922, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW5bMSPxaJsJ"
      },
      "source": [
        "# To generate the output file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BzNkLS1YaJsJ",
        "outputId": "fe527ac0-6219-4762-f260-a95557a707c6"
      },
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "from_dir = '/content/drive/MyDrive/HackerEarth Holiday Season Challenge/input_dataset/dataset/test'\n",
        "to_dir = '/content/drive/MyDrive/HackerEarth Holiday Season Challenge/test_dataset'\n",
        "copy_tree(from_dir,to_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-9c349531c165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfrom_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/HackerEarth Holiday Season Challenge/input_dataset/dataset/test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mto_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/HackerEarth Holiday Season Challenge/test_dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcopy_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/distutils/dir_util.py\u001b[0m in \u001b[0;36mcopy_tree\u001b[0;34m(src, dst, preserve_mode, preserve_times, preserve_symlinks, update, verbose, dry_run)\u001b[0m\n\u001b[1;32m    174\u001b[0m             copy_file(src_name, dst_name, preserve_mode,\n\u001b[1;32m    175\u001b[0m                       \u001b[0mpreserve_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                       dry_run=dry_run)\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/distutils/file_util.py\u001b[0m in \u001b[0;36mcopy_file\u001b[0;34m(src, dst, preserve_mode, preserve_times, update, link, verbose, dry_run)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Otherwise (non-Mac, not linking), copy the file contents and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# (optionally) copy the times and mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0m_copy_file_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreserve_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreserve_times\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/distutils/file_util.py\u001b[0m in \u001b[0;36m_copy_file_contents\u001b[0;34m(src, dst, buffer_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mfdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             raise DistutilsFileError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSP2MaxaJsJ"
      },
      "source": [
        "from torchvision import datasets\n",
        "test_data = datasets.ImageFolder('/content/drive/MyDrive/HackerEarth Holiday Season Challenge/test_dataset', transform=transf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAvrRpUraJsK"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcEhqcAmaJsK",
        "outputId": "cd4e6b1d-4f0e-444c-d210-a7b3137275ef"
      },
      "source": [
        "predictions = []\n",
        "for i,data in enumerate(test_loader):\n",
        "        inputs,labels = data\n",
        "        inputs = inputs.to(device)\n",
        "#         print(np.array(inputs.cpu()))\n",
        "        labels = labels.to(device)\n",
        "#         print(np.array(labels.cpu()))\n",
        "        outputs = model(inputs)\n",
        "        _,predicted = torch.max(outputs,1)\n",
        "        \n",
        "#         print(predicted)\n",
        "#         print(np.array(predicted.cpu()))\n",
        "#         np.array(inputs.cpu()\n",
        "        predictions.extend(le.inverse_transform([x for x in np.array(predicted.cpu())]))\n",
        "    \n",
        "        if i % 10 == 0:\n",
        "            print(i)\n",
        "        \n",
        "#         zip(le.inverse_transform([x for x in np.array(predicted.cpu())]))\n",
        "    \n",
        "#         break\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Ur-R1e-gaJsK",
        "outputId": "68b08f29-19d8-43c3-d90c-fcbb05cdf838"
      },
      "source": [
        "df = pd.DataFrame(zip([x[0].split('/')[-1] for x in test_data.imgs],predictions),columns=['Image', 'Class'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image10.jpg</td>\n",
              "      <td>Miscellaneous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image100.jpg</td>\n",
              "      <td>Airplane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image1013.jpg</td>\n",
              "      <td>Jacket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image1014.jpg</td>\n",
              "      <td>Miscellaneous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image1018.jpg</td>\n",
              "      <td>Miscellaneous</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Image          Class\n",
              "0    image10.jpg  Miscellaneous\n",
              "1   image100.jpg       Airplane\n",
              "2  image1013.jpg         Jacket\n",
              "3  image1014.jpg  Miscellaneous\n",
              "4  image1018.jpg  Miscellaneous"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDrAFhBhaJsL"
      },
      "source": [
        "df.to_csv('output.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}